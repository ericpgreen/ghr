[
["index.html", "Global Health Research: Design and Methods Preface About this Book Organization Icons Acknowledgements Colophon", " Global Health Research: Design and Methods Eric P. Green 2016-05-12   Preface Does the world really need another book about research methods? I think so. But I spent a fair amount of time writing down the ideas in this book, so I’m biased. But here’s my rationale. I went to graduate school for clinical psychology, and my classmates and I read all of the classic psychology texts on research design and methods. Books like “Experimental and Quasi-Experimental Designs for Generalized Causal Inference” by Shadish, Cook, and Campbell (2003). I still remember staying up late trying to memorize all of the different threats to internal validity outlined by Donald Campbell and colleagues. Meanwhile, across campus, my econ colleagues were reading the ideas of another Donald—Donald Rubin and what is now known as Rubin’s causal model. But I didn’t know this at the time. When I set off for Uganda in 2007, determined to learn more about this field called global health, I met some of these mini Donald Rubin’s in the wild. I tried communicating with them, but they had a strange dialect that I couldn’t quite understand. And they did not understand me and my Campbellian drawl. We were usually trying to say the same thing, just in the language of our peoples. But I couldn’t put all of the blame on the economists and the disciplinary gap between us. There was a lot I didn’t know that went far beyond differences in jargon. I was a psychologist trained in clinical research, and nearly every applied example I read about came from the U.S. or Europe. The young field of global mental health was still an infant when I was in school. The first Lancet series on global mental health that really put the field on the map was published in September 2007 as I was getting on a plane to fly back home. I really knew nothing about global health. Fortunately, students entering university today have many more opportunities to learn about global health through interdisciplinary studies. Duke University launched the first liberal arts global health major in the U.S. in 2013, and other universities have followed suit. The Duke program is unique because it requires global health students to co-major in another discipline, such as biology, economics, psychology, or public policy. I started teaching at Duke around the time the new co-major started, and I found myself in the position of needing to pick a textbook for a course called “Research Methods in Global Health”. I reviewed a lot of excellent books that covered the basics, but none integrated examples from the very diverse and interdisciplinary field that is global health. I saw this as a real limitation. So I decided to write my own book.  About this Book A guiding principle of this book is that a student of global health needs to be a student of medicine, biology, statistics, economics, psychology, public policy, and the list goes on. Just take a topic like malaria. A literature search will return articles about the spread of the disease (epidemiology), the impact of illness on future productivity (economics), the merits of free or subsidized bed nets (public policy), mosquito habitats (ecology), the efficacy of vaccines to prevent the disease (medicine and statistics), rapid diagnostic tests (biomedical engineering), the adoption and use of bed nets (psychology), and the list goes on. No one book or author could ever hope to provide full disciplinary coverage of even one topic like malaria, so my goal was much more modest. I wanted to create a resource that would teach the basics of research design and methods by exposing readers to real world global health examples from different disciplines. Another guiding principle is openness. Whenever possible, the examples come from open access sources. Every reader should be able to access 90% of the references in this book.   Organization The book is organized as follows. One objective of my course on global health research—and thus this book—is to make students better consumers of research. I wrote the first three chapters with this end in mind. Chapter 1 reviews the fundamentals of scientific research, but with a global health spin. In Chapter 2, you’ll learn how to search the literature for existing evidence. Better yet, you’ll learn to let someone else do that for you in the form of a systematic review or meta-analysis. You’ll also learn how to begin asking your own research questions. Chapter 3 on critical appraisal will teach you how to read and evaluate scientific evidence. One of the most important claims you’ll need to assess as a consumer of research is causality. Chapter 4 examines strategies for building up causal arguments. A second objective of my course is to give students the tools they need to begin careers as producers of research. The next few chapters lay this foundation. Chapter 5 gives you practical advice on developing a theory of change to guide program development, monitoring, and evaluation. It also helps to organize your approach to measurement, the topic of Chapter 6. This chapter explains how to define the measurement of your key study outcomes and covers fundamental psychometric concepts such as reliability and validity. In Chapter 7, you’ll learn strategies for determining how many research subjects you should recruit and methods for selecting them. The next two chapters introduce methods for collecting data. Chapter 8 covers quantitative methods, and Chapter 9 covers qualitative and approaches to mixing qualitative and quantitative methods. With the basics out of the way, we turn to designs that you can use to answer your research questions. Chapters 10 and 11 cover non-experimental designs. Chapter 10 focuses on the observational designs you typically find in epidemiology (e.g., cohort, case-control) and psychology (e.g., correlational). Chapter 11 introduces several “quasi-experimental” designs that manipulate a cause that comes before an effect, but without the benefit of randomization, the topic of Chapter 12. All of these chapters will refer back to the foundation in cause and effect we set in Chapter 4. The book concludes with several chapters to help you use your new knowledge to make an impact. In Chapter 13 you will learn how to prepare a study protocol. Chapter 14 will introduce you to the publication process and other opportunities for disseminating your work, such as professional conferences. Chapter 15 pushes you to think beyond your study results to make an impact on policy and practice. One limitation of this book is that it does not teach statistics. Statistical concepts are discussed throughout, but not in great detail. Every statistician will tell you that you need to think through your analysis at the study design stage. Listen to this advice. While this book will not give you the technical tools you need to plan your analysis, I hope you will come away with more appreciation for the gaps in your knowledge that you need to fill with further study. A great resource for learning applied statistics is OpenIntro Stats. If you are fortunate to be at a university with an applied statistics department, chances are you could get excellent consultation on your study protocol. This book WILL prepare you to narrow your options and have a smart conversation about how to meet your study objectives.   Icons I’ve sprinkled several types of asides throughout the book. If you are a student enrolled in my course, I recommend that you actually read them. Everyone else can flip the page and feel productive.   Help piecing together the global health puzzle     Extended discussion of a special topic     Tips     Videos     Review questions     Application exercises     Acknowledgements I’d like to thank some folks for their helpful feedback at various points throughout my writing process. My graduate student teaching assistants, Kaitlin Saxton, Kathleen Perry, and Jenae Logan, read and commented on the initial drafts. This could not have been fun, so thanks! Thanks to Duke librarians Megan Von Isenburg and Hannah Rozear for setting me straight on literature searches. I still have a lot to learn! Liz Turner, biostatistican extraordinaire, kept me from making too many mistakes on technical details here and there. I’d also like to thank students in my undergraduate and graduate global health research courses for test driving the book before all the parts were in place. Your feedback was [placeholder], and the book would have been [placeholder] without you. Special shoutout to the following students for sharing written feedback: Kelsey Sumner, Karly Gregory, Qian Yudong, and Christina Schmidt. Despite everyone’s best efforts to help me catch mistakes, I’m certain errors remain in the book. My bad.   Colophon This book is a work in progress. If you find errors (gasp!), please create an issue on Github, email me, or shame me on Twitter (@ericpgreen). I’m writing the book in R Markdown within RStudio. The `bookdown’ package from the makers of RStudio does most of the heavy lifting to compile the book. The source code for the book is available on Github.   "],
["science.html", "1 Science Fair Redux 1.1 Scientific Research 1.2 The Fundamentals Chapter Review and Application", " 1 Science Fair Redux     Figure 1.1: Example of intended style of overview video.       Believe it or not, you already know the basics of the research process. You probably have a yellowing, tri-fold piece of cardboard tucked in the back of the closet in your parents’ house that would prove me right. Just like Jimmy.1    Figure 1.2: What you see here is the scientific method in action. Jimmy asked a question, made a hypothesis, collected and analyzed data, and ultimately, made some conclusions based on the results. Science!. Source: http://bit.ly/1HbluM4   Even if you have not been as productive as Jimmy, I’m certain that you’ve had years of practice consuming research. We’re exposed to popular press accounts of research every day on TV, the radio, and the internet. Much of it might be wrong— “new study proves that eating chocolate prevents all cancer” —but it’s a start. So it’s a safe bet that just about every reader has some foundation to build upon. The goal of this chapter, therefore, is to re-introduce familiar concepts about scientific research from a global health perspective. We’ll come back to these fundamentals throughout the book and explore them in more detail. By the end you’ll be ready to move your work from primary school cafeteria to academic conferences, policy debates, and real world program design and delivery.  1.1 Scientific Research Let’s start with what we mean by “scientific research”. King et al. (1994) offer a nice definition in their book, “Designing Social Inquiry.”2 They point to several main characteristics:  The goal is inference The procedures are public The conclusions are uncertain   1.1.1 All about inference By stating that the goal of scientific research is inference, we mean that science goes beyond the collection of facts. You’ve probably seen the word inference used in several different contexts in your global health studies. At the most basic level, when we talk about inference, we are referring to the process of making conclusions about some unobserved or unmeasured phenomenon based on our direct observations of the world. This process can be deductive or inductive. In deductive reasoning, we start from general theories, make hypotheses, collect data, and make conclusions about our original theories and hypotheses based on the data. Inductive reasoning flows the other direction, from specific observations to the generation of hypotheses and theories. Remember it this way: if you are testing a specific hypothesis, you are using deductive reasoning. If you are starting with your observations and making more general statements, then you are using inductive reasoning. To say that quantitative research is deductive and qualitative research is inductive is not quite right, but it’s often true.3 For instance, Singla et al. (2015) report the results of a cluster randomized trial of a parenting intervention in rural Uganda.4 This study used quantitative methods; the primary outcomes of this study were cognitive and receptive language development of the children of participating caregivers measured with the Bayley Scales of Infant Development. The authors hypothesized that the intervention would improve child development. As you can see in the following table from the article, the program increased cognitive and receptive language scores, but did not have an effect on height-for-age, thus partially supporting the hypothesis.    Figure 1.3: Source: Singla et al. (2015), http://bit.ly/1UcVtoZ   Later in the book we’ll get into the nitty gritty details of how you read and interpret results like you see here. For now, let’s focus on the approach to reasoning. Singla et al. is an example of deductive reasoning. The authors started with a hypothesis, collected quantitative data (i.e., scores on a measure called the Bayley), and inferred something about the impact of the intervention.5 We can contrast the Singla et al. trial with a qualitative study by Sahoo et al. (2015) that exemplifies inductive reasoning.6 Sahoo and colleagues used a grounded theory approach to conduct and analyze interviews with 56 women in Odisha, India about their sources of stress and sanitation practices.7 This study is an example of inductive reasoning because the authors started with the data—their observations—looked for themes and patterns, and came to some conclusions about the nature of sanitation-related stress.8 Sahoo et al. observed that “sanitation” encompassed much more than defecation and urination, such as washing, bathing, and menstrual management. These sanitation activities brought numerous challenges that could be classified as environmental, social, or sexual and understood in the context of a woman’s life stage, living environment, and access to sanitation facilities. One result of this work was a conceptual framework for thinking about sanitation-related psychosocial stress, as shown below.    Figure 1.4: Source: Sahoo et al. (2015), http://bit.ly/1JB6nSs   The point to take away about inference is that, regardless of the approach to reasoning, the goal of scientific research is to use what we observe to make conclusions about what we do not observe directly. This is sometimes referred to as empiricism, and our systematic observations as empirical evidence. Empiricism is at the heart of scientific research.   1.1.2 Research as a public act Scientific research uses public methods that can be examined and replicated. Replication is a core principle of scientific research. No one study rules the day. If the results of your study are robust, another research group should be able to follow your methods and replicate the findings. When findings are replicated, we all have more confidence in the results. Replications are relatively rare, however. For one, there are often few resources for replicating studies, especially when it comes to big field experiments. Second, journal space is limited (especially if there is still a print version) and peer review takes a lot of resources. Journals want to use their space and resources to publish novel ideas (ironically, novelty can sometimes mean small effects with a lot of noise that might fail to replicate). Without the promise of a publication, researchers have little incentive to spend time and money trying to replicate published findings. Publications are a key criterion for tenure and promotion in academia, so many researchers don’t waste their efforts on studies that won’t get published. What happens when replications are attempted? Well, that’s a topic for a later chapter. The short answer is bitterness. Replicators grab more headlines when they “debunk” findings, and the original authors almost invariably call into question the quality of the replication. Just see #wormwars to learn what happened when a famous de-worming study was re-examined. Or Google social psychology and priming. Yikes! A separate but related issue is reproducibility, the ability to generate a study’s findings given the original dataset and sometimes the original analysis code. Think irreproducible findings are rare? Think again. The Quarterly Journal of Political Science found that slightly more than half of their published empirical papers subjected to review had results that could not be replicated with the author’s own code. The positive part of this story is that it’s becoming more common for authors to share their data and analysis code. This has been standard practice in economics for some time, but the idea is revolutionary in medicine and public health. We’ll explore why this is so important and easier than ever to do.   1.1.3 Living with uncertainty Every method has limitations, every measurement has error, and every model is wrong to some extent. In short, research is an imperfect process. Sometimes researchers make outright mistakes. These mistakes may or may not be detected and corrected in the peer review process, or during post-publication review if authors share their data and analysis code. Other findings are free of obvious mistakes, but fail to be replicated, and over time run counter to a growing body of literature that points in the other direction. In this way science is said to be self-correcting. We’ll discuss how this ideal can fall short in the face of challenges like publication bias, but the point here is to get comfortable in the short term with the idea of uncertainty. A good example of uncertainty comes from the estimation of maternal mortality. Hogan et al. (2010) published estimates for 181 countries.9 Some countries like the United States have vast amounts of data; vital registries that attempt to track all births and deaths. Countries with vital registries struggle with changing definitions over time, but the uncertainty interval around their estimates is typically tight, as shown in the figure below from the Hogan et al.’s supplementary webappendix, because there is a lot of good data.    Figure 1.5: Source: Hogan et al. (2010), http://bit.ly/1JBCelO   In many low-income countries the situation is very different. Here is the estimate and uncertainly interval for maternal mortality in Afghanistan. There are only four data points! No wonder the uncertainty interval is so great.10    Figure 1.6: Source: Hogan et al. (2010), http://bit.ly/1JBCelO   So how many women die during pregnancy or within 42 days of delivery? The same research group that published Hogan et al., the Institute for Health Metrics and Evaluation, estimated that there were 292,982 maternal deaths globally in 2013, with a 95% uncertainty interval ranging from 261,017 to 327,792; that’s a range of 66,775 for everyone who struggles with mental math.11 This might seem like a lot, but remember that we’re talking about global statistics for a world population of more than 7 billion people.12 The takeaway message is that there is uncertainty in everything. Don’t take any single estimate as the “Truth”. Instead, try to learn about the origin of estimates and recognize the limitations of what we know.    1.2 The Fundamentals Before we get too far along, we need to establish a common understanding of some fundamental concepts and terms. We’ll do so in the context of research on malaria.13  1.2.1 The nature of research Research can be classified as basic or applied. Basic research—or “pure” research—is the pursuit of fundamental knowledge of phenomena. An example would be the bench science to understand the parasitic life cycle and how parasites interact with humans at different stages. Basic research can be contrasted with applied research which is focused on specific problems or applications. For instance, an applied research question is how to increase the coverage and use of bed nets that prevent malaria transmission. Applied science takes many different forms, including clinical research. Clinical research is a broad field that encompasses patient-oriented research, epidemiological and behavioral studies, and outcomes research and health services research.14 Basic research is the foundation of clinical research.  Clinical trials One type of clinical research is a clinical trial. Drugs and vaccines have to pass through different phases of clinical trials before regulatory bodies will approve their use with humans:  Preclinical research Phase I Phase II Phase III Phase IV  Let’s take the development of a vaccine for malaria as an example of the clinical trial life-cycle. A vaccine candidate called RTS,S, or Mosquirix™, recently made news for getting one step closer to becoming a licensed vaccine after a successful Phase III trial. This moment was more than 30 years in the making. Development of RTS,S began in 1984 through a partnership between the pharmaceutical company GSK and the Walter Reed Army Institute of Research. The vaccine candidate was created in 1987 and entered preclinical research. During the pre-clinical phase, testing is performed in non-human subjects with the goal of collecting data on how well the vaccine works (efficacy), how much damage it can do to an organism (toxicity), and how it is affected by the body (pharmacokinetics). Clinical research on humans began in 1992. To obtain regulatory approval, the vaccine had to complete three phases of testing. Doherty et al. (1999) conducted a Phase I safety and immunogenicity trial with 20 adults in The Gambia in 1997.15 This small sample size is typical of Phase I trials where the objective is usually to find a safe dosing range and look for side effects. The authors reported that the vaccine did not have any significant toxicity but did produce the expected antibodies. Several Phase II studies conducted over a decade (Phase IIa and Phase IIb) demonstrated efficacy of the vaccine against several endpoints (a.k.a. outcomes).16 A Phase IIb trial began in Mozambique in 2003 with more than 2,000 children aged 1 to 4.17 Children were randomly assigned to receive three doses of RTS,S or a control vaccine. At 6-months, the prevalence of malaria was 37% lower in the treatment group compared to the control group. A follow-up study with 214 infants also showed partial protection.18 This was an important proof-of-concept. Final results of a large Phase III trial with more than 15,000 infants and young children in seven African countries were published in The Lancet in 2015.19 Children in the study were randomly assigned to 1 of 3 arms: 3 doses of RTS,S and a booster dose at month 20; 3 doses of RTS,S and a booster dose of a comparator vaccine at month 20; or 4 doses of a comparator vaccine. The study reported that RTS,S reduced clinical malaria cases by 28% and 18% among young children and infants, respectively, over a 3 to 4 year period. This is the goal of a Phase III trial—to show that a treatment is efficacious. On the basis of these results, the European Medicines Agency issued a “European scientific opinion”, which could help inform the decision of the WHO and African national regulatory authorities. If RTS,S is approved for use and eventually hits the market, researchers will likely conduct Phase IV trials to evaluate the vaccine’s long-term effects. This will not be the end for research on RTS,S, however. The vaccine may be efficacious, but that does not mean it will be easy or cost-effective to deliver at scale to millions. Studies that assess how to best get efficacious treatments to the people who need it most fall under the domain of implementation science. There are many stumbling blocks from getting interventions from “bench to bedside”, so to speak. Practitioners of translational research point to four key bottlenecks:  T1: translation from basic science to clinical research T2: translation from early clinical trials to Phase III trials and beyond with larger patient populations T3: translation from efficacy (Phase III) to real-world effectiveness—the domain of implementation science T4: translation from evidence about delivery at scale to new policy  Behavioral research (e.g., development and evaluation of parenting interventions) does not follow the same exact phases of vaccine and drug development, but the broad principles are the same.   Monitoring and evaluation Another arena of applied work in global health is monitoring and evaluation, or M&amp;E. Let’s start with the “E”, program evaluation. In the U.S., program evaluation became commonplace by the end of the 1950s and grew dramatically in the 1960s as the federal government expanded and introduced new social programs. Lawmakers wanted accountability, and the evaluation of social programs took off.20 But is program evaluation considered research? Methods giant Donald Campbell thought so:21  The United States and other modern nations should be ready for an experimental approach to social reform, an approach in which we try out new programs designed to cure specific problems, in which we learn whether or not these programs are effective, and in which we retain, imitate, modify or discard them on the basis of their effectiveness on the multiple imperfect criteria available.  Campbell had an outsized impact on the field. It’s no surprise that an organization dedicated to synthesizing the best available evidence on social interventions, the Campbell Collaboration, bears his name.   A good candidate for Donald Campbell’s successor is French economist and MIT researcher Esther Duflo. Together with Abhijit Banerjee and Sendhil Mullainathan, she co-founded JPAL, which stands for the Abdul Latif Jameel Poverty Action Lab. JPAL is a global research organization headquartered at MIT that uses randomized evaluations (i.e., experiments) to answer policy questions related to poverty alleviation. The JPAL website, http://www.povertyactionlab.org, contains excellent resources about the methods of randomized evaluations, published studies, and policy briefs. Interested readers should also check out Innovations for Poverty Action, or IPA, a sister organization of sorts that is also a leader in the use of randomized evaluations to study important policy questions about global poverty.   Yet, not everyone agrees. Educational psychologist Lee Cronbach certainly did not.22 Cronbach recognized the overlap in methods and designs, but he thought that program evaluation was really designed for program implementers and funders, and that the messy nature of programs required a loosening of research standards.23 Just learn what you can. In their introductory text on evaluation, Rossi et al. (2004) strike a balance in views.24 Their answer is perhaps a bit unsatisfying, but I’d argue true nevertheless. It depends. Program evaluations should be as rigorous as logistics, ethics, politics, and resources permit. And no less. Is there a lower bound in terms of quality that should limit what is even worth doing? Probably, but the line is so context dependent that it is not sensible to attempt a definition. If there is one rule to follow, I’d suggest that it’s this: “don’t go beyond the data”. Everyone wants to claim “impact”, but not every evaluation can based on the design and implementation. Now we turn to the “M”, program monitoring. Program monitoring is concerned with the implementation of programs, policies, or interventions. How are resources being used? Is the program being delivered as intended (a.k.a. with fidelity)? How many people participate, and does the program reach the intended targets? These are all program monitoring questions. Accurate monitoring is essential for reporting to funders, but it’s also essential for all good evaluations. The reason is simple. If a program is shown to not “work”—to have no impact—the next question is why? Did the program fail to have an impact because the idea or theory behind the program was wrong (theory failure)? Or was it the case that the implementation of the program was so troubled that there was never a chance of having an impact (implementation failure)? Every trial should include ongoing monitoring or a formal process evaluation.    1.2.2 Research problems and questions Every study begins with a research problem. A research problem represents a gap in our knowledge. In academic research, this is another way of saying a gap in “the literature”.   Usually when people speak of “the literature”, they mean scholarly or peer-reviewed journal articles. There is also something called “grey literature” that is more encompassing and harder to search systematically. Grey literature sources are typically disseminated through channels other than peer-reviewed journals. Examples could include technical reports or white papers published on the web.   Research problems are typically broad. For instance, stakeholders might want to know how to increase the use of bed nets for children under 5 years of age. Or whether all children should receive deworming medication prophylactically.   Stakeholders can refer to a wide range of people and organizations. Typically we mean donors (i.e., the public and private organizations that fund research and programs), policy makers (i.e., government officials and bureaucrats at international bodies like the WHO), program implementers (i.e., organizations like Doctors Without Borders that actually deliver services to beneficiaries, a.k.a. people), and scholars who study the topic or policy issue.   These problems have something in common: they are solvable. In his introductory text on behavioral research methods, Leary (2012) writes that this is another key criterion for scientific research. The problems must be solvable. This does not mean easy; it just means that we can use systematic, public methods to gather and analyze data on the problem. Think of it this way: we can come up with a method for studying how to get more parents to ensure that their kids sleep under a mosquito net every night, but we don’t yet have a scientific method for determining whether there is a mosquito afterlife where these pests get to buzz around for all of eternity. In order to study a broad research problem, we must narrow to a more specific research question. de Vaus (2001) says there are essentially two types of research questions:25  Descriptive—what is going on? Explanatory—why is it going on?  Let’s stick with the bed net example. If we want to study uptake or use of bed nets, we might ask a descriptive research question like, “How many children sleep under bed nets?” But this is too general. Children of what age? Living where? We also need to operationalize what we mean by sleeping under a bed net. It’s common in this line of research to ask about the previous night, as in the night before the survey. As we will discuss in the chapter on measurement, we have to consider challenges to getting valid information, such as recall difficulties. A better way to phrase the question might be, “What percentage of children under 5 years of age in Kenya slept under an insecticide treated net the previous night?”   1.2.3 Research designs As Glennerster and Takavarasha (2013) explain in their excellent practical guide to running randomized evaluations, different research questions require different research designs.26 We’ll spend most of our time in this book looking at strategies for answering the “why” questions. I lump them into three categories and differentiate them from descriptive designs:  descriptive correlational quasi-experimental experimental   Descriptive research The goal of descriptive research is to characterize the population. Often this means estimating the prevalence of a phenomenon or disease. X% are illiterate. Y% have an unmet need for contraception. Z% are HIV positive. Description can also be qualitative in nature (e.g., thick description). Just about every study will have some descriptive element. Some studies are exclusively descriptive. A good example are the Demographic and Health Surveys, more commonly referred to as DHS surveys (yes, “surveys” is redundant). Every student of global health should come to know what the DHS Program has to offer. The program is funded by the U.S. Agency for International Development (USAID), and registered users can request access to data from more than 300 surveys conducted in 90+ countries.   DHS surveys are a good example of demographic research. Demographers contribute to and use data sources like DHS surveys and national population and housing censuses to understand more about population size, structure, and change (e.g., birth, death, migration, marriage, employment, education).   Many countries strive to conduct a census, or an enumeration of all citizens, every 10 years. The United Nations Statistics Division and the United Nations Population Fund (UNFPA) provide technical support (a.k.a., help) to countries preparing for, conducting, and analyzing a national population and housing census. These two organizations, in partnership with the United Nations Children’s Fund (UNICEF), maintain CensusInfo, a database of global census data.   Here is the relevant table from the 2014 Kenya DHS Key Indicators Report for describing the prevalence of ITN use.27 This is a typical DHS cross tabulation (or crosstab) of the results. In this example, the percentage of children under the age of 5 that slept under an insecticide treated net the previous night in Kenya was 54.1%. This descriptive data is further disaggregated by residence and wealth quintile as is typical for DHS tables.28    Figure 1.7: Source: Kenya 2014 DHS Key Indicators Report, http://bit.ly/1g4NYS5   The data summarized in this table describes the problem of bed net use. Descriptive questions are well-suited for needs assessments. Before we can design a program or policy to increase bed net usage, for instance, we need to understand the need. In Kenya, almost half of children under 5 are not sleeping under insecticide treated nets according to the DHS. This is a particular concern for children living in areas of high risk. According to research by Noor et al. (2009), this risk is most acute for the area shaded in dark red in the figure below, the area around Lake Victoria in the west.29 What is the prevalence of ITN use in this region? To answer this question of need, we can look to another survey series from the DHS Program called the Malaria Indicator Survey (MIS).    Figure 1.8: Source: Noor et al. (2009), http://bit.ly/1T06tU9. Spatial distribution of P. falciparum malaria in Kenya at 1×1 km spatial resolution. Endemicity classes: PfPR2-10 &lt; 0.1%; ≥0.1 and &lt; 1%; ≥1 and &lt;5%; ≥5 and &lt;10%; ≥10 and &lt;20%; ≥20 and &lt;40%; ≥40%.   Here are descriptive results from the 2010 Kenya MIS. The overall prevalence of ITN use (42.4%) is about 12 percentage points lower than what was estimated in the 2014 DHS (54.1%), so we might conclude that the use of ITNs among young children—those most at risk, along with pregnant women—is on the rise. But from a needs assessment perspective, we are probably most interested in knowing more about the patterns of use, including by age and malaria endemicity. Use appears to fall off as kids get older, and in the “Lake endemic” region—the region of highest risk—more than 55% of children under 5 are not sleeping under ITNs. If there is good news, it’s shown in the second to last column. Use jumps to 70.2% if the household owns a net.    Figure 1.9: Source: 2010 Kenya MIS Survey, http://bit.ly/1VZqM8E     These DHS reports are all examples of cross-sectional studies. These are typically one-off surveys but can include other forms of data collection. The key is that it’s a snapshot. The goal is often description but might also include correlation. Cross-sectional studies are differentiated from panel or longitudinal studies by their participants; the latter include the same research participants (sample) over time in multiple studies, whereas cross-sectional studies only include a particular sample once. So even though the DHS Program will conduct a new survey in a country every five years or so, they always recruit a new sample of participants (a.k.a. “successive independent samples”). This makes the DHS surveys cross-sectional rather than panel or longitudinal in design.     Correlational research This descriptive information sheds light on programmatic and policy priorities, but we have to go beyond describing the problem to make a difference. A helpful next step is often to build on descriptive insights by attempting to predict or explain the behavior or phenomenon. For instance, Noor et al. (2006) asked a correlational research question (edited below) about the factors associated with net use among children under the age of 5:  Are wealth, mother’s education, and physical access to markets associated with the use of nets purchased from the retail sector among rural children under five years of age in four districts in Kenya?30  Correlational research asks questions about the relationship (a.k.a. association) between two or more variables. In this case, ITN use and a variety of potentially influential factors, such as household wealth and a mother’s education level.   You’ll encounter different labels for variables, and it can be confusing to keep straight at first. The behavior Noor et al. are trying to predict, bed net use, is the dependent variable, often referred to as the outcome, response variable, or simply Y. The factors thought to be related to ITN use are the independent variables. Some disciplines will call them predictor or explanatory variables. Sometimes they are controls, exposures, or simply X. This will become more clear as we go.   Noor and colleagues reported that only 15% of children in the rural study sample slept under a net the previous night—a much lower percentage than the national prevalence reported by recent DHS surveys. As shown in the table below, they also found that several factors were associated with higher odds of bed net use, including: greater household wealth, living closer to a market center, not having older children present in the household, having a mother who is married and not pregnant, being younger than 1 year old, and having an immunization card.    Figure 1.10: Source: Noor et al. (2006), http://bit.ly/1HoltVo     We’ll review in detail how to read tables like this in later chapters, but it might be helpful to preview some concepts here. The results in this table come from a multivariable logistic regression. The authors describe the model they fit in the article, but we won’t worry ourselves with these details. Instead, let’s highlight a few key points.   The table starts with household-level predictors of net use, specifically household wealth quintile. The first two columns show us the number and percentages of households that have or do not have retail sector nets, disaggregated by wealth quintile. So of all the households without retail sector nets, 690 or 21.6% were classified as being the most poor. The distribution looks a bit different for households with nets; 40.1% of households with retail sector nets were classified as being the least poor. This makes intuitive sense: if you are poor, you are less likely to purchase a net from the market.   The next three columns give us the results of the regression. Since wealth quintiles have different categories, the authors set one category—the most poor—to be the reference category. So the results will be relative to the poorest households.   As you can see, the odds of using a net are 10.17 higher among the “least poor” compared to the “most poor”. This does not tell us why wealthier households are more likely to use nets for their young kids, but we know that there is some relationship here.   Remember, however, that 10.17 is just what we call a point estimate for the odds ratio. The 95% confidence interval ranges from 5.45 to 18.98. We’ll talk more about the specifics as we go, but it’s important to get in the habit of evaluating the uncertainty of every estimate. In this case, it’s pretty clear that having more money is associated with better preventive behaviors.   You will often see descriptive and correlational studies like the DHS and Noor et al. (2006) classified as non-experimental or observational studies. Other observational designs include cohort and case-control studies (the topic of a later chapter). Researchers use these designs to determine whether there is an association between some exposure and a disease.   Observational studies are the bread and butter of epidemiology. Epidemiologists often conduct cross-sectional studies to estimate the prevalence and incidence of different disorders as well as correlational research to understand risk and protective factors.    Prospective cohort In a prospective cohort study, healthy participants are recruited and followed into the future for a period of time. For instance, Lindblade and colleagues (2015) conducted a prospective cohort study in Malawi to test the efficacy of ITNs in an area of moderate resistance to pyrethroids, a common class of insecticide.31 A prospective cohort of 1,199 healthy children aged 6-59 months was followed for a year. The group of children make up the cohort, and the fact that they were recruited and followed for a period of time into the future makes the design prospective.32 Compared to no bed nets, ITNs reduced the incidence of malaria infection by 30%. This is promising, but the study design has limitations. One important limitation is that the children were not randomized to ITN access. So it could be the case that children who used the ITNs were somehow different from the children who did not use the ITNs. This is a potential selection bias, a threat to internal validity. You’ll learn more about such threats in a later chapter. The basic challenge for causal inference is that the design does not rule out the possibility that something other than ITN use accounted for the reductions in malaria infections.   Case-control Sometimes it is not possible to recruit a group of healthy people and wait to see who gets sick. Imagine having to wait a decade or more to see who develops rare diseases like gliomas. This would be a very expensive study that would need to involve thousands of people to study such a rare disease that takes time to emerge. A case-control study might be a better fit. In this design, researchers identify people with the disease (cases) and without the disease (controls) and ask them about past exposures. Obala et al. (2015) did this in Kenya with 442 children hospitalized with malaria and healthy matched controls without evidence of malaria.33 They wanted to know why there is a high malaria burden despite high ITN coverage. The research team visited visited the home of each case and control and asked questions about ITN coverage and recent use, along with measuring the parasite burden of family members, mapping nearby potential vector breeding sites, and assessing neighborhood ITN coverage. Obala and colleagues found that ITN coverage was not correlated with hospitalizations, but consistent ITN use decreased the odds of hospitalizations by more than 70%. As with prospective cohort designs, there is a risk of selection bias. In this case, we have to be concerned that the matching process was not perfect. The matching was done on the basis of age, gender, and village. But there could be unmeasured ways in which the cases and controls differ, which would undermine the results.  Correlational studies can yield important insights of course, but they have limitations. You’ve probably heard that correlation does not equal causation. For instance, did you know there is a nearly perfect correlation between the per capita consumption of cheese and the number of people who have died by becoming tangled in their bedsheets? (If you just put down the hunk of aged cheddar you were eating, please keep reading this book!) That said, all studies have limitations and tradeoffs. Designing a good study is a process of weighing scientific objectives with logistical constraints, ethical considerations, time, money, and a host of other factors. Keep reading to learn more about how to make these tough calls.    Experimental and quasi-experimental research Without a doubt, the correlational results described in studies like Noor et al. (2006) can help to design programs and policies. But what we often want to know is whether our programs and policies “work”. When we ask whether something works, it’s a question of impact, and impact evaluations use experimental or quasi-experimental research designs. (Experimental and quasi-experimental are addressed in more detail in later chapters) The goal of impact evaluations is causal inference. Does X cause Y? Does a particular program or intervention or treatment increase or decrease a particular outcome? For reasons we will explore in greater detail later, experimental research designs offer the cleanest estimate of impact. The hallmark of an experimental design is that we as researchers manipulate some independent variable and examine changes to some dependent variable that result. A common example in global health is the randomized controlled trial (RCT) in which some units, such as individuals, schools, or communities, are randomly assigned to receive an intervention (treatment) or not (control). We measure an outcome after the intervention period and estimate the average difference between the two study arms, also known as the average treatment effect. Experiments are the “gold standard” in the eyes of many people, but researchers are not always able to assign people or clusters to study arms or otherwise manipulate an independent variable. Logistics and ethics can get in the way. In these cases, researchers might rely on non-experimental designs commonly referred to as “quasi-experimental” designs. The name of the game in quasi-experimental research is to reduce threats to internal validity, something that randomization pretty much takes care of naturally. Beware: not all non-experimental designs are created equal. We’ll discuss several of them later in this book, including:  pre-post post-test only difference-in-differences multivariate regression and matching regression discontinuity instrumental variables interrupted time series   An important global health policy question that has been studied using experimental and quasi-experimental methods is the impact of user fees on the adoption of health goods, such as bed nets. Advocates of fees argue that free distribution is not sustainable and leads to waste when people who don’t need or want the goods are recipients. There is also an argument that people only value what they pay for, so removing fees will make people less likely to use goods like bed nets. The flip side is that the provision of some health goods, in economics-speak, creates “positive externalities” and should therefore be financed with public dollars. What this means is that some interventions have spillover effects whereby people who are not treated still experience some indirect effect. A good example of a spillover effect is vaccines and the resulting herd immunity. Hawley et al. (2003) showed a similar protective effect of ITN use on child mortality and other malaria-related outcomes among households without ITNs that were located within 300 meters of households with ITNs.34 So we know that there is evidence that ITNs have direct35 and indirect benefits. The research question is then how to increase coverage and use of nets. Is free distribution the best strategy, or should users have to spend something to get a bed net that might retail for a price that is out of reach for many poor households?  Quasi-experimental Agha et al. (2007) used a quasi-experimental design to estimate the impact of a social marketing intervention on ownership and use of ITNs in rural Zambia.36 Nets that commonly sold for USD $27 were subsidized and sold for $2.50 at public health clinics. Neighborhood health committees were established and 600 volunteer “promoters” were trained to teach residents about malaria and encourage them to purchase the nets. To estimate the impact of the intervention, the authors analyzed data from post-intervention surveys in three intervention and two comparison districts. This study design was quasi-experimental because the districts were not randomized to the intervention or control arms.    Figure 1.11: Source: Agha et al. (2007), http://bit.ly/1MkO5a0   Agha and colleagues reported that ITN ownership and use was higher in intervention districts according to the post-intervention data, but were careful to avoid going ‘beyond the data’ to claim evidence of a causal relationship. There are several design limitations to consider here, and you will learn more about how to spot these issues as we go. Briefly, we can note that (i) the authors did not randomize districts to study arms and (ii) no baseline (a.k.a. pre-treatment) data was collected. Experimental studies benefit from but do not require baseline (or pre-intervention) data because randomization usually ensures that the treatment and comparison groups are similar at the start—if enough units are randomized. But a non-randomized study like this leaves itself open to criticism without baseline data to show that the intervention and comparison districts were similar before the intervention was introduced. The results suggest that they were different after the intervention period, but we can’t be sure this was caused by the intervention itself. Given the limitations, how should we view the results? If this was one of the first studies on the topic, we would view it as a starting point that would encourage more rigorous investigations. As part of a larger body of evidence, however, it would probably be passed over in systematic reviews and meta-analyses—studies of studies—because of the limitations of the design for causal inference.   Experimental Another limitation of Agha et al. (2007), at least for our purposes, is that it does not provide a direct answer to our policy question: should ITNs be free or subsidized? Fortunately, other studies fill this gap. Cohen and Dupas (2010) used an experimental design to study this question in Kenya where malaria is the leading cause37 of morbidity and mortality.38 The authors randomly assigned 20 prenatal clinics in an endemic region to 1 of 5 groups: a control group that did not distribute ITNs, a free distribution group, a group that charged 10 Ksh per ITN (97.5% subsidy), a group that charged 20 Ksh (95% subsidy), and a group that charged 40 Ksh or about $0.60 USD (90% subsidy). When units like clinics, schools, and villages are randomized, we refer to the design as a cluster-randomized trial, or CRT. The authors followed up a subset of pregnant women over time and found that those who paid a subsidized price were no more likely to use the bed nets than women who received one for free. They also found that the increase in price from $0 to $0.60 USD reduced demand for ITNs by 60%. This implies that the cost-sharing model of having women pay something for ITNs will reduce coverage. This is bad for the women who forgo a net purchase because of the direct prevention effects of ITNs, but we know from Hawley et al.’s work that it’s also bad for the community since ITNs have spillover effects. Cohen and Dupas conclude that free distribution would ultimately save more child lives.     1.2.4 Research methods If research designs are strategies for answering research questions with the best possible evidence, then research methods are the tactics for obtaining the evidence. Often methods are divided into three broad categories:  quantitative qualitative mixed  Quantitative methods are used to collect and analyze numerical data. This includes binary or dichotomous** data (e.g., hospitalized or not), categorical data (e.g., wealth quintile), and continuous data (e.g., hematocrit). A good example of a quantitative method is a survey in which people are asked to answer questions with fixed response options or provide numerical values, such as their monthly income. Lab tests resulting in disease classifications (yes/no) or a measurement such as the number of blood cells in a sample of blood are also examples of quantitative methods. Qualitative methods focus on non-numerical data. Participant observation, interviews, and focus group discussion are common qualitative methods in global health. Qualitative methods are well-suited for obtaining thick description and for exploration. For instance, Scandurra et al. (2014) analyzed data from interviews, observations, photos, and videos to study perceptions and practices related to bed net care and repair in Uganda.39 As is typical of manuscripts based on qualitative data, the authors include illustrative quotes, such as this one regarding net repair from a 55 year-old female:  [It] depends on one’s situation. If you have money, there is no need of sewing a net, you just buy a new one but if you are poor, you have to do it. So this is when you are poor.  Scandurra et al. found that there are strong social norms around net hygiene and appearance. Dirt floors and indoor cooking with dirty fuel sources and little ventilation tarnish the look of nets, and as a result, nets get washed frequently and may reach their lifetime wash limit much sooner than commonly assumed. If true, this could have implications for preventive efficacy. Often qualitative methods are seen as being less rigorous because they are more flexible and do not lead to the same type of hypothesis testing and results compared to quantitative methods. But this is not true. As we’ll discuss in a later chapter, rigor is a characteristic of how the methods are applied rather than the methods themselves. Your choice of methods should be based on your research question. It’s often the case that impact evaluations use quantitative methods, but there is not a 1-to-1 match between research designs and methods. Many studies incorporate both quantitative and qualitative methods, and we refer to this as mixed methods. Sometimes the goal of mixing methods is triangulation of results with respect to the same research question. Other times we begin with qualitative work to develop the tools and measures that we will use in a trial. When qualitative work follows a quantitative phase, the goal is often to explain or explore results in more depth that was not possible with the quantitative data. Increasingly you will see RCTs complement their use of quantitative methods with qualitative inquiry.40 Alaii et al. (2003) provide a good example.41 The authors of this paper incorporated qualitative interviews on non-adherence into a larger randomized trial42 of the efficacy of ITNs on child morbidity and mortality in Kenya. They wanted to better understand why people, particularly children under the age of 5, were not using their ITNs correctly. Alaii et al. found that more than a quarter of individuals were non-adherent, often due to excessive heat.   1.2.5 Theories and hypotheses Many impact evaluations fit the label of black box evaluations, meaning that they don’t focus on why programs do or don’t have an impact. The evaluation is not guided by theory, and the hypotheses are as simple as “the program will have an impact on the outcome”. White (2009) outlines a strategy for changing this and moving to theory-based impact evaluations.43 Leary (2012) defines a theory as “a set of propositions that attempts to explain the relationships among a set of concepts.”44 In quantitative research, you could replace “propositions” with “hypotheses” and “concepts” with “variables”. As we reviewed earlier, the logical approach in quantitative research is often deductive. You start with theory and develop research hypotheses that are then tested. A hypothesis is an a priori prediction about what will occur—about how constructs are related. If the hypothesis is supported by the data, you have support for the underlying theory. If your study is well designed, it might be given more weight as other researchers consider the evidence in support of the theory. In theory testing, 1+1 does not always equal 2. For a hypothesis to be scientific it should be falsifiable, or testable. To return to a silly example from earlier, the following would not be a research hypothesis because it cannot be tested: “if a mosquito is killed, it goes to mosquito heaven”. Maybe, but we can’t test this hypothesis. Science progresses through the possibility of falsification, so hypotheses must be engineered to potentially fail.  Proving and disproving theories To return to an earlier example, some people advocate against the free distribution of ITNs out of the belief that there is a “sunk cost” effect when having to spend money for a bed net; people will use the net more to justify their purchase.45 In this case, the theory is one of sunk costs directing behavior. The falsifiable hypothesis tested by Cohen and Dupas (2010) was that people who paid a non-zero price for an ITN would use the ITN more than those who received the ITN for free. As you will recall from our discussion, there was not support for this hypothesis. So the theory is rejected, right? Not necessarily. Leary (2012) offers some helpful advice for thinking about proof and disproof.[leary3] Proof is logically impossible, whereas disproof is practically impossible. Frustrating, right?  Proof is not possible It helps to state the theory and hypothesis as an if-then statement. For example, “If the theory of sunk cost effects is true, then people who pay for an ITN will be more likely to use it than people who get an ITN for free.” If the theory is true, the hypothesis will be true. What happens if you flip this statement? If you find evidence that the hypothesis is true—as you might in a study—does it mean that the theory is true? Cohen and Dupas (2010) did not find support for the hypothesis that people who paid a non-zero price for an ITN would use the ITN more than those who received the ITN for free. But let’s pretend for a moment that they did. Would that prove the sunk cost theory? No, logically it can’t. It would be like concluding that my raging fever is malaria because I have mosquito bites all over my arm. The “theory” here would be that my fever is malaria, and the hypothesis would be that I must have been bitten by a mosquito. If I have mosquito bites all over, my fever must be malaria, right? Well, no. I was bitten by a mosquito, but maybe the scene of the crime was my backyard in the eastern United States where we don’t worry about malaria. So in this case, the hypothesis was true, but it doesn’t prove the theory.   Disproof is possible, but uncommon What if the hypothesis was not supported, and I was not bitten by mosquitos? Could my “theory” be true—could my fever be malaria? No. And logic would support this. If the hypothesis is derived from the theory, and if the hypothesis is not supported, the logical inference is that the theory is wrong. Yet, we still shy away from concluding that the theory is wrong. The reason is simple: complexity. A study like Cohen and Dupas (2010) could fail to reject the null hypothesis that use does not differ between free and subsidized clients—thus not supporting the hypothesis of different use rates—but there are many practical reasons for this. For instance, maybe their measure of bed net use was systematically flawed and hid the difference as a result. The possibilities are endless. This is partly why journals are hesitant to publish null results.    Science marches on So, where do we go from here? Answer: the literature! No one study is enough to lead people to discard a theory. But several null results might be. Conversely, no study ever proves a theory, but an accumulation of studies showing support for the theory-derived hypothesis builds confidence in the theory. Particularly when the studies are conducted by different researchers, across different populations, and triangulating with multiple methods. Of course you see the challenge here. How do researchers know that several studies have failed to support a certain theory if journals are reluctant to publish null results? And if negative evidence is missing, won’t the positive evidence be over-represented in the literature? Yes. This is the problem of publication bias, or the file drawer problem, and there is not an easy answer. Efforts like AllTrials to register and report the results of all trials, regardless of outcome, seem like a step in the right direction. So, to the literature we go.     Chapter Review and Application   Review Questions       Figure 1.12: Example quiz.     Application     "],
["literature.html", "2 Going Beyond Google 2.1 Start with Systematic Reviews and Meta-Analyses 2.2 Devising a Search Strategy 2.3 For the Love of Everything Holy Use a Reference Manager Chapter Review and Application", " 2 Going Beyond Google Repeat after me:  I will not start my research by Googling “deworming”. I will not start my research by Googling “deworming”. I will not start my research by Googling “deworming”.  Back in 2012, Google’s CEO Eric Schmidt said we create as much information in two days as we did from the beginning of time through 2003. Two days! And that was back in 2003. Sure, 10% of this info is probably cat photos, but still. This is a lot of information to sift in search of a gold nugget. Without a very specific query, Google probably won’t find this nugget. Google Scholar, the search giant’s solution to the challenge of finding scholarly sources, will get you closer, but you’ll have better luck if you know what you are looking for. Let’s assume you don’t. Where do you start?  2.1 Start with Systematic Reviews and Meta-Analyses If your topic is deworming and you have in mind the wiggly creatures that are great fish bait, then Wikipedia is probably a good first stop. No shame in that. Otherwise, it is always a good idea to start with a check for relevant systematic reviews or meta-analyses.    Figure 2.1: Levels of evidence   Meta-analyses and systematic reviews are ‘studies of studies’, and they sit atop the evidence hierarchy. They enjoy this status because they synthesize the best available evidence.  2.1.1 Meta-Analyses A meta-analysis is a quantitative approach in which the results from multiple studies are combined to estimate an overall effect size. We’ll talk more about effect sizes later, but the concept is pretty simple. There are several types of effect sizes, but we’ll use mean difference in weight as an example. Here is a forest plot from a meta-analysis by Taylor-Robinson et al. (2015) with results from five similar deworming studies.46 These studies randomly assigned children infected with parasites—607 across all five studies—to receive a single dose of a deworming drug like albendazole (n=319) or a placebo (n=308). Each study examined the impact of the medication on child weight.    Figure 2.2: Source: Taylor-Robinson et al. (2015), http://bit.ly/1DLEu9o   In Yap (2014), a study of 194 children, the mean weight among the deworming group was 2.2 kg. This compared to 1.9 kg among the control group. The difference between the two means, 0.30 kg, is the effect size. On average, the medication increased child weight by 0.30 kg.47 The effect size for each study is presented in the far right column and depicted graphically in the size of point estimate square. The mean difference in weight in each study was positive, suggesting that every study found that medication increased child weight. All of the point estimates fall to the right of the line of no effect running up from 0, thus favoring deworming. The overall effect size is shown last as 0.75 kg. The calculation that resulted in 0.75 kg was not as simple as averaging the five studies. This is because the studies were not given equal weight (emphasis, not kg), as you can see in the “weight” column. Freij (1979) only had a sample size of 13 children. As a result, the effect size estimate is very noisy. The 95% confidence interval is huge and crosses 0. Consequently, it’s weight is a low 2.8%. So here in one figure we get a summary of the best available evidence and an estimate of the overall effect size, with uncertainly intervals. You can’t get that from a Google search.   2.1.2 Systematic Reviews You might be wondering how Taylor-Robinson et al. (2015) found these five studies in the first place. The answer is through a systematic review of the literature. Not all systematic reviews include meta-analyses, but most meta-analyses include systematic reviews. A capital S “Systematic review” is distinct from a literature review—even if you go about your lit review systematically.  The goal of a systematic review is to to be comprehensive and include every relevant article. The literature review that you write for the introduction of your manuscript is not expected to be exhaustive. For this reason, most systematic reviews are conducted by teams given the large scope of the work, whereas literature reviews can be handled solo. Systematic reviews must define and follow a method that can be replicated, just like any other study. Literature reviews, on the other hand, don’t have to follow such rigid methods or make the methods explicit. Most systematic reviews pre-register this plan, meaning that the authors submit their planned methods to a registry like PROSPERO prior to conducting the study. Pre-registration gives other researchers confidence that the team is not cherry picking results at the end to make an interesting paper. It also lets other researchers know that a group is already working on the same review, thus signaling that their work might duplicate efforts and fail to get published. Included in these pre-registration plans will be a specific search strategy with exact search terms for individual scholarly databases so other researchers can recreate the search. It’s a good idea to do the same for a literature review, even if not a strict requirement. Similarly, a systematic review must also outline clear criteria for including and excluding studies (e.g., keep if assignment to study arms was random). With these criteria in place, team members screen all search results, usually starting with title and abstract reviews only and moving to full text reviews as the pool of eligible studies dwindles. Screening for a literature review is typically less intensive. Systematic reviews also develop and follow guidelines for extracting details from every included study, such as numbers of participants and key outcomes. An annotated bibliography might suffice for a literature review. Finally, teams conducting systematic reviews formally assess the quality of each included study, including the potential for bias, and take these assessments into account when synthesizing the results. This process is more ad hoc for literature reviews.   Where to find systematic reviews Three excellent sources for finding systematic reviews in global health are the Cochrane Library, the Campbell Collaboration, and 3ie. You can also get to many of the reviews in these databases by searching within PubMed using the Clinical Queries feature.   How to read systematic reviews Cochrane reviews follow a standard format that can look overwhelming at first, but is actually quite easy to read and understand. As with most journal articles, Cochrane reviews begin with an abstract. Next comes a plain language summary which can be helpful for newcomers to a particular topic. Summary tables round out everything you need to make your initial judgment. In the Taylor-Robinson et al. (2015) deworming review, we find the following in the plain language summary:  In trials that treat only children known to be infected, deworming drugs may increase weight gain (low quality evidence), but we do not know if there is an effect on cognitive functioning or physical well-being (very low quality evidence).  This one sentence brings us up to speed with the state of the science for treating infected kids (and points you to some gaps in the literature!). Google does not filter the evidence in this manner. Starting with systematic reviews pays off almost every time.     2.2 Devising a Search Strategy Of course not every topic has been the subject of a systematic review or a meta-analysis. In this case you need to search the literature. It helps to clearly define your research question before you begin your search.  2.2.1 Asking a research question Here’s a helpful mnemonic for creating a good clinical question: PICO.    P Patient, Population, or Problem   I Intervention, Prognostic Factor, or Exposure   C Comparison   O Outcome    Let’s try to use PICO to create a research question for a portion of the Taylor-Robinson et al. (2015) systematic review. The problem we want to address is soil-transmitted intestinal worms. The population is children with infections. Not every clinical question involves testing of a treatment or intervention, but we’ll focus a lot on these types of questions in this book. For the example at hand, the intervention would be school-based deworming (single dose). [Prognostic factor refers to covariates that could influence the prognosis of the patient. An exposure would be something that we think might increase the risk of an outcome.] Similarly, not every question involves a comparison group. Clinical trials always will. In this example, the comparison is a placebo. The primary outcome of interest is child weight in kg. We can combine all of this into a research question:  Among children infected with soil-transmitted intestinal worms, is a single dose of deworming medication delivered through a school-based program more effective than a placebo at increasing child weight?    2.2.2 Approaches With your basic research question outlined, you’re ready to begin searching. At the beginning you might take a quick and dirty™ approach to get started. Eventually you’ll need to graduate to a proper search strategy to be more systematic, even if your end goal is not a capital S Systematic review.  Quick and dirty A reasonable initial approach is to find a few recent articles to get a quick sense of what is out there. Google Scholar could come in handy here. For instance, my advanced Google Scholar search for “deworming” identified a recent paper by Aiken et al. (2015) that attempted to replicate an earlier deworming study.48   Customize your Google Scholar experience by clicking on the gear icon. Enable use of a bibliography manager, and click on “Library links” to add your library to get links to full text.   A good starting point for future searching is to note an article’s keywords. You’ll often find them listed right after the abstract. Aiken and colleagues list several: helminth, worms parasitic, randomized controlled trial, primary schools, and Kenya. Next comes the introduction. Some journals and disciplines have very brief introduction sections and might not be of much help. This is often true in medicine and public health. It is definitely the case for Aiken et al., but we still get a few interesting leads: Worms are an important public health concern [2 references given]  Helminth infections cause substantial morbidity across the developing world.  There is some debate over the efficacy of deworming [1 reference given]  Opinions differ over whether deworming schoolchildren in such settings improves nutritional outcomes, school attendance or educational achievement.  A study on school-based deworming in Kenya is a particular source of debate and disagreement [2 references given]  Central to this debate is a study describing the impacts of a school-based deworming programme in Kenya on the health, school attendance and academic performance of school pupils.  The discussion section is also a place to look for new leads. Authors typically use the discussion to link the study results to the existing literature, demonstrating how the results add to what is already known. After looking at the introduction and discussion sections, it’s often useful to skim the references to get a sense of which journals publish this type of work. If a certain journal appears to be a common outlet for this work, a scan of the journal’s table of contents for recent issues could be useful.49   If you have access to a university library, you can learn more about the scholarly journals in a field by looking up Journal Citation Reports. This annual report ranks the journals in each field according to impact factors. Impact factors are one metric used to evaluate the importance of a journal in its field.     More systematic  Plan and document your search strategy Whether or not you are conducting a capital S Systematic review, it’s a good idea to plan and document your search. You don’t need to be as thorough in a lit review as you would for a systematic review, but it wouldn’t hurt to take a page from the approach. Let’s look at Taylor-Robinson et al. (2015) for some inspiration. Every good systematic review will include a table or appendix like this one to make the method reproducible. If you and I run this search query at the same time on two different computers, we should get the same results.    Figure 2.3: Source: Taylor-Robinson et al. (2015), http://bit.ly/1DLEu9o   For the purposes of your literature review, you don’t necessarily need to ensure that other people can recreate your results, but you should make sure that you can. Pro tip: If you can create an account with the database, do it and login to save your searches.   Differences in the design of each database and interface often require you to customize your search. If you are conducting an actual systematic review that you wish to publish—as opposed to just searching the literature systematically—then you would benefit from consulting with a clinical librarian who will be familiar with the intricacies of building search strategies.     Selecting a database As you can see from the table, Taylor-Robinson et al. searched five databases.50 MEDLINE is probably the most well known of this group. When you search in PubMed, PubMed is searching the MEDLINE database. This is typically a good place to start to find health-related studies. Talk with a research librarian to understand if other databases might be a better choice for your topic.   Generate search terms Once you decide on a database, you need to generate search terms. A good place to start is the keywords published with the sample articles you dig up. You can learn a lot about potential keywords by searching for MeSH terms. MeSH, which stands for “Medical Subject Headings”, is a controlled vocabulary thesaurus that is used to index articles in PubMed. This thesaurus is helpful because there are often many ways to refer to the same phenomenon. For instance, the MeSH term for “breast cancer” is “Breast Neoplasm”. When you search for “breast cancer” in PubMed, the database helps you out by casting a wider net: &quot;breast neoplasms&quot;[MeSH Terms] OR (&quot;breast&quot;[All Fields] AND &quot;neoplasms&quot;[All Fields]) OR &quot;breast neoplasms&quot;[All Fields] OR (&quot;breast&quot;[All Fields] AND &quot;cancer&quot;[All Fields]) OR &quot;breast cancer&quot;[All Fields] Turns out there are a lot of ways that we refer to breast cancer! The following entry terms are indexed by PubMed humans to the MeSH term “breast neoplasms”:  Breast Neoplasm Neoplasm, Breast Neoplasms, Breast Tumors, Breast Breast Tumors Breast Tumor Tumor, Breast Mammary Neoplasms, Human Human Mammary Neoplasm Human Mammary Neoplasms Neoplasm, Human Mammary Neoplasms, Human Mammary Mammary Neoplasm, Human Mammary Carcinoma, Human Carcinoma, Human Mammary Carcinomas, Human Mammary Human Mammary Carcinomas Mammary Carcinomas, Human Human Mammary Carcinoma Breast Cancer Cancer, Breast Cancer of Breast Mammary Cancer Malignant Neoplasm of Breast Malignant Tumor of Breast Breast Carcinoma Cancer of the Breast  Back in the world of worms, the MeSH term for “helminths” is “helminths”, conveniently, and a search for this term in PubMed actually searches: (&quot;parasitology&quot;[Subheading] OR &quot;parasitology&quot;[All Fields] OR &quot;helminths&quot;[All Fields] OR &quot;helminths&quot;[MeSH Terms]) The following entry terms are indexed to the MeSH term “helminths”:  Helminth Worms, Parasitic Parasitic Worms Parasitic Worm Worm, Parasitic Aschelminthes Aschelminthe Gordius Nematomorpha Nematomorphas    Running your search Once you have some initial search terms, it’s time to build and run your query. This will be an iterative process, full of trial and error. You might start with 200,000 results. Some terms and combinations will fail to narrow this field. Others will trim too much.    Figure 2.4: Boolean operators: AND, OR, NOT   You’ll need to know some basic Boolean operators to be an effective searcher: AND, OR, NOT. For instance, let’s consider the search PubMed runs when you enter “helminths”. (&quot;parasitology&quot;[Subheading] OR &quot;parasitology&quot;[All Fields] OR &quot;helminths&quot;[All Fields] OR &quot;helminths&quot;[MeSH Terms]) These four terms are combined with OR, meaning we keep results that match any of these terms. At the time of writing, PubMed returns 230,875 results. If we want to limit the results humans, we enclose our previous search terms in parentheses and add AND &quot;humans&quot;[MeSH Terms] to the end.51 Doing so drops our pool of results to 85,102. The AND operator will always maintain or decrease the number of results. ((&quot;parasitology&quot;[Subheading] OR &quot;parasitology&quot;[All Fields] OR &quot;helminths&quot;[All Fields] OR &quot;helminths&quot;[MeSH Terms])) AND &quot;humans&quot;[MeSH Terms] Alternatively, we could use the NOT operator to limit the results to non-humans. Not surprisingly, we get 145,773 records. (If you are making a surprised face, think about it this way: 145,773 + 85,102 = 230,875 or non-human results + human results = all results) ((&quot;parasitology&quot;[Subheading] OR &quot;parasitology&quot;[All Fields] OR &quot;helminths&quot;[All Fields] OR &quot;helminths&quot;[MeSH Terms])) NOT &quot;humans&quot;[MeSH Terms] Let’s return to our PICO question and use Boolean operators to combine the components.  Among children infected with soil-transmitted intestinal worms, is a single dose of deworming medication delivered through a school-based program more effective than a placebo at increasing child weight?  Here’s what we want to do in plain English:  humans AND children (redundant, but we’ll keep to show the strategy) AND intestinal worms AND deworming medication AND randomized controlled trial AND weight  Within each group, we have several ORs to consider. The parentheses can get confusing, so let’s build the search one line at a time.  ((&quot;child&quot;[MeSH Terms] OR children[Text Word]) AND &quot;humans&quot;[MeSH Terms]) (&quot;parasitology&quot;[Subheading] OR &quot;parasitology&quot;[All Fields] OR &quot;helminths&quot;[All Fields] OR &quot;helminths&quot;[MeSH Terms]) (deworming[All Fields] OR (&quot;albendazole&quot;[MeSH Terms] OR &quot;albendazole&quot;[All Fields]) OR (&quot;mebendazole&quot;[MeSH Terms] OR &quot;mebendazole&quot;[All Fields]) OR (&quot;piperazine&quot;[Supplementary Concept] OR &quot;piperazine&quot;[All Fields]) OR (&quot;levamisole&quot;[MeSH Terms] OR &quot;levamisole&quot;[All Fields]) OR (&quot;pyrantel&quot;[MeSH Terms] OR &quot;pyrantel&quot;[All Fields]) OR (&quot;thiabendazole&quot;[MeSH Terms] OR &quot;thiabendazole&quot;[All Fields])) ((&quot;randomized controlled trial&quot;[Publication Type] OR &quot;randomized controlled trials as topic&quot;[MeSH Terms] OR &quot;randomized controlled trial&quot;[All Fields] OR &quot;randomised controlled trial&quot;[All Fields]) OR (&quot;clinical trial&quot;[Publication Type] OR &quot;clinical trials as topic&quot;[MeSH Terms] OR &quot;clinical trial&quot;[All Fields]) OR ((&quot;random allocation&quot;[MeSH Terms] OR (&quot;random&quot;[All Fields] AND &quot;allocation&quot;[All Fields]) OR &quot;random allocation&quot;[All Fields] OR &quot;randomized&quot;[All Fields]) AND (&quot;Trials&quot;[Journal] OR &quot;trials&quot;[All Fields])))  All together now with ANDs. As I tap out these words on my keyboard, this search returns 259 records in PubMed. ((&quot;child&quot;[MeSH Terms] OR children[Text Word]) AND &quot;humans&quot;[MeSH Terms]) AND (&quot;parasitology&quot;[Subheading] OR &quot;parasitology&quot;[All Fields] OR &quot;helminths&quot;[All Fields] OR &quot;helminths&quot;[MeSH Terms]) AND (deworming[All Fields] OR (&quot;albendazole&quot;[MeSH Terms] OR &quot;albendazole&quot;[All Fields]) OR (&quot;mebendazole&quot;[MeSH Terms] OR &quot;mebendazole&quot;[All Fields]) OR (&quot;piperazine&quot;[Supplementary Concept] OR &quot;piperazine&quot;[All Fields]) OR (&quot;levamisole&quot;[MeSH Terms] OR &quot;levamisole&quot;[All Fields]) OR (&quot;pyrantel&quot;[MeSH Terms] OR &quot;pyrantel&quot;[All Fields]) OR (&quot;thiabendazole&quot;[MeSH Terms] OR &quot;thiabendazole&quot;[All Fields])) AND ((&quot;randomized controlled trial&quot;[Publication Type] OR &quot;randomized controlled trials as topic&quot;[MeSH Terms] OR &quot;randomized controlled trial&quot;[All Fields] OR &quot;randomised controlled trial&quot;[All Fields]) OR (&quot;clinical trial&quot;[Publication Type] OR &quot;clinical trials as topic&quot;[MeSH Terms] OR &quot;clinical trial&quot;[All Fields]) OR ((&quot;random allocation&quot;[MeSH Terms] OR (&quot;random&quot;[All Fields] AND &quot;allocation&quot;[All Fields]) OR &quot;random allocation&quot;[All Fields] OR &quot;randomized&quot;[All Fields]) AND (&quot;Trials&quot;[Journal] OR &quot;trials&quot;[All Fields]))) Once you are satisfied with your results, you could choose to apply the same search to another database. This might be worth the effort if your topic crosses disciplinary boundaries, like economics and health. Best to check with a research librarian.   Screening results Even the best search queries return some rocks alongside the gold, so the next step is screening. We can return to Taylor-Robinson et al. (2015) to see what a thorough approach looks like. You would likely take some shortcuts for a regular literature review. Typically systematic review searches will return hundreds or thousands of potential hits, so a study team will screen titles and abstracts only to exclude obvious mistakes. When beginning this process, it’s common to have team members screen some of the same records to establish reliability, a concept that we will discuss in more depth in the chapter on measurement. Basically, you want to know that everyone screening records would make the same inclusion/exclusion decision. The Taylor-Robinson et al. search strategy only turned up 59 records, and the authors excluded 49 of them after screening the abstracts. The excluded studies did not meet certain pre-defined criteria. For instance, the authors only wanted to include studies using RCTs and quasi-experimental designs. This left the team with 10 studies that required a full-text review. Only 4 of the 10 studies still met eligibility criteria after this review, so these four were added to the bank of 41 studies included in the earlier systematic review.52   Supplemental searches It is customary in a systematic review–and helpful in general reviews—to augment database searches with reference reviews and hand searches to ensure that no key references were missed in the database query. A reference review is nothing more than a scan of an eligible article’s bibliography. In a hand search, you would go to the website of journals that published the eligible articles and scan the tables of contents for each issue published during the search window. If you find that either supplemental method turns up a lot of new results, it could make sense to revise your systematic review search strategy to be more comprehensive.   Extracting data Depending on your objectives you might choose to systematically extract data from each study—key facts related to study design, methods, and results. Or you might take a shorter path and create an annotated bibliography. If you need to be more systematic—an essential requirement for a capital S Systematic review—then you need to design a data extraction strategy. Your PICO research question can be a helpful guide to identifying the minimum data you should extract. Returning to ours:  Among children infected with soil-transmitted intestinal worms, is a single dose of deworming medication delivered through a school-based program more effective than a placebo at increasing child weight?  Some possibilities include:  study setting/population sample size sample demographics, including age range of children study design intervention details, such as specific deworming medication and key points about school-based implementation primary outcome (e.g., weight in kg) effect size  There are numerous software options for storing your extracted data, but you’ll likely find that a simple spreadsheet with rows of studies and columns of study variables will work just fine. Lots of teams use this approach for big systematic reviews, so it will probably serve you well for something more modest.      2.3 For the Love of Everything Holy Use a Reference Manager Even if you chose to ignore everything I’ve written up to this point, do yourself a favor and use a program for managing references. I’m amazed every year when I learn that students on the precipice of graduation manually type and format their in-text citations and bibliographies. What a waste of time! There are several reference managers you might consider. I’ll mention one because it is free and open-source: Zotero. The concept of “free” does not need much explanation, but students often have several free options that are not really free. A good example is a program like EndNote. A university might make this program a free download for enrolled students, but the license expires upon graduation or soon becomes obsolete without a paid upgrade. Additionally, in global health it’s common to work with colleagues who do not have access to a program like EndNote, which makes collaboration challenging. For these reasons, I highly recommend a program like Zotero that is free to use and open to improve. A tutorial is beyond the scope of this chapter, but it’s worth mentioning some features that are common to many reference managers:  Easy importing of references from databases like PubMed. Go from your search results to reference manager in seconds. Automatic retrieval of full-text PDF. Sync PDFs in collection to tablets and phones Connections to word processing software to make inserting references in papers a snap. Automatic creation of bibliographies based on works cited. Push button reformatting of in-text citations and references to different styles, such as APA and Harvard. Shared collections with automatic syncing via the cloud to facilitate collaboration. Easy export of references for migration to just about any other reference manager.  So next time you see someone typing references and complaining about APA formatting, open your laptop and run your reference manager. Pro tip: Be prepared to offer tissues. Your colleague will either cry for joy or deep despair about wasted effort. You’ll feel good about yourself regardless.   Chapter Review and Application   Review Questions     Application     "],
["critical.html", "3 Reading Between the Lines", " 3 Reading Between the Lines  "],
["causeeffect.html", "4 Cause and Effect", " 4 Cause and Effect  "],
["logic.html", "5 Theory of Change 5.1 Overview 5.2 What Makes a Good Theory of Change? 5.3 Other Approaches Chapter Review and Application", " 5 Theory of Change If you run out of small talk at your next mixer for M&amp;E professionals, ask people to tell you about their theory of change. You’ll get an earful. Let’s cover the basics so you too can become the life of the party.  5.1 Overview A theory of change articulates how an intervention—or a policy, program, or treatment—is expected to impact an outcome. It explains how X causes Y, and what is needed for this to happen. You might see this referred to as a logic model, logical framework, causal model, results chain, pipeline model, results framework, program theory, or one of several other combinations of these words. They all pretty much mean the same thing, but don’t claim this at your party. I’ll deny saying it. I believe there are three stages to learning how to develop a theory of change (and logic models, results frameworks, etc):  Smugness: This is so basic. I can’t believe people write books about this. Confusion: (raises hand) Wait, what is the difference between an outcome and and output again? Smugness: That’s an output, not an outcome. How do you expect your program to work if you can’t articulate a clear theory of change?  Let’s get started!   5.2 What Makes a Good Theory of Change? The first time you encounter a program’s theory of change, it will most likely be in the form of a graphic. This is not always a good thing. Just ask retired Army general Stanley McChrystal.    Figure 5.1: Source: PA Consulting Group, via MSNBC, http://bit.ly/1gRbouw   When he saw this PowerPoint slide about the American military strategy in Afghanistan during a briefing in Kabul in 2009, Gen. McChrystal remarked, “When we understand that slide, we’ll have won the war.”53 And that is how it goes with theories of change. If your model is grounded in the evidence, testable, and clearly articulated, you might actually make an impact. If not, you lose. Let’s look at the theory of change for another USG effort to solve a protracted problem: hunger and poverty.  A plan to end hunger and poverty In 2009, President Obama pledged $3.5 billon dollars over three years to a global hunger and food security initiative that came to be called Feed the Future. FTF is said to take a “whole of government” approach. USAID and a coalition of 10 other federal agencies work in partnership with 19 focus countries, including Kenya, to promote agricultural development and end poverty and hunger. The 2010-15 strategy document for the FTF portfolio in Kenya exemplifies how large donors like the USG use theories of change in support of global health and development initiatives.54 Here is how the theory of change is explained in the text:  To generate the economic growth needed to reduce poverty and hunger and to achieve the GOK’s vision of a commercial and modern agricultural sector, FTF in Kenya will invest in transforming agriculture through improved competitiveness of high-potential value chains and the promotion of diversification into higher-return, on- and off-farm activities. As documented by IFPRI, the development of selected value chains will have multiplier effects that spawn off- and non-farm employment opportunities.    This is known as development speak. One of the hallmarks of development speak is that you reach the end of the paragraph, nod in approval, and then say “wait, what?”, suddenly realizing you know less than you did when you started the paragraph.   This diagram (more) clearly depicts what the report prose struggles to do:    Figure 5.2: FTF Theory of Change. Source: Feed the Future, http://1.usa.gov/1NDuQpn     Core components Theory of change diagrams come in a variety of flavors. There is no RIGHT WAY™ to create one, as long as you can convey the fundamentals of how X leads to Y. If people understand your diagram, it is a good diagram. People are more likely to understand your diagram and your theory of change if you include a few common elements. The United Kingdom’s Department for International Development, commonly known as DFID or UK Aid, commissioned a report on the uses of theories of change in international development that identified several common components:55  influence of context discussion of long-term change process/sequence of change explained underlying assumptions presented as a diagram and narrative summary  Going by this list, the FTF diagram could be improved by the addition of contextual factors and underlying factors.  Templates are your friend My preferred approach to outlining a theory of change is to follow this template from the W.K. Kellogg Foundation.56 Here is an editable PowerPoint version.    Figure 5.3: Theory of change template. Source: W.K. Kellogg Foundation, http://bit.ly/1My75Ay   Start with the (research) problem statement (Box 1). This box gets at the heart of the reason your intervention exists. What are you trying to solve? You’d be surprised how often program design is disconnected from the problem that led to the program in the first place. This planning process will help you prevent you from ending up in this situation. Next think about what assets already exist and what needs remain (Box 2). There is always something to build upon, which is why it’s important to look for strengths in addition to challenges. This process is best done in collaboration with people impacted by the problem so that the proposed solution is grounded in their reality. If available, descriptive data sources like the DHS can help to outline (1) and (2). For a more local perspective, it might be beneficial to conduct a brief needs assessment in partnership with the local community if resources permit. Box 3 jumps to the desired results. If the program works, what will change? We’ll come back to what is meant by “outputs, outcomes, and impact” in a moment. With the results articulated, you can go back to the beginning and consider what factors might affect your program’s success positively or negatively (Box 4). The next step is to outline strategies for achieving your desired results that take into account potential barriers and facilitators (Box 5). This is where you articulate what your program will actually do. Finally, Box 6 reminds you that every theory of change is built on a set of assumptions. Be thorough and transparent as you think through the hidden beliefs that underlie your ideas about how your program will achieve its results.  Example: Kenyan “Sugar Daddies” In 2014, an estimated 1.4 million people in Kenya were living with HIV. This is a prevalence rate of 5.3% among adults aged 15 to 49. Without a cure for AIDS, prevention remains critical to ending the epidemic. Starting in 2001, Kenya integrated HIV/AIDS education into the primary school curriculum as a new prevention strategy.57 At the time, the focus of this program—and many others programs across sub-Saharan Africa—was complete risk avoidance.58 Abstinence. Information on risk reduction was limited. In particular, students were not learning about the differential prevalence of HIV infection by age and gender. Girls were not learn that the older “sugar daddies” who provide nice things like phones and airtime in return for sex are more likely than the girls’ goofy age mates to be infected. An organization called ICS Africa set out to change this by rolling out a “Relative Risk Information Campaign” in Kenya. The intervention was brilliant in its simplicity. A program staffer would talk with students for 40 minutes. During this time, the staffer showed the class a 10-minute video on sugar daddies and led a discussion about cross-generational sex. During the session, the staffer reviewed results of recent studies and wrote facts about the distribution of HIV prevalence on the chalk board. JPAL researchers tested ICS Africa’s risk reduction strategy in a randomized experiment in Western Kenya. In the first phase (2003), 328 schools were randomized to teacher training on the national HIV prevention curriculum.59 In the second phase (2004), 71 of these schools were stratified and randomized to receive the sugar daddy intervention.60 In total there were four study arms: (i) teacher training only, (ii) sugar daddy only, (iii) teacher training and sugar daddy, and (iv) nothing. The results were shocking. Teacher training was a bust. Sure, the training led to a change in teaching practices—notably that trained teachers mentioned HIV in class more often than non-trained teachers—but it had little effect on HIV knowledge or childbearing.   Increasing knowledge about HIV makes intuitive sense as an outcome for a study about HIV prevention. But why childbearing?   Well, babies don’t come from storks. They come from unprotected sex. It’s harder to lie about birthing a baby than it is to lie about your private sexual behavior, so childbearing is thought to be a more objective measure of unprotected sex. Unprotected sex is a main driver of HIV transmission, so measuring childbearing is a proxy for HIV risk from unprotected sex.   In contrast, the 40-minute sugar daddy discussion and video reduced childbearing with men at least five years older by 65%—and not because girls started having babies with males their own age. The overall incidence of childbearing fell by 28%. With a cost of $28.20 USD per school and $0.80 per student, the cost per childbirth averted was $91.61  Returning to our template, let’s piece together the theory of change for the relative risk reduction program.    Figure 5.4: Sugar daddy awareness theory of change        5.3 Other Approaches While a theory of change tends to be a high-level depiction of the “why”, other approaches are more detailed and focus on the “how”.  Results framework Donors like USAID commonly require project staff to create results frameworks to monitor progress toward achieving the stated goals. Here’s an example results framework from the Feed the Future strategic plan for Kenya.    Figure 5.5: FTF Results Framework. Source: Feed the Future, http://1.usa.gov/1NDuQpn   You can read this diagram from the top down. The overall goal reflects the mission of the broader FTF program: reduce poverty and hunger. The plan for attaining this goal is to reach two objectives: inclusive agricultural sector growth and improved nutritional status. If the goal represents the ultimate desired impact, objectives are long range strategies. You know you are on track to reaching your objectives if you hit several intermediate results, or IRs. For instance, IR6, “Improved utilization of MCH and nutrition services”, is expected to improve the nutritional status of women and children. This IR has three sub IRs, including 6.3 “Strengthened MCH nutrition surveillance”. The rationale for sub-IR 6.3 is that better monitoring and data will enable earlier identification of at-risk individuals, which in turn should mean earlier initiation of nutrition interventions. Every IR has a set of indicators for measuring progress. In the case of IR6, FTF lists the following illustrative indicators:  prevalence of maternal anemia number of children under five years of age who received vitamin A from USG-supported programs number of people trained in child health and nutrition through USG-supported health area programs (disaggregated by gender) number of clients who received food and/or nutrition services (PEPFAR indicator)    Logic models DFID requires grantees to prepare logic models—or logframes as they like to say—for planning, monitoring program implementation, and program evaluation and reporting.62 Logic models take the “results chain” or “pipeline” approach shown below.    Figure 5.6: Logic model. Source: W.K. Kellogg Foundation, http://bit.ly/1My75Ay   In a logic model, inputs and activities represent your planned work. Outputs, outcomes, and impact are your intended results. Inputs: The resources needed to implement the program. People, money, time, etc. Activities: What your program will do. Trainings, events, distribution of goods, etc. Outputs: What your program did. Number of people trained, number of events held, number of goods delivered and number of people who benefitted. Outcomes: Short- and medium-term results of your program. Increased knowledge, decreased risky behavior, improved functioning, etc. Impacts: Long-term effects of the program outcomes. Lower HIV prevalence, reduced morbidity and mortality, etc.  Here’s what a logic model might look like for the Kenyan relative risk reduction program.    Figure 5.7: Sugar daddy awareness logic model      Chapter Review and Application   Review Questions     Application     "],
["measurement.html", "6 Measure Twice, Cut Once 6.1 What is an Indicator? 6.2 Selecting Good Indicators 6.3 Evaluating Data Collection Instruments and Methods Chapter Review and Application", " 6 Measure Twice, Cut Once    Figure 6.1: Source: http://bit.ly/1X5yjU2   This proverb, well known to carpenters everywhere, should be the motto of every aspiring researcher. Ignore it and you run the risk of severely limiting what you can learn from your study—or ruining the whole thing altogether. Despite its importance, however, measurement rarely gets the attention it deserves. One reason, I think, is that to really understand measurement issues in a particular sub-field you need to get deep into the weeds. It’s relatively easy to pick up a journal article outside of your major area of study and find something to say about the study design. Not so when it comes to the measurement. Take an issue like maternal mortality. Dead or alive. Easy, right? Not quite. What counts as a maternal death? What if a pregnant woman slips on a patch of ice and dies? Is that a maternal death? What if she dies from an infection 40 days after giving birth to a healthy baby girl? Is that a maternal death? No and yes. I’ll explain why. In the process, I’ll introduce some key measurement issues to consider when designing or reviewing a study. I’ll also introduce you to some population-level indicators that every global health data junkie should know.  6.1 What is an Indicator? Part of developing a good logic model is to identify indicators for each part of the causal chain.63 An indicator is an observable measure of a concept.64 For instance, a primary outcome in the Kenya “sugar daddy” intervention study discussed in Chapter 3 was “reduction in unprotected sex”. We can’t observe unprotected sex directly, so it is a concept in need of a measurable indicator. That indicator could be self-reported unprotected sex, but it might be hard to get people to report honestly. A potentially more accurate measure of unprotected sex among girls of reproductive age is the incidence of childbirth.  Let’s look at another example and consider the importance of measuring indicators at each step of the causal chain.    Logic Model Concept Indicator     Input Financial resources Expenditures   Activities Treatment fidelity Checklist score   Output Compliance # TBAs trained   Outcomes Fewer complications # hemorrhage   Impacts Reduced mortality # maternal death    Our example logic model comes from a study by Jokhio and colleagues (2005) of the impact of training traditional birth attendants on stillbirths, neonatal deaths, and maternal deaths.65 When this study was conducted in 1998, the maternal mortality ratio in Pakistan was more than 280 per 100,000 live births (compared to 170 in 2013).66 This represents more than 13,000 women who died giving birth. The tragedy in this number is that maternal mortality is preventable. Women continue to die before, during, and after childbirth because of three major delays: (i) delays in seeking healthcare; (ii) delays in reaching healthcare; and (iii) delays in receiving high quality healthcare upon reaching a medical facility.67 The most common proximate causes of death are postpartum hemorrhage, hypertensive disorders, and sepsis.68 It’s hard to predict who will suffer life threatening complications, so there is debate about how to best intervene. Some argue that we should invest in interventions to increase facility deliveries so that women would be better able to access emergency obstetric care if needed. Others point to the low frequency of complications and the high prevalence of poor quality care at medical facilities as a reason that women should be supported in their decision to stay at home where the family can better support them. At the time Jokhio and colleagues conducted this study, facility deliveries in Pakistan were rare. The authors cited descriptive studies showing that most women in Pakistan deliver at home (80%) without a skilled birth attendant (89%). Only 1 out of 20 women who experience complications makes it to a medical facility with emergency obstetric care. Given the high prevalence of home deliveries, Jokhio et al. tested an intervention that trained traditional birth attendants—the mothers, aunts, sisters, and neighbors who already help women through childbirth but have no formal medical training—and provided them with delivery kits. The training lasted 3 days and covered topics such as checking for danger signs, conducing a clean delivery using the provided kits, and making emergency referrals. Trained TBAs were asked to visit with women at least three times during the pregnancy to look for danger signs, and then again for the delivery. The study design was a cluster randomized trial. Seven subdisticts of a rural district were randomized to the TBA intervention or the standard of care. TBAs in subdistricts assigned to the control group did not receive any training or delivery kits. The primary endpoint was reduced perinatal deaths (stillbirths and neonatal deaths) and maternal deaths. Before discussing the study results, let’s think about measurement throughout the causal chain.  Process indicators M&amp;E managers think long and hard about how to track inputs, activities, and outputs—process or monitoring indicators—and how to summarize the data in quarterly monitoring reports to donors or NGO/ministry supervisors. All of these stakeholders want to know how much was spent and for what purpose, whether the implementation plans were followed, and how many people benefitted. As researchers we also care about collecting good process data, but usually as a means to developing a better understanding why programs do or don’t work. Examples include program costs (to estimate cost-effectiveness), treatment fidelity (to judge the quality of implementation), and compliance with study protocols (to assess threats to internal validity).  Inputs  Cost and cost-effectiveness Impact evaluations produce estimates of the effectiveness of a program or intervention. Does the program “work”? For some public health and behavioral health nerds, evidence of impact is enough because they are narrowly focused on developing and testing new interventions. Not true for many economists and policymakers who are thinking about delivering programs at scale with limited public funding; they want to know whether the intervention is cost-effective, not just effective.69 A cost-effectiveness analysis requires close tracking of the cost of all program inputs.70 Unfortunately, Jokhio et al. don’t tell us about the cost, other than to say the intervention was designed to be “low cost and sustainable.”71   Cost effectiveness analysis (CEA) is not the same thing as cost-benefit analysis (CBA). The latter involves putting a dollar amount to benefits, such as the benefit to society of averting a maternal death. This is hard to do and quite subjective. This is why you will come across CEA much more frequently in global health. The typical calculation is the effect size divided by the costs. There is no effort to put costs and effects on the same dollar metric.      Activities/Outputs  Treatment fidelity The TBA intervention included a 3-day training and the provision of delivery kits. If a trainer decided to skip portions of the training or knock off a day, the program would not be delivered as intended (aka, without fidelity to the treatment). The consequence of low treatment fidelity is usually an attenuation of treatment effects.72 This is a threat to internal validity. If the study shows no effect but treatment fidelity is low, we can’t be confident in the null result. Implementation failure rather than theory or program failure could be to blame. Low fidelity is also a threat to external validity because it isn’t possible to truly replicate the study. The first step in developing a measure of fidelity is to identify what constitutes the intervention. Then for each component of the intervention, you define the benchmark for success. If your intervention involves the distribution of goods such as delivery kits, this is relatively straightforward: count the kits delivered. When your intervention involves the delivery of a service, fidelity tracking is more complicated. Training is an example of a service. In the write-up of the TBA study, we’re told that TBAs were trained on the following topics:  giving advice on antepartum, intrapartum, and postpartum care; how to conduct a clean delivery; how to use the disposable delivery kit; when to refer women for emergency obstetrical care; and care of the newborn.  There are two levels of training we can evaluate: the training of the trainers (TOT) and the competency of the trainees. First, we want to make sure that the people leading the TBA training (i) adhere to the training manual (“do the facilitators deliver all of the training content?) and (ii) correctly present all of the training material (”do they present the training material correctly?“). We might do this by creating a TOT fidelity checklist that can be used to assess adherence and competence.73 For instance:  Adherence  trainer reviews didactic material on conducting a clean delivery trainer facilitates the demonstration activity on how to use the disposable delivery kit  Competence  trainer correctly explains the key danger signs to look for during the antepartum period trainer demonstrates proper use of the delivery kits   Second, we want to know that the TBAs learned something from the training. This could be as simple as administering a knowledge and practical test at the end of the training. To continually monitor fidelity over time, we could arrange to observe or record TBA interactions with patients and evaluate fidelity with another checklist. Depending on the situation, we could also use “mystery clients”—confederates who pose as real clients—to evaluate fidelity without the target’s knowledge.   Treatment compliance In a randomized trial, people (or other units like schools and clinics) are randomly assigned to different study arms. In Jokhio et al., subdistricts were randomly assigned to the TBA intervention group (treatment) or the no program group (control). Sometimes plans don’t work out and people or units are “non-compliant” with these assignments. For instance:  Some people assigned to the treatment group are never treated or only partially treated. Maybe some TBAs never showed up for the training program or stopped attending before the end. Some people assigned to the control (comparison) group receive the treatment despite their assignment to the inactive control group. An example would be if TBAs in control subdistricts heard about the nearby training and decided to attend.  Just like low-treatment fidelity, partial compliance with the study protocol can attenuate treatment effects. We’ll discuss these threats more in the chapter on experimental designs. The take home point for now is that close tracking of outputs can be critical for your analysis of treatment effects.     Outcome/impact indicators Researchers tend to spend the most time and effort defining measurement strategies for study outcomes and impacts. Jokhio et al. evaluated the effect of the TBA intervention on a set of outcomes that included the occurrence of major complications of pregnancy and referral to emergency obstetric care. The authors also examined the impact of the intervention on maternal mortality, but the study only had the statistical power—the topic of a later chapter—to detect a 90% drop in maternal deaths. Thus the authors were unlikely to find a statistically significant effect on maternal deaths, but the indictor made sense in the theory of change: better equipped TBAs who are present at deliveries will reduce the incidence of major complications that are most likely to lead to maternal deaths; better access to emergency referrals will have the same life saving effect.   So what did they find?   The intervention reduced the odds of hemorrhage and sepsis by 39% and 83%, respectively. Women exposed to trained TBAs were also 1.5 times more likely to get a referral to emergency obstetric care.74 Obstructed labor was more likely among the intervention group, but this can’t be caused by TBAs. So how do we interpret this finding given that the study design was an experiment, and we know experiments are made for causal inference? Increased reporting. TBAs were trained to recognize danger signs and make referrals. It’s likely that trained TBAs were more likely to see something and say something.   So what about maternal mortality? The effect size estimate suggested a 26% reduction in maternal deaths, but the estimate was not very precise. The study was only designed to detect a reduction of 90% with precision, so this is not surprising. So no definitive evidence about the impact of the intervention on maternal mortality, but large effects on the intermediate outcomes supports the underlying theory of change.      6.2 Selecting Good Indicators There are some people who say that indicators must be SMART™:    Secific clearly defined   Measurable able to be quantified   Attainable target must be realistic   Relevant related to the construct   Time-bound observed at specific times    I think indicators should be DREAMY™:    Defined clearly specified   Relevant related to the construct   Expedient feasible to obtain   Accurate valid measure of construct   Measurable able to be quantified   customarY75 recognized standard     Dreamy indicators™ Jokhio and colleagues wanted to develop and test an intervention that would make motherhood safer. The ultimate indicator of safe motherhood is reduced maternal mortality. Let’s put this indicator to the DREAMY test.  Defined The first step to becoming a good indicator is having a clear definition. Jokhio et al. used the following definition of a maternal death:  Maternal deaths were defined as death of the mother during pregnancy, delivery, and up to six weeks post-partum, excluding deaths known to have been due to injury or accident.    Relevant Maternal deaths are clearly relevant to the construct of safe motherhood.   Expedient Data on maternal deaths are not easy to collect. Jokhio et al. relied on Lady Health Workers, a cadre of educated women with 3 to 6 months of training in primary health care and family planning who are an official part of Pakistan’s public health system. Despite the challenges of collecting data on maternal mortality, the Lady Health Workers were surprisingly effective; they successfully followed up with 99.8% of the 19,557 women recruited into the study.   Accurate Simply, yes. Maternal deaths measure what we think they measure—mortality.   Measurable Not such a simple yes. Maternal deaths can be quantified, but without a registry of death certificates indicating the cause of death, it’s easy to make mistakes. So what did Jokhio et al. do?  In cases of maternal death, the cause was ascertained by Lady Health Workers on the basis of oral reports from relatives, neighbors, or traditional birth attendants.  The authors do not use the term “verbal autopsy”, but that is the inferred method. In this particular study, a VA is a reasonable choice because the timing of death was ascertainable for all women. It is possible that some deaths due to accident or injury were incorrectly labeled as maternal deaths, but there is no way to know.   Measurement can change over time, and these changes can impact your analysis. For instance, the U.S. revised the U.S. Standard Certificate of Death in 2003 to include “late maternal deaths” (43 to 365 days) and “pregnancy-related deaths” to better align with the ICD-10 criteria. At the time, only 21 of 50 states recorded a woman’s pregnancy status; those that did used 1 of 6 different questions.76     CDC, 2003; http://1.usa.gov/1UhYl7v      customarY Jokhio et al.’s definition of maternal mortality will look familiar to folks who study reproductive health issues because it largely follows the authoritative definition that comes from a set of classification standards known as the “International Statistical Classification of Diseases and Related Health Problems”, or ICD. According to the latest version, the ICD-10:77  A maternal death is the death of a woman while pregnant or within 42 days of termination of pregnancy, irrespective of the duration and the site of the pregnancy, from any cause related to or aggravated by the pregnancy or its management, but not from accidental or incidental causes.78     Use international standards Whenever possible, you should seek out standard indicators and follow existing definitions and calculation methods. The WHO publishes a Global Reference List of the 100 core health indicators.79 The 2015 version categorizes the indicators several ways:  domain  health status risk factors service coverage health systems (service delivery, quality of care, health financing, essential medicines, the health workforce, health information)  subdomains  communicable diseases (HIV/AIDS, sexually transmitted infections, tuberculosis, malaria, neglected tropical diseases, outbreaks, epidemic diseases) reproductive, maternal, newborn, child and adolescent health (including sexual health and reproductive rights and immunization), noncommunicable diseases (including chronic disease, health promotion, nutrition, mental health and substance abuse) injuries and violence and the environment  levels of the results chain framework  Here is the full list of indicators organized by steps in a logic model.    Figure 6.2: Source: http://bit.ly/1NgGeLh   In the entry for maternal mortality ratio, we find the following definition:  The annual number of female deaths from any cause related to or aggravated by pregnancy or its management (excluding accidental or incidental causes) during pregnancy and childbirth or within 42 days of termination of pregnancy, irrespective of the duration and site of the pregnancy, expressed per 100 000 live births, for a specified time period.  The calculation of the MMR is specified in this entry, as is the method of measurement and estimation.    6.3 Evaluating Data Collection Instruments and Methods Sometimes it’s possible to use existing data sources, such as administrative or medical records.80 It’s often the case, however, that administrative records are incomplete, unreliable, and unsuitable for constructing your specific outcome of interest—especially in developing countries. In all likelihood, especially in the case of program evaluations, part or all of your data will come from original data collection efforts. Potentially the most common forms of data collection in global health are interviews and surveys. Some less commonly used non-survey instruments include:81  direct observation random spot checks mystery clients incognito enumerators physical tests biomarkers mechanical tracking devices spatial demography participatory resource appraisals observe purchasing decisions games standardized tests vignettes   Psychometrics In some fields such as psychology, it is common to measure outcomes by administering self-reported instruments to participants and calculating a score from their responses. Whether you are using an off-the-shelf instrument or developing your own, you want to be sure that the instrument is a reliable and valid measure of your indicator. A reliable measure is one that is capable of producing the same results when measuring the same thing repeatedly. Imagine your bathroom scale. If you stepped on, then off, then on again and the scale read 210 lbs and then 185 lbs, you would realize that you are the owner of a broken, unreliable scale. Undeterred, you head to the store and pick up a new scale—this time one that posts your weight to Facebook to keep you “accountable”. So you step on, off, and back on again. The scale reads 400 lbs then 402 lbs. As your Facebook wall updates, you whack the scale and get back on again. It reads 399. Your mom posts a comment congratulating you on dropping out of the 400s and 50 people like it. Meanwhile, you call customer support and explain that your scale is very reliable (precise), but also very inaccurate. You actually tell the customer service rep that the scale is an “invalid measure of your current weight”. He’s heard it before, but it’s actually true in your case. Rule #1: Don’t get a scale that posts to Facebook. Rule #2: An instrument can be reliable but not valid. Rule #3: But an instrument cannot be valid without being reliable.  Reliability There are several methods for assessing the reliability of an instrument. Let’s imagine that you developed a new 8-item measure of anxiety in which people have the following response options: (0) never, (1) rarely, (2) sometimes, or (3) often. This means that possible scores could range from 0 to 24.  Test-retest reliability This is when scores are correlated on repeat administrations of the instrument. Participants complete your survey today and then again in a week. If each person’s score is the exactly the same the second time, your instrument would be perfectly reliable. It won’t be, but you’ll hope for a high correlation coefficient (conventionally higher than 0.70). For instance, let’s say we survey 50 people and enter their data. Here are the responses for the first 6 people in the dataset. Each row consists of data for 1 person. The last column is the sum of the the person’s responses to the instrument’s 8 items. &gt; head(datT1)   q1 q2 q3 q4 q5 q6 q7 q8 t1.sum 1  1  1  1  0  2  2  2  2     11 2  2  2  2  1  3  2  2  3     17 3  3  3  3  0  2  3  3  3     20 4  2  3  3  0  2  2  2  2     16 5  2  3  3  1  3  3  2  3     20 6  2  2  2  0  3  3  2  2     16 Then we go back and survey these same people again and enter their data once more. Here’s the data for the first 6 people in the re-test dataset. &gt; head(datT2)   q1 q2 q3 q4 q5 q6 q7 q8 t2.sum 1  0  1  0  0  0  3  2  3      9 2  3  1  3  2  3  3  3  3     21 3  3  3  2  2  3  1  3  3     20 4  3  3  3  1  2  3  2  3     20 5  3  3  3  3  3  3  2  3     23 6  3  3  3  1  3  2  2  2     19 To calculate the test-retest reliability, we’ll correlate the sum scores for each administration: t1.sum and t2.sum. We find a high correlation of 0.78, suggesting that the instrument has good test-retest reliability. &gt; cor(datT1$t1.sum, datT2$t2.sum) [1] 0.7753866   Interitem reliability This is when responses to items in your instrument are consistent. If not, you have to wonder if they are measuring the same underlying construct of anxiety. Item total correlation: One approach to finding ugly ducklings in your instrument is to calculate item-total correlations. It’s easy: correlate scores on each item with the total scores. Generally item-total correlations exceeding 0.30 are sufficient. Returning to our dataset from the first administration, we see that most items have an acceptable item-total correlation, except q4. This item seems to be measuring something different than the others. Maybe the rest are clearly tapping into the construct of anxiety, whereas q4 asks people about how often they feel sad. Maybe it would fit better in an instrument assessing depression. &gt; library(multilevel) &gt; item.total(datT1[,1:8])   Variable Item.Total Alpha.Without  N 1       q1  0.6021512     0.8033555 50 2       q2  0.5914340     0.8044035 50 3       q3  0.7402437     0.7808027 50 4       q4  0.1452794     0.8458352 50 5       q5  0.5295935     0.8127216 50 6       q6  0.5636561     0.8081818 50 7       q7  0.5369509     0.8117495 50 8       q8  0.6430992     0.7967467 50 Cronbach’s alpha: Instruments are imperfect. Even your new anxiety instrument that you worked so hard to create. Flawed. The result is that every person’s observed score is actually a function of their ‘true’ score (which we can’t know) plus some amount of measurement error. Cronbach’s alpha gives you an estimate of how much variance in people’s scores on your instrument is measurement error. When you calculate Cronbach’s alpha in a program like R, behind the scenes R does the equivalent of splitting the dataset into two halves over and over again and calculating the correlation between total scores for the first half with total scores for the second half. Cronbach’s alpha is the average of all possible correlation coefficients. If alpha is 0.70—a rough guide for the low-end of acceptable—it means that only 30% of the variance in scores is measurement error. In our sample dataset, alpha is 0.81, meaning that 19% of the variance is error. &gt; library(psych) &gt; a &lt;- alpha(datT1[,1:8]) &gt; a[[1]][2]  std.alpha   0.814491   Interrater reliability This is another type of reliability that indicates whether two observers are consistent in their ratings of the same thing. Instead of using an 8-item self-report instrument, we might want to have two observers watch a video of a parent and child interacting and rate each behavior as “warm” or “cold”. If the observers agree a lot, they would be reliable. Percent agreement is one way to measure interrater reliability, but it’s not the best. It does not account for agreement that can happen by chance. Cohen’s kappa coefficient does, and you generally want a value greater than 0.40.    Validity Does your measure measure what you intend to measure with your measure? (unrelated bonus: how many metrics could your measure measure if your measure could measure metrics?). In other, less annoying words: Does your 8-item anxiety instrument actually measure this thing called anxiety? If not, it’s not a valid measure of anxiety. There are several types of validity that you can use to determine whether your instrument is valid.  Face validity This is the weakest form of validity. The basic idea is: if it looks like a duck, quacks like a duck, it’s a duck. If people think your anxiety instrument asks about anxiety, it has face validity as a measure of anxiety. This is a weak standard, however. A great looking instrument can perform very poorly in practice, and an instrument that appears to lack face validity might perform very well. The bottom line is that if you read an article and the only mention of validity is face validity, it’s a lame duck.   Construct validity Anxiety is a hypothetical construct. If your 8-item anxiety instrument has construct validity, it will be related to other instruments that are also thought to measure anxiety (convergent validity), but it will not be related to other instruments that have nothing to do with anxiety (discriminant validity). For instance, your anxiety instrument would have convergent validity if participant scores on your measure are correlated with their scores on an existing measure like the Multidimensional Anxiety Scale for Children (MASC). Your instrument would have discriminant validity if scores were not strongly correlated with scores on a measure of narcissism. If your instrument has both convergent and discriminant validity, you have more confidence that it measures the construct of anxiety that you think it measures.   Criterion-related validity An even more robust form of validity is criterion validity. Do scores on your 8-item anxiety instrument correctly predict which participants will be diagnosed as suffering from an anxiety disorder when evaluated independently by a psychologist or psychiatrist? In this case, the mental health professional’s judgement—made completely independent of the data generated by your instrument—is the “gold standard”. A valid instrument will produce accurate predictions. This is easier to understand with a figure.    Figure 6.3: Confusion matrix   Your anxiety instrument has a possible range in scores from 0 to 24. Let’s say we have reason to believe that the cutoff for anxiety is a score of 15. Anyone with a result greater than or equal to 15 will be classified by your instrument as “anxious”. Everyone else is classified as “not anxious”. These are the two possible test results. There are also two possible diagnostic results from the gold standard mental health professional: has anxiety and doesn’t have anxiety. Taken together, there are four possible combinations of test and gold standard results:  true positive (a): both the test result and the gold standard indicate that the person is anxious true negative (d): both the test result and the gold standard indicate that the person is NOT anxious false positive (b): the test result suggests anxiety, but the gold standard disagrees false negative (c): the test result suggest no anxiety, but the gold standard disagrees  The true positive rate is known as sensitivity. It is calculated as follows: true positive / all positives = a / (a + c)    Figure 6.4: Sensitivity   30 / (30 + 3) = 91% 91% of patients with anxiety correctly screened positive. 9% of patients did not (false negative rate). The true negative rate is known as specificity. It is calculated as follows: true negatives / all negatives = d / (b + d)    Figure 6.5: Specificity   10 / (5 + 10) = 67% 67% of patients with no anxiety correctly screened negative. 33% incorrectly screened positive (false positive rate). There is often a tradeoff between sensitivity and specificity. The tradeoff can be visualized with a receiver operating characteristic (ROC) analysis. ROC curves plot sensitivity (the true positive rate) against 1 minus specificity for a range of different cutoff points. Earlier I mentioned that the cutoff score for your anxiety measure was 15. To determine if 15 is the best cut point, we could calculate the sensitivity and specificity again for several different cutoff points and plot the relationship in a ROC curve. For instance, Rathore et al. (2014) evaluated different cut scores for the Patient Health Questionnaire 9 (PHQ-9) when administered to adults with epilepsy and displayed the results on a ROC curve.82    Figure 6.6: ROC. Source: http://1.usa.gov/1EHevjO   The area under the ROC curve was 0.90, generally considered to be a very accurate benchmark. It appears that a cut score of 12 on the PHQ-9 balances sensitivity and specificity. Decreasing false positives (higher specificity) could be favored to avoid labeling non-depressed patients as depressed and using resources for unnecessary additional evaluations. Doing so would mean missing more true positives, but it could be a defensible tradeoff.     To use, adapt, or develop? When you design a new study that includes new data collection, you have to decide whether to use or adapt existing instruments that measure your indicators, or to develop your own. There are pros and cons to each approach. There is a good chance that the off-the-shelf instrument you want to use in Country Z was developed in Country A and never used or at least never validated in Country Z. Using it would enable some cross-cultural and cross-national comparisons, but without running your own validity study first, you don’t have much confidence that the instrument is valid or reliable in Country Z. An alternative approach is to start with the off-the-shelf measure and adapt it for your needs. You translate it from one language to another, and you have someone else translate it back to the original language before you and your colleagues attempt to resolve discrepancies. Maybe you conduct some formative qualitative research to determine if there are some constructs not represented in the existing instrument that you could add. Or you pilot test the existing instrument and ask people to explain to you their understanding of the questions and response choices (cognitive interviewing). Then you ask yourself these six questions about cross-cultural validity:83  What is the purpose of the instrument? What is the construct to be measured? What are the contents of the construct? What are the idioms used to identify psychological symptoms and behaviors? How should questions and responses be structured? What does a score on the instrument mean?  Or at the other end of the continuum, you believe that your instrument should be completely grounded in the lived experience of where you are working in Country Z, so you start from scratch with formative qualitative work. It is only through this work that a new instrument emerges.84    Chapter Review and Application   Review Questions     Application     "],
["sampling.html", "7 Duck, Duck, Research Participant 7.1 Probability Sampling 7.2 Non-Probability Sampling Chapter Review and Application", " 7 Duck, Duck, Research Participant Now that you have a good, evidence-based research question and a primary outcome that you can define and measure, you are ready to think about the subjects of your research. Most often your research subjects—participants—will be people or places. You have two basic questions to answer about these participants:  How will you find, recruit, and select them? (sampling) How many do you need to include to meet your study objectives? (power)  We’ll tackle both questions in this chapter.  There are two general approaches to sampling: probability and non-probability sampling. The main difference is that in a probability sample, every unit (e.g., person) in the population has a knowable, non-zero probability of selection. Most research in global health does NOT use probability sampling.85 There’s a good chance that you won’t either, unless you’re trying to accurately estimate some characteristic of the population. For instance, you want to estimate the prevalence of depression among pregnant women in Thailand. To get an unbiased answer to this question, you’ll need a probability sample. These types of epidemiological/descriptive questions are critical, of course, but most research examines the relationships between variables rather than prevalence, and probability samples are not essential.  7.1 Probability Sampling While you may not ever have a need to organize a probability sample, it’s important to know the mechanics since you will consume and evaluate descriptive research. In global health, the most common application of probability sampling comes from nationally representative surveys like the DHS, which we discovered in Chapter 1. The sampling strategy in a DHS survey is complex, so we will come to it in a moment. First, let’s review basic principles and approaches.  Sampling error Returning to an example from Chapter 1, insecticide treated nets (ITNs) can prevent malaria transmission and save lives, especially the lives of young children living in endemic regions. So a policymaker in a country like Kenya, with its high rates of malaria, might want to know what percentage of her country’s more than 7 million children younger than 5 years of age sleep under an ITN. Let’s start with the unreasonable and assume that we’re able to survey every caregiver about every child and that we do so without any measurement bias.86 If this were possible, the results would reflect the “true” level of ITN use among the population of more than 7 million Kenyan children under the age of 5. This would of course be nearly impossible to do in practice, but more importantly, it would be completely unnecessary. Rather than try to survey everyone, we can recruit a representative sample—a subset of the population that reflects the population. This subset, however, will not be a perfect reflection of the population. Of course the closer our sample gets in size to the population, the more it will reflect the population, but it will always be imperfect because of sampling error. This is because our sample, however we obtain it, is just one possible draw of all possible samples. Whether the sample is 0.09% of the population or 90% of the population, it is still one possible subset of all possible subsets. Both the 0.09% and 90% samples will produce results that differ from each other and the population. For instance, let’s imagine that we have a population of 40,000 adults ages 18 to 65. We want to estimate some characteristic of the population, so we plan to survey a sample of 100 people. For the sake of this example, we’ll assume that we can randomly select 100 people from the population, so everyone has a 100/40000 chance of being selected. The image below shows three possible draws of 100 people from this population.    Figure 7.1: Three possible draws from the population. Each circle in this illustration represents a person, and the circles are different sizes and colors to remind us that our population is made up of different types of people.   Each draw from the population is unique, but they are alike in the fact that each sample is subject to sampling error because it is not a perfect reflection of the population. We can’t know the true population mean in reality, but let’s break the rules and assume that we can for this example. As shown in the generic example below, the “true” population mean is 100. Notice how each sample of 100 is slightly wrong, with means of 99.0, 99.6, and 97.5 from left to right. This is sampling error in action. Just by chance all of the means are below 100.    Figure 7.2: Histograms of the 100 data points in each sample and the sample means showing sampling error relative to the population.   In practice, we would only ever get one draw from the population. One study, one sample. If our study happened to draw the sample on the far left (A), our estimate of the sample mean ({\\[}\\bar{x}{/\\]}) would be 99.0. We call this a point estimate. It’s our study result. If we had drawn a different sample, we would have obtained a slightly different estimate of the mean. The next image tries to drive this point home. Imagine that we could sample 100 people from the population over and over again. 10,000 times for good measure. Each of these samples would have an estimate of the mean. The next image shows the distribution of these sample means. In other words, we draw 10,000 samples, calculate the mean for each sample, and plot a histogram of all 10,000 means. As you can see, the means of samples A, B, and C from the previous figure make an appearance in this distribution.    Figure 7.3: Sampling distribution for the sample mean. Histogram of the means of 10,000 random samples.   If the “true” population mean—again, something we never get to know—is 100, then we see that all 10,000 sample means shown in the figure above get us pretty close. But we don’t escape sampling error. Don’t despair. The beauty of a probability sample is that we can take the mean and standard deviation of our one study sample and quantify the difference between our sample and the expected population. We call this quantity the error of estimation, or the margin of error. We can calculate the margin of error for our sample with the following formula: 1.96*standard error. 1.96 is a critical value related to our desire to construct a 95% confidence interval. The standard error of the sample mean is the sample standard deviation divided by the square root of the sample size. For Sample A, the standard deviation is 9.1 and the sample size is 100, so the standard error is 0.908. This means that the margin of error is 1.96*0.908, or ± 1.8. With a sample mean of 99.0 for Sample A, this translates to a 95% confidence interval of 97.2 to 100.8. Informally, we could say that we are 95% confident that the “true” population mean falls between 97.2 and 100.8. The width of this confidence interval gives us a sense of the plausible range of values87 and thus how closely our sample reflects the population. Naturally, we want our confidence intervals to be narrow but it is important to remember that they may, by chance, be centered on a value that is far away from the population mean. The challenge is that you never know if the CI you calculated from your data are close or far from the truth. The good news is that if the confidence interval is narrow, you should at least feel better about any inference you make than if it is wide. We’ll discuss how to shrink this interval in the next chapter. Before moving on to a discussion of approaches to probability sampling, it’s important to note that it is possible to calculate a margin of error on data collected from a non-probability sample—your calculator will not protest—but the result will not be reliable, and therefore not meaningful, because our statistical methods assume probability sampling. So if anyone asks you about how to recruit a sample to estimate the prevalence of a disorder in a population, you can tell them that THE RIGHT WAY™ is to use probability sampling. Here’s how.   Approaches  Simple random sampling The easiest method of creating a probability sample is a lottery in which every unit has an equal chance of being selected. This is just like pulling slips of paper from a hat; the slips of paper we pull out make up our simple random sample. The challenge with this approach is that our hat must contain a slip of paper for every eligible participant from the population. In other words, we need a sampling frame or a master list of all members of the population. If the population is narrowly defined, such registered primary care nurses in District A of Country Z, it’s possible that we could obtain a list of all 400 registered nurses, let’s say, put their names in a hat, and pull 30 names at random.88 This would be a probability sample, and we could quantify our confidence that the sample reflects the population of nurses. We can’t definitively claim that our results generalize to all nurses in Country Z, but that might not be our goal in the first place. In most cases, however, we don’t have sampling frames that are suitable for answering our research questions. Therefore, simple is impossible.   An example from space:   Moss et al. (2011) looked to space to solve the sampling frame problem in Southern Province, Zambia.89 The team used remote sensing (satellite images of the area) to construct a sampling frame of households, and then selected 128 to participate in the study.      Figure 7.4: Source: Moss et al. (2011); Brown circles indicate all households in the study area. Yellow circles indicate sampled households in which at least one resident was RDT positive at the study visit. Green circles indicate sampled households in which no resident was RDT positive at the study visit; http://www.malariajournal.com/content/10/1/163     Systematic sampling If you don’t have a sampling frame at the outset, you can sometimes use systematic sampling to randomly select participants on the fly. For instance, we might want to know something about the women who attend antenatal care services at a clinic or network of clinics. Of course we don’t know who will become pregnant and seek care until they appear at the clinic, so we don’t start the study with a sampling frame. Instead of randomly selecting women from a list, we can decide to invite every Nth woman to participate and continue until we hit our target sample size. This is what Oyibo et al. (2011) did in Ebonyi State, South Eastern Nigeria.90 They wanted a sample size of 208 participants, so they invited every 5th pregnant woman who came to a particular health clinic to participate in the study. This meant that they had to wait for 1,040 women to pass through the clinic; 1 invitation for every 5 women.91   Stratified random sampling A twist on simple random sampling is to group like units into strata (a.k.a. different hats) and randomly select from within each stratum. This is important if we want to compare subgroups, such as single mothers and women from two parent households. If we randomly sample from one big pool, we might not get enough units from each group by chance. This is more likely when some groups have fewer units in the population. If there a small number of single mothers in the population, for instance, simple randomization might not produce a sample with enough single mothers to enable a comparison. In this example of stratified random sampling, we have two strata—single mothers and women from two parent households—and randomly select from each stratum in equal proportions to facilitate the comparison. Alternatively, we could prioritize the need to be representative and randomly select a number of women from each stratum in proportion to the size of the stratum. In other words, if there are fewer single mothers in the population, we select fewer participants from the single mother stratum. This method of proportional sampling would help to ensure a representative sample.92 A side effect is that we might end up with too few single mothers to enable a between-groups comparison, but that might not be our primary objective.   Hypertension in West Africa:   Cappuccio et al. (2004) conducted a study in Ashanti, West Africa to assess the prevalence of hypertension among men and women.93 They identified 12 villages (6 semiurban and 6 rural) and conducted a population census in each village over the course of three months. The authors then invited a stratified random sample of 1,896 adults aged 40 to 75 years to participate. The sample was stratified by village and then age and sex within each village to reflect the population structure determined by the village census.     Cluster and multistage sampling So far we’ve been discussing relatively small populations where it is possible to develop a sampling frame. But what if we want to think big, such as a study to estimate the prevalence of ITN use among children under 5 years of age in Kenya? To recruit a nationally representative sample, we would need to use a combination of cluster and multistage sampling.  Cluster sampling The first step in cluster sampling is to identify groups of people—clusters—such as administrative units like districts or counties. Next, rather than randomly selecting people into the sample directly,94 we first randomly select clusters. So if there are 100 clusters, we might select 10. Then in each of the 10 clusters, we obtain lists of people (sampling frame). If we select 50 people from each cluster, that’s a total sample size of 500. To make things simple, let’s assume there are exactly 2000 eligible people in each cluster. In that case, each person selected would have a probability of selection of 10/100 * 50/2000 = 0.0025. This is an easier approach than simple randomization because we don’t need a master sampling frame of individuals that covers all 100 clusters. But it’s still not reasonable in many cases because we need a sampling frame for the clusters we do select. Many developing countries lack phone books and address registries, so developing a sampling frame form scratch can be a big hassle, or worse—mission impossible.   Multistage sampling To make life easier, researchers often employ multistage sampling in conjunction with cluster sampling. Clusters can take various forms, but administrative units are popular choices. Scanning right to left, we see how communities in Uganda are nested in parishes, which are nested in subcounties, inside districts, and ultimately inside sub-national regions.    Figure 7.5: Nesting of clusters   In a multistage cluster sample, the idea is to select samples of clusters in stages so that the burden of developing a sample frame at the lowest level is minimized. At each stage there is a probability of selection that can ultimately be applied to individuals, if individuals are the lowest unit of selection. Here’s an example from Uganda. As part of an exercise with UNICEF to estimate the national coverage of programs for adolescents, we conducted a multistage cluster sample that began at the level of districts. At the time this work was conducted, there were 120 districts in Uganda. We randomly selected 40 of these 120 districts from within 6 regional strata; the number of districts selected from each stratum was proportional to the population size of the stratum.    Figure 7.6: Example multistage cluster sample   Then within each of the 40 districts, we randomly selected 3 sub-counties. The number of subcounties per district varied; the figure shows 6 subcounties in the example district highlighted in red. Within each subcounty, we organized a data collection effort to enumerate (list) all adolescent programs that fit a set of criteria, and then we randomly selected 5 programs from this list to visit. In the end, each enumerated program had a known probability of selection: 40/120 * 3/subcounties * 5/programs.    Kenya 2008-09 DHS:   DHS surveys are a great example of the multistage cluster sampling approach. Let’s take a look at the 2008-09 Kenya DHS (KDHS) report.   The sample for the 2008-09 KDHS included nearly 10,000 households, enough to be representative at the national and provincial levels, as well as urban and rural settings. “Representative” used in this context means that the estimates are unbiased because of probability sampling AND that the sample size is large enough to enable disaggregation to lower levels including provinces and setting (urban vs rural). The sample size is not large enough, however, to permit valid estimates at even lower levels, like counties.95   The KDHS sample is a multistage cluster sample. In the first stage, 400 clusters (133 urban and 267 rural) were randomly selected from a master sampling frame of clusters. In the second stage, a team of enumerators updated lists of households in each selected cluster and randomly sampled 25 households from this list.   Within each selected household, any females age 15-49 years who were living in the dwelling or visiting the night before the survey could be selected. A total of 8,767 women were eligible to participate, and the response rate was 96.3%. The selection of male participants (age 15-54) was via systematic random sampling of every second household; 3,910 men were eligible and 88.6% participated.   Since the KDHS is a probability sample, it’s possible to estimate the sampling error at the national and provincial levels, as well as by urban and rural settings. For instance, the survey found that 25.5% of currently married women have an unmet need for family planning (meaning they want to delay or prevent pregnancy, but are not using contraception). The standard error of this estimate was 0.008, so the 95% confidence interval (± 2*0.008) ranged from 24.0% to 27.2%.        7.2 Non-Probability Sampling In many cases probability sampling is not possible or feasible. The alternative is to recruit a non-probability sample. The downside is that it’s not possible to estimate how closely the sample reflects the population without probability sampling, but this is only a limitation if the goal is to characterize the larger population.  Approaches  Convenience sampling Convenience samples are convenient for us as researchers. If you have ever participated in a research study on a college campus, you were probably part of a convenience sample of available undergraduates. Without convenience samples, there would be little research to publish in psychology. Convenience samples are not suited for precisely estimating some characteristic of the population, but most research does not aim to do this. Most research looks at the relationship between variables. For instance, is there a relationship between sleeping under a mosquito net and household income? To answer this question, we could recruit a convenience sample of households in a particular community and ask them about their bed net ownership and use, as well as their monthly income or consumption. Our results would not be nationally representative, but they would tell us something about potential associations. It might be possible (and not cost prohibitive) to conduct the same study with a probability sample; for instance, if the population of households is small enough, we might be able to create a sampling frame for the entire community or for selected clusters. In the end, however, the results would still not be nationally representative—just community representative. This could be a good thing and worth doing if feasible. It’s just not essential to answer the correlational research question. The bigger question is whether the results from this one study tell us something larger about the relationship between bed net use and socioeconomic status. Do the results generalize to other places and faces? Make no mistake: this is a valid question for both the small study with a convenience sample and the small study with a probability sample that is representative of the community. In both cases, reviewers will ask whether the findings are applicable to other contexts. The only good way to answer this question is to conduct additional studies in new contexts.   Purposive sampling Whereas a convenience sample is “convenient” because it targets easily available people (or other units), a purposive sample is “purposeful” because the researcher sets out to include certain types of people (or other units). For instance, we might want to examine the training practices of physicians in Brazil, so we find a group of recent medical school graduates and ask them about their experience in school. It might be possible to carry out the same study with a probability sample, in which case we could estimate our confidence that the results are representative of medical schools in Brazil.   Quota sampling If group comparisons are important and a probability sample is not possible, a quota sample might be a good choice. This is a type of convenience or purposive sample that ensures the sample contains a set number or percentage of certain types of units. In the household survey example about bed net use, we might choose to recruit a convenience sample that consists of half single-parent households and half two-parent households.    Snowball sampling and respondent driven sampling:   Snowball sampling is a type of purposive sampling that is often used to sample hard to reach populations, such as people who engage in illegal behaviors, like drug users. In this technique, the initial participants (seeds) refer acquaintances to join the study. These new participants do the same, and the sample “snowballs” from there.   Some researchers believe that a variant of this approach called respondent driven sampling can produce unbiased estimates of the population in certain instances.96 The process of recruitment looks similar to the snowball approach. The key difference comes in the analysis phase; participants’ social network data and the recruitment patterns are used to adjust the estimates and confidence intervals.      Figure 7.7: Source: McCreesh et al. (2012); Seeds are shown at the top of each recruitment network. Symbol area is proportional to network size. HIV serostatus is shown by shading: black indicates HIV positive; white, HIV negative; grey, HIV status unknown. HIV status omitted for seeds for confidentiality; http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3277908/       Chapter Review and Application   Review Questions     Application     "],
["quant.html", "8 Quantitative Methods", " 8 Quantitative Methods  "],
["qualmixed.html", "9 Qualitative and Mixed Methods", " 9 Qualitative and Mixed Methods  "],
["observational.html", "10 Observational", " 10 Observational  "],
["quasi.html", "11 Quasi-Experimental", " 11 Quasi-Experimental  "],
["experimental.html", "12 Experimental", " 12 Experimental  "],
["protocol.html", "13 Planning a Study", " 13 Planning a Study  "],
["sharing.html", "14 Publishing and Conferences", " 14 Publishing and Conferences  "],
["impact.html", "15 From Research to Policy", " 15 From Research to Policy  "],
["references.html", "16 References", " 16 References   Shadish, W. R., T. D. Cook, and D. T. Campbell. 2003. Experimental and Quasi-Experimental Designs for Generalized Causal Inference. Cengage Learning. http://amzn.to/1E8UYIG.      OK, Jimmy is not this kid’s real name, and his poster is not really about the observation that birds are everywhere. The internet had a little fun with Photoshop and re-imagined a number of science fair posters like little Jimmy’s.↩ King, G., Keohane, R. O., &amp; Verba, S. (1994). Designing Social Inquiry: Scientific Inference in Qualitative Research. Princeton University Press.↩ But as we will see later, studies will not necessarily fit into one box. Often in global health research, you will see studies using mixed methods approaches and both types of reasoning.↩ Singla, D. R., Kumbakumba, E., &amp; Aboud, F. E. (2015). Effects of a parenting intervention to address maternal psychological wellbeing and child development and growth in rural Uganda: a community-based, cluster-randomized trial. The Lancet Global Health, 3(8), e458-e469.↩ This is also an example of causal inference, a major focus in this book. Does X impact Y? Does this program cause a specific outcome? As we will discuss later, this is also an example of statistical inference. The authors recruited one sample of adult-child dyads, collected data, and used inferential statistics to generalize from the sample to the population. Don’t worry, by the time you close this book, all these terms will make sense. Repetition, repetition, repetition.↩ Sahoo, K. C., Hulland, K. R., Caruso, B. A., Swain, R., Freeman, M. C., Panigrahi, P., &amp; Dreibelbis, R. (2015). Sanitation-related psychosocial stress: A grounded theory study of women across the life-course in Odisha, India. Social Science &amp; Medicine, 139, 80-89.↩ Grounded theory will come up again in a later chapter as a specific example of approaches to qualitative inquiry. Without getting into the weeds here, we can just say that it is an approach that involves iterative data collection and analysis. Most importantly, data come first in grounded theory. It is only through the iterative process of data collection and analysis that theories and broader implications emerge.↩ Sahoo et al. (2015) is also an example of descriptive inference. Unlike with causal inference, descriptive inference does not seek to establish that X caused Y. Yet descriptive inference goes beyond basic description—or the collection of facts—to say something about how the individual experiences and opinions of these women tell us something more universal about the nature of sanitation-related stress and its possible connections to factors like a woman’s life stage and her behavior.↩ Hogan, M. C., Foreman, K. J., Naghavi, M., Ahn, S. Y., Wang, M., Makela, S. M., … &amp; Murray, C. J. (2010). Maternal mortality for 181 countries, 1980–2008: a systematic analysis of progress towards Millennium Development Goal 5. The Lancet, 375(9726), 1609-1623.↩ You might be wondering how they even come up with any estimates without much data. The answer is statistical modeling. Afghanistan may not have much data on maternal mortality, but there are data on other indicators like total fertility rate, gross domestic product per head, HIV seroprevalence, female education, etc. Using the data we do have from all countries and all years, we can model how these variables are related to maternal mortality. We take this equation, plug in values for Afghanistan, and solve for maternal mortality. More or less.↩ Kassebaum, N. J., Bertozzi-Villa, A., Coggeshall, M. S., Shackelford, K. A., Steiner, C., Heuton, K. R., … &amp; Balakrishnan, K. (2014). Global, regional, and national levels and causes of maternal mortality during 1990–2013: a systematic analysis for the Global Burden of Disease Study 2013. The Lancet, 384(9947), 980-1004.↩ The World Health Organization (WHO) and partners published their own estimates for 2013. They estimated that there were 289,000 maternal deaths, which is pretty close to the IHME estimate of almost 293,000. As Kassebaum et al. (2014) explain, however, the consistency in these estimates masks substantial disagreements, including estimates that diverge at least 20% in 120 countries in 2013 and different perspectives on progress toward achieving the Millennium Development Goal 5.↩ Malaria is a preventable and curable disease that took the lives of more than 500,000 people in 2013—mostly African children. Four species of parasites cause malaria and are transmitted to humans through the bites of Anopheles mosquitoes.↩ Glossary of Terms for Human Subjects Protection and Inclusion Issues, based on the 1997 Report of the NIH Director’s Panel on Clinical Research, entry: “clinical research”. Available at http://grants.nih.gov/grants/peer/tree_glossary.pdf.↩ Doherty, J. F., Pinder, M., Tornieporth, N., Carton, C., Vigneron, L., Milligan, P., … &amp; Cohen, J. (1999). A phase I safety and immunogenicity trial with the candidate malaria vaccine RTS, S/SBAS2 in semi-immune adults in The Gambia. The American Journal of Tropical Medicine and Hygiene, 61(6), 865-868.↩ Moorthy, V. S., &amp; Ballou, W. R. (2009). Immunological mechanisms underlying protection mediated by RTS, S: a review of the available data. Malaria Journal, 8(1), 312.↩ Alonso, P. L., Sacarlal, J., Aponte, J. J., Leach, A., Macete, E., Milman, J., … &amp; Cohen, J. (2004). Efficacy of the RTS, S/AS02A vaccine against Plasmodium falciparum infection and disease in young African children: randomised controlled trial. The Lancet, 364(9443), 1411-1420.↩ Aponte, J. J., Aide, P., Renom, M., Mandomando, I., Bassat, Q., Sacarlal, J., … &amp; Alonso, P. L. (2007). Safety of the RTS, S/AS02D candidate malaria vaccine in infants living in a highly endemic area of Mozambique: a double blind randomised controlled phase I/IIb trial. The Lancet, 370(9598), 1543-1551.↩ RTS,S Clinical Trials Partnership (2015). Efficacy and safety of RTS, S/AS01 malaria vaccine with or without a booster dose in infants and children in Africa: final results of a phase 3, individually randomised, controlled trial. The Lancet, 386(9988), 31-45.↩ Rossi, P. H., Lipsey, M. W., &amp; Freeman, H. E. (2003). Evaluation: A Systematic Approach. Sage publications.↩ Campbell, D. T. (1969). Reforms as experiments. American Psychologist, 24(4), 409.↩ You might know Cronbach from that statistic you report for reliability but don’t really know what it means—Cronbach’s alpha.↩ Cronbach, L. J. (1982). Designing Evaluations of Educational and Social Programs. San Francisco: Jossey-Bass.↩ Rossi, P. H., Lipsey, M. W., &amp; Freeman, H. E. (2003). Evaluation: A Systematic Approach. Sage publications.↩ De Vaus, D. A., &amp; de Vaus, D. (2001). Research Design in Social Research. Sage.↩ Glennerster, R., &amp; Takavarasha, K. (2013). Running Randomized Evaluations: A Practical Guide. Princeton University Press.↩ The DHS Program runs several types of surveys, with the DHS surveys being the most well known. A DHS survey takes an average of 18-20 months to complete. Preliminary results are released about a month after the end of data collection, but it can take up to a year to release the final report and data. See here for more details about the DHS process: http://dhsprogram.com/What-We-Do/Survey-Process.cfm.↩ As we will discuss later, DHS surveys include enough people to be representative for different subgroups, such as urban and rural settings or wealth quintiles (the rich, the poor, and everyone in between).↩ Noor, A. M., Gething, P. W., Alegana, V. A., Patil, A. P., Hay, S. I., Muchiri, E., … &amp; Snow, R. W. (2009). The risks of malaria infection in Kenya in 2009. BMC Infectious Diseases, 9(1), 180.↩ Noor, A. M., Omumbo, J. A., Amin, A. A., Zurovac, D., &amp; Snow, R. W. (2006). Wealth, mother’s education and physical access as determinants of retail sector net use in rural Kenya. Malaria Journal, 5(1), 5.↩ Lindblade, K. A., Mwandama, D., Mzilahowa, T., Steinhardt, L., Gimnig, J., Shah, M., … &amp; Mathanga, D. P. (2015). A cohort study of the effectiveness of insecticide-treated bed nets to prevent malaria in an area of moderate pyrethroid resistance, Malawi. Malaria Journal, 14(1), 1-15.↩ In a retrospective cohort design, researchers would find a cohort of people without the disease at some point in the past and assess available records to determine each person’s exposure status and outcome.↩ Obala, A. A., Mangeni, J. N., Platt, A., Aswa, D., Abel, L., Namae, J., &amp; O’Meara, W. P. (2015). What Is Threatening the Effectiveness of Insecticide-Treated Bednets? A Case-Control Study of Environmental, Behavioral, and Physical Factors Associated with Prevention Failure. PloS One, 10(7), e0132778.↩ Hawley, W. A., Phillips-Howard, P. A., ter Kuile, F. O., Terlouw, D. J., Vulule, J. M., Ombok, M., … &amp; Hightower, A. W. (2003). Community-wide effects of permethrin-treated bed nets on child mortality and malaria morbidity in western Kenya. The American Journal of Tropical Medicine and Hygiene, 68(4 suppl), 121-127.↩ Phillips-Howard, P. A., Nahlen, B. L., Kolczak, M. S., Hightower, A. W., TER KUILE, F. O., Alaii, J. A., … &amp; Hawley, W. A. (2003). Efficacy of permethrin-treated bed nets in the prevention of mortality in young children in an area of high perennial malaria transmission in western Kenya. The American Journal of Tropical Medicine and Hygiene, 68(4 suppl), 23-29.↩ Agha, S., Van Rossem, R., Stallworthy, G., &amp; Kusanthan, T. (2007). The impact of a hybrid social marketing intervention on inequities in access, ownership and use of insecticide-treated nets. Malaria Journal, 6(1), 13.↩ KEMRI (n.d.). Kenya malaria fact sheet. Available at http://www.kemri.org/index.php/help-desk/search/diseases-a-conditions/29-malaria/113-kenya-malaria-fact-sheet.↩ Cohen, J., &amp; Dupas, P. (2010). Free distribution or cost-sharing? Evidence from a randomized malaria prevention experiment. The Quarterly Journal of Economics, 125(1), 1-45.↩ Scandurra, L., Acosta, A., Koenker, H., Kibuuka, D. M., &amp; Harvey, S. (2014). It is about how the net looks”: a qualitative study of perceptions and practices related to mosquito net care and repair in two districts in eastern Uganda. Malaria Journal, 13, 504.↩ O’Cathain, A., Thomas, K. J., Drabble, S. J., Rudolph, A., &amp; Hewison, J. (2013). What can qualitative research do for randomised controlled trials? A systematic mapping review. BMJ open, 3(6), e002889.↩ Alaii, J. A., Hawley, W. A., Kolczak, M. S., Ter Kuile, F. O., Gimnig, J. E., Vulule, J. M., … &amp; Phillips-Howard, P. A. (2003). Factors affecting use of permethrin-treated bed nets during a randomized controlled trial in western Kenya. The American Journal of Tropical Medicine and Hygiene, 68(4 suppl), 137-141.↩ Phillips-Howard, P. A., Nahlen, B. L., Kolczak, M. S., Hightower, A. W., TER KUILE, F. O., Alaii, J. A., … &amp; Hawley, W. A. (2003). Efficacy of permethrin-treated bed nets in the prevention of mortality in young children in an area of high perennial malaria transmission in western Kenya. The American Journal of Tropical Medicine and Hygiene, 68(4 suppl), 23-29.↩ White, H. (2009). Theory-based impact evaluation: principles and practice. 3ie Working Paper 3.↩ Leary, M. (2012). Introduction to Behavioral Research Methods (6th Edition). Pearson.↩ Arkes, H. R., &amp; Blumer, C. (1985). The psychology of sunk cost. Organizational Behavior and Human Decision Processes, 35(1), 124-140.↩ Taylor‐Robinson, D. C., Maayan, N., Soares‐Weiser, K., Donegan, S., &amp; Garner, P. (2012). Deworming drugs for soil‐transmitted intestinal worms in children: effects on nutritional indicators, haemoglobin and school performance. The Cochrane Library.↩ However, the 95% confidence interval crossed 0, and the effect is not statistically significant. We’ll cover this concept in more detail in a later chapter.↩ Aiken, A. M., Davey, C., Hargreaves, J. R., &amp; Hayes, R. J. (2015). Re-analysis of health and educational impacts of a school-based deworming programme in western Kenya: a pure replication. International Journal of Epidemiology.↩ This is referred to as “hand searching”.↩ The time period of their search spanned from only 2012 to 2015 since they were updating an earlier systematic review from 2012.↩ PubMed lets you limit results to humans or animals from the results page with one click, so it’s not essential to use Boolean operators manually in this case. PubMed will do it behind the scenes for you.↩ Only the five studies represented in the earlier forest plot studied the impact of a single dose of deworming medication on sick kids, so the remaining 40 articles were dropped from this specific analysis. Take a look at Taylor-Robinson et al. (2015) to see forest plots of every key outcome tracked.↩ Bumiller, E. (2010, April 26). We Have Met the Enemy and He Is PowerPoint. The New York Times. Retrieved from http://www.nytimes.com/2010/04/27/world/27powerpoint.html.↩ Feed the Future (2011). Kenya: FY 2011-2015 Multi-Year Strategy. Available at http://www.feedthefuture.gov/sites/default/files/country/strategies/files/KenyaFTFMulti-YearStrategy.pdf↩ Consensus on the definition of ‘theory of change’ remains elusive.↩ W. K. Kellogg Foundation (2004). Logic Model Development Guide.↩ JPAL (2007). Cheap and effective ways to change adolescents’ sexual behavior. Policy Briefcase No. 3.↩ Dupas, P. (2011). Do Teenagers Respond to HIV Risk Information? Evidence from a Field Experiment in Kenya. American Economic Journal. Applied Economics, 3(1), 1.↩ Duflo, E., Dupas, P., Kremer, M., &amp; Sinei, S (2006). Education and HIV/AIDS Prevention: Evidence from a Randomized Evaluation in Western Kenya. World Bank Policy Research Working Paper No. 4024.↩ Dupas, P. (2011). Do Teenagers Respond to HIV Risk Information? Evidence from a Field Experiment in Kenya. American Economic Journal. Applied Economics, 3(1), 1.↩ JPAL (2007). Cheap and effective ways to change adolescents’ sexual behavior. Policy Briefcase No. 3.↩ DFID (2013). DFID how to note: Guidance on using the revised logical framework.↩ The key measurement lessons of this chapter apply whether your research question is descriptive, correlational, or causal.↩ You’ll sometimes see the terms outcome, indicator, measure, target, instrument, and variable used interchangeably. Don’t fall into this trap. Indicators are measures of outcomes (or impacts). A target is a goal for change in the value of an indicator (e.g., reduce maternal mortality ratio by half). Instruments are the tools for measuring indicators. (Instruments might also go by different names, including scales, measures, inventories, batteries). Variables are the numeric values of the indicators that can vary between observations.↩ Jokhio, A. H., Winter, H. R., &amp; Cheng, K. K. (2005). An intervention involving traditional birth attendants and perinatal and maternal mortality in Pakistan. New England Journal of Medicine, 352(20), 2091-2099.↩ WHO (2014). Trends in maternal mortality: 1990 to 2013. Estimates by WHO, UNICEF. UNFPA, the World Bank and the United Nations Population Division. Geneva, World Health Organization.↩ Thaddeus, S., &amp; Maine, D. (1994). Too far to walk: maternal mortality in context. Social Science &amp; Medicine, 38(8), 1091-1110.↩ Say, L., Chou, D., Gemmill, A., Tunçalp, Ö., Moller, A. B., Daniels, J., … &amp; Alkema, L. (2014). Global causes of maternal death: a WHO systematic analysis. The Lancet Global Health, 2(6), e323-e333.↩ You’ll recall from Chapter 1 that this gap between developing evidence of effective programs and actually implementing them at scale is an example of a “T4” translational research bottleneck.↩ See the “WHO guide to cost-effectiveness analysis” for guidance.↩ The Kenya “sugar daddies” study from Chapter 3 included a cost-effectiveness analysis. Recall that the program reduced the incidence of childbearing by 28%. The program was reported to cost $28.20 USD per school and $0.80 per student. This worked out to $91 per childbirth averted.↩ Think about it this way: if you replace half the dose of an injection with saline, the medication would not work as well, if it even works at all.↩ Breitenstein, S. M., Fogg, L., Garvey, C., Hill, C., Resnick, B., &amp; Gross, D. (2010). Measuring implementation fidelity in a community-based parenting intervention. Nursing Research, 59(3), 158.↩ Which is not to say that they were 1.5 times more likely to need this referral or that they were 1.5 times more likely to actually receive referral care. Just that the a referral was more likely to be made.↩ Anyone who figures out a better Y gets a free signed copy of this book.↩ Hoyert, D. L. (2007). Maternal Mortality and Related Concepts.↩ The ICD-9 definition—which would have been the standard at the time of the study—is very similar. For more information, see http://1.usa.gov/1JF0vnv.↩ Maternal deaths are therefore characterized by a causal and a temporal component. In the chapter opening I asked whether it would be a maternal death if a pregnant woman slips on a patch of ice and dies—or if she dies from an infection 40 days after giving birth to a healthy baby girl. No and yes. Accidents like slipping on ice don’t count, but a non-accidental death within 42 days of a baby’s birth would.↩ WHO (2015). Global Reference List of 100 Core Health Indicators.↩ There is also secondary data analysis in which you analyze data that have already been collected for another purpose—typically another study.↩ Glennerster, R., &amp; Takavarasha, K. (2013). Running Randomized Evaluations: A Practical Guide. Princeton University Press.↩ Rathore, J. S., Jehi, L. E., Fan, Y., Patel, S. I., Foldvary-Schaefer, N., Ramirez, M. J., … &amp; Tesar, G. E. (2014). Validation of the Patient Health Questionnaire-9 (PHQ-9) for depression screening in adults with epilepsy. Epilepsy &amp; Behavior, 37, 215-220.↩ Kohrt, B. A., Jordans, M. J., Tol, W. A., Luitel, N. P., Maharjan, S. M., &amp; Upadhaya, N. (2011). Validation of cross-cultural child mental health and psychosocial research instruments: adapting the Depression Self-Rating Scale and Child PTSD Symptom Scale in Nepal. BMC Psychiatry, 11(1), 127.↩ And if you realize that you do not have a gold standard to help validate your new culturally-anchored instrument, you’ll read: Bolton, P. (2001). Cross-cultural validity and reliability testing of a standard psychiatric assessment instrument without a gold standard. The Journal of Nervous and Mental Disease, 189(4), 238-242.↩ Leary, M. (2012). Introduction to Behavioral Research Methods (6th Edition). Pearson.↩ While we’re pretending, we can assume that the definition of ITN use was perfectly clear, there was no error in data collection, and everyone answered honestly without fear of retribution or a desire to please. This would eliminate just about every source of non-sampling measurement error (systematic bias and unsystematic error).↩ Diez, D.M., Barr, C.D., &amp; Çetinkaya-Rundel, M. (2015). OpenIntro Statistics (3rd Edition).↩ You might literally pull slips of paper out of a hat if your population is small, but more likely you would use a random number generator.↩ Moss, W. J., Hamapumbu, H., Kobayashi, T., Shields, T., Kamanga, A., Clennon, J., … &amp; Glass, G. (2011). Use of remote sensing to identify spatial risk factors for malaria in a region of declining transmission: a cross-sectional and longitudinal community survey. Malaria Journal, 10, 163.↩ Oyibo, P. G., Ebeigbe, P. N., &amp; Nwonwu, E. U. (2011). Assessment of the risk status of pregnant women presenting for antenatal care in a rural health facility in Ebonyi State, South Eastern Nigeria. North American Journal of Medical Sciences, 3(9), 424.↩ Oyibo et al. ended up with a probability sample with this approach, but they could not claim that their sample represented the population of pregnant women. First, they only sampled from one small health clinic in one small part of one country. Second, they did not sample all pregnant women, just those who visited the clinic for antenatal care. However, their sample was representative of women in this particular community who attended antenatal care services. In cases like this, researchers often make an argument that the results could possibly generalize to other settings and populations, but these arguments are conceptual, not empirical.↩ If our sample size is large enough, we should get a representative sample by chance alone. Proportional sampling just helps to ensure that we will.↩ Cappuccio, F. P., Micah, F. B., Emmett, L., Kerry, S. M., Antwi, S., Martin-Peprah, R., … &amp; Eastwood, J. B. (2004). Prevalence, detection, management, and control of hypertension in Ashanti, West Africa. Hypertension, 43(5), 1017-1022.↩ Presumably this would not be possible anyway because we are turning to cluster sampling precisely because we do not have a sampling frame of individuals across all clusters. If we did, we could just randomly select from the overall sampling frame or treat the clusters as strata and select from within each strata. Cluster sampling presumes that we don’t have this magical list.↩ The 2014 KDHS was designed to be representative down to the county level (47).↩ Malekinejad, M., Johnston, L. G., Kendall, C., Kerr, L. R. F. S., Rifkin, M. R., &amp; Rutherford, G. W. (2008). Using respondent-driven sampling methodology for HIV biological and behavioral surveillance in international settings: a systematic review. AIDS and Behavior, 12(1), 105-130.↩  "]
]
