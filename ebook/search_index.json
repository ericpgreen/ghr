[
["index.html", "Global Health Research: Designs and Methods Preface About this Book Organization Icons Acknowledgements About the Cover Colophon", " Global Health Research: Designs and Methods Eric P. Green 2017-09-02 Preface Does the world really need another book about research methods? I think so. But I spent a fair amount of time writing down the ideas in this book, so I’m biased. Here’s my rationale. I went to graduate school for clinical psychology, and my classmates and I read the classic psychology texts on research design and methods—books like Experimental and Quasi-Experimental Designs for Generalized Causal Inference by Shadish, Cook, and Campbell (2003). I remember staying up late trying to memorize all of the different threats to internal validity outlined by Donald Campbell and colleagues. Meanwhile, across campus, my econ colleagues were reading the ideas of another Donald—Donald Rubin—and what is now known as Rubin’s causal model. When I set off for Uganda in 2007, determined to learn more about this field called global health, I met some Donald Rubin proselytes in the wild. I tried communicating with them, but they did not understand my Campbellian drawl. We were usually trying to say the same thing, just in different the languages. But I couldn’t place the blame on the economists and the disciplinary gap between us. So much of what I didn’t know went far beyond differences in jargon. I was a psychologist trained in clinical research, and nearly every applied example I read came from the U.S. or Europe. The field of global mental health was still in its infancy when I was in school. The important Lancet series on global mental health that really put the field on the map was published in September 2007 as I was getting on a plane to fly back home. I really knew nothing about global health. Fortunately, students entering university today have many more opportunities to learn about global health through interdisciplinary studies. Duke University launched the first liberal arts global health major in the U.S. in 2013, and other universities have followed suit. The Duke program is unique because it requires global health students to specialize in a second discipline, such as biology, economics, psychology, or public policy. I started teaching at Duke around the time the new major started, and I needed to choose a textbook for a course called “Research Methods in Global Health.” After reviewing many excellent books that covered the basics, I discovered none that integrated examples from this very diverse and interdisciplinary field. So I decided to write my own book. About this Book One guiding principle of this book is that a student of global health needs to be a student of medicine, biology, statistics, economics, psychology, public policy, and many other disciplines. In the study of malaria, for example, a literature search returns articles about the spread of the disease (epidemiology), the impact of illness on future productivity (economics), the merits of free or subsidized bed nets (public policy), mosquito habitats (ecology), the efficacy of vaccines to prevent the disease (medicine and statistics), rapid diagnostic tests (biomedical engineering), the adoption and use of bed nets (psychology), and many others. No one book or author could ever hope to provide full disciplinary coverage of even a single topic like malaria. Therefore, I wanted to create a resource that would teach the basics of research design and methods by exposing readers to real world global health examples from different disciplines. Another important guiding principle of this book is openness. Whenever possible, the examples come from open access sources. In this way, every reader should be able to access at least 90% of the references provided here. Organization The first objective of my course on global health research—and this book—is to make students better consumers of research. So I begin by explaining the research process and the importance of critical appraisal. Thus, I begin by explaining the research process and the importance of critical appraisal. In Part I, I survey the landscape of global health research and outline the core steps of the scientific research process. In Part II, I explain how to search the literature for existing evidence, how to use filtered evidence like systematic reviews and meta-analyses, how to critically appraise scientific work, and how to evaluate claims of causality. The second main objective of my course is to make students producers of research. The rest of the book is devoted to this aim. In Part III and Part IV I address the technical aspects of global health research and cover issues related to data collection and measurement. I begin with practical advice on developing a theory of change to guide program development, data monitoring, and study evaluation. Next, theories of change and logical models are demonstrated because a good model can organize and support the approach to measurement used in a study. I then explain how to define the measurement of key study outcomes and how to use quantitative and qualitative methods to collect data on these outcomes. Thereafter, a discussion of sampling methods and sample size determination is presented. Following these basics, I turn in Part V and Part VI to researh design. This section provides a foundation that will enable researchers to evaluate their research questions to determine the most appropriate study design: experimental, quasi-experimental, or observational. Part VII is comprised of several chapters that further guide the researcher to hone the techniques within the chosen study design to ensure that the research outcomes make an impact. One limitation of this book is that I do not teach statistics. Statistical concepts are discussed throughout but not in great detail. Because statistical analysis is an intrinsic part of the study design stage, I recommend downloading a copy of OpenIntro Stats and read it alongside this book. Icons I’ve sprinkled several types of asides throughout the book: Help piecing together the global health puzzle Extended discussion of a special topic Tips Videos Interactive aids Acknowledgements I’d like to thank some folks for their helpful feedback at various points throughout my writing process. My graduate student teaching assistants, Kaitlin Saxton, Kathleen Perry, Olivia Fletcher, and Jenae Logan, read and commented on the initial drafts. This could not have been fun, so thanks! Thanks to Duke librarians Megan Von Isenburg and Hannah Rozear for setting me straight on literature searches. I still have a lot to learn! Liz Turner, biostatistican extraordinaire, kept me from making too many mistakes on technical details here and there. Gavin Yamey helped me understand what we do and don’t know about funding for global health research. I’d also like to thank students in my undergraduate and graduate global health research courses for test driving the book before all the parts were in place. Special shoutout to the following students for sharing written feedback: Kelsey Sumner, Karly Gregory, Qian Yudong, and Christina Schmidt. Despite everyone’s best efforts to help me catch mistakes, I’m certain errors remain in the book. My bad. About the Cover Jef Brown is an Artist &amp; Illustrator in Seattle, WA. To find more of his work visit: www.jefbrown.com Creating the cover for this book, I wanted to create imagery that reflected the “action” of research in Global Health. Rather than depict a portrait of impoverished villages, a nebulous globe, or still life of medicine- I created a representation of the data that drives the change. I was inspired by the bubble charts of Hans Rosling and the aesthetic philosophy of Wassily Kandinsky. For both, “shape” was an important tool for expression. In creating the constellation of circles I was able to convey a dynamic representation for a larger living idea. Colophon This book is a work in progress. If you find errors (gasp!), please create an issue on Github, email me, or shame me on Twitter (@ericpgreen). I’m writing the book in R Markdown within RStudio. The bookdown package from the makers of RStudio does most of the heavy lifting to compile the book. The source code for the book is available on Github. This work is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 United States License. References "],
["science.html", "1 Introduction to Global Health Research 1.1 Who Funds Global Health Research? 1.2 Who Produces Global Health Research? 1.3 Where is Global Health Research Published? 1.4 What Constitutes Global Health Research? Share Feedback", " 1 Introduction to Global Health Research Global health is the study and practice of “improving health and achieving equity in health for all people worldwide” (Koplan et al. 2009). The determinants of ill health and inequality are as far-reaching and complex as this definition is ambitious, and, the search for global health solutions spans many disciplines: Public health Medicine Social behavioral sciences (e.g., economics, psychology, anthropology) Biomedical engineering Biostatistics Public policy Business Global health research is multidisciplinary and interdisciplinary by design (Merson, Black, and Mills 2011). It is multidisciplinary in the sense that no one field can solve the great global health challenges of our time and interdisciplinary in the recognition that solutions can be reached through collaboration on new approaches that integrate ideas from different academic and clinical traditions. Its multidisciplinary nature means that global health research is a vast, spralling enterprise that involves many stakeholders. Let’s survey the landscape of global health research before establishing some fundamentals about scientific research. The term “stakeholders” can refer to a wide range of people and organizations. Typically, it means donors (i.e., the public and private organizations that fund research programs), policy makers (i.e., government officials and bureaucrats at national or international bodies like the CDC or the WHO), and program implementers (e.g., organizations like Doctors Without Borders) that actually deliver services to beneficiaries (i.e., individuals, populations, or patients), and it even extends to scholars who study a particular topic or policy issue. 1.1 Who Funds Global Health Research? Billions of dollars are spent on global health research every year.1 This funding comes from public institutions such as the National Institutes of Health (NIH) in the United States and private philanthropy organizations such as the Bill and Melinda Gates Foundation. In 2015, the international community disbursed $36.4 billion in development assistance for health, down from a peak of $38 billion in 2013 (Institute for Health Metrics and Evaluation 2016). Although Figure 1.1 does not present total research dollars contributed, per se, it does show the sources of global health financing, the channels through which they flow, and the areas on which they focus (see here for an interactive version).2 Figure 1.1: Flows of global health financing. Abbreviations: BMGF, Bill &amp; Melinda Gates Foundation; SWAps &amp; HSS, sector-wide approaches and health-sector support; Gavi, the Vaccine Alliance. Source: Institute for Health Metrics and Evaluation, http://vizhub.healthdata.org/fgh/ The United States Government contributed one-third of all development assistance for health in 2015 ($13 billion). More than half of USG funding was directed to HIV/AIDS, malaria, tuberculosis, and other infectious diseases. Noncommunicable diseases received &lt;1% of resources, despite accounting for 7 of the 10 leading causes of death globally (Figure 1.2). Figure 1.2: Global deaths per 100,000. Abbreviations: COPD, chronic obstructive pulmonary disease. Source: Institute for Health Metrics and Evaluation, http://vizhub.healthdata.org/gbd-compare/ 1.2 Who Produces Global Health Research? Academic centers around the world, like the Duke Global Health Institute, are major contributors to global health research. Public and private donors like the USG, the Bill and Melinda Gates Foundation, and the World Bank make grants to university-affiliated faculty members who partner with colleagues inside and outside governments to plan and conduct research. Donors also channel research support to nonprofit research organizations such as FHI 360 and RTI International, who work in a similar fashion. Interesting hybrid models include the Abdul Latif Jameel Poverty Action Lab (JPAL) and Innovations for Poverty Action (IPA). JPAL is a global network of university-affiliated professors from more than 40 universities that uses randomized evaluations (i.e., experiments) to answer policy questions related to poverty alleviation. The JPAL website, http://www.povertyactionlab.org, contains excellent resources regarding the methods used in randomized evaluations, as well as links to published studies and policy briefs. IPA is a sister organization that is also a leader in the use of randomized evaluations to study important policy questions about global poverty. 1.3 Where is Global Health Research Published? Global health research is published in medical journals (e.g., The Lancet and JAMA), general science journals (e.g., Science and PLOS ONE), discipline-specific journals (e.g., The Journal of Immunology and Epidemiology), and disease-specific journals (e.g., AIDS and Malaria Journal). Journals specializing in global health include The Lancet Global Health, BMJ Global Health, Global Public Health, and Global Health: Science and Practice. 1.4 What Constitutes Global Health Research? Like any scientific endeavor, global health research begins with basic research founded in principles of hard science. This is only the beginning, however; basic research constitutes only a small portion of the overall body of ongoing research. The areas of applied exploration built on this foundation include the clinical arenas from which important global health changes emerge. Figure 1.3: A research taxonomy 1.4.1 BASIC RESEARCH Basic research—or “pure” research—is the pursuit of fundamental knowledge of phenomena. For example, scientists conduct laboratory experiments to understand the parasitic life cycle and how parasites interact with humans at different stages. Likewise, basic ecology research seeks to understand plant species diversity. The information generated by basic science becomes the universally acknowledged background for more advanced research—applied science. 1.4.2 APPLIED RESEARCH Basic research is different than applied research, which focuses on specific problems or applications in several ways. For instance, an applied research question might be, “How can we increase the coverage and use of bed nets that prevent malaria transmission?” The basic science, such as the behavioral habits of the mosquito and the transmission conditions for malaria, have already been determined by entomologists and epidemiologists. In contrast, applied science takes many different forms, including clinical research. Clinical research is a broad field that encompasses patient-oriented research, epidemiological and behavioral studies, outcomes research, and health services research.3 Basic research provides the foundation for all clinical research. Clinical Trials One type of clinical research is a clinical trial. Drugs and vaccines have to pass through different phases of clinical trials before regulatory bodies, such as the Federal Food and Drug Administration (FDA), will approve their use with humans: Preclinical research Phase I Phase II Phase III Phase IV Behavioral research (e.g., development and evaluation of parenting interventions) does not follow the same exact phases of vaccine and drug development, but the broad principles are the same. Case study: Developing a malaria vaccine The development of a vaccine for malaria provides a good example of the life cycle of a clinical trial. In 2015, after 30 years of research, a vaccine candidate called RTS,S, or Mosquirix™, made the news for having gotten one step closer to becoming a licensed vaccine after a successful Phase III trial. Development of RTS,S began in 1984 through a partnership between the pharmaceutical company GlaxoSmithKline (GSK) and the Walter Reed Army Institute of Research. In 1987, a promising vaccine candidate entered preclinical research. During the preclinical phase, researchers performed tests on nonhuman subjects to collect data on how well the vaccine worked (efficacy), how much damage it could do to an organism (toxicity), and how the body affected the vaccine (pharmacokinetics). Clinical research on humans began in 1992. To obtain regulatory approval, the vaccine had to complete three phases of testing. Doherty et al. (1999) conducted a Phase I safety and immunogenicity trial with 20 adults in The Gambia in 1997. This small sample size is typical of Phase I trials, where the main objectives are usually to determine a safe dosing range and to evaluate side effects. These researchers reported that the vaccine did not have any significant toxicity but did produce the expected antibodies. Several Phase II studies conducted over the following decade (Phase IIa and Phase IIb) demonstrated the efficacy of the vaccine against several end points (or outcomes) (Moorthy and Ballou 2009). A Phase IIb trial began in Mozambique in 2003 with more than 2,000 children aged 1 to 4 years (Alonso et al. 2004). Each child was randomly assigned to receive 3 doses of RTS,S or a control vaccine. After 6 months, the prevalence of malaria was 37% lower in the treatment group than in the control group. A follow-up study with 214 infants also showed partial protection from malaria (Aponte et al. 2007). This Phase II trial was an important proof-of-concept study. The final results of a large Phase III trial with more than 15,000 infants and young children in 7 African countries were published in The Lancet in 2015 (RTS,S Clinical Trials Partnership 2015). Children who participated in the study were randomly assigned to 1 of 3 arms: (a) 3 doses of RTS,S and a booster dose at month 20, (b) 3 doses of RTS,S and a booster dose of a comparator vaccine at month 20, or (c) 4 doses of a comparator vaccine. RTS,S reduced clinical malaria cases by 28% and 18% among young children and infants, respectively, over a 3–4-year period. This Phase III trial achieved its goal—to show that the treatment was efficacious. On the basis of these results, the European Medicines Agency issued a “European scientific opinion” to help inform the decision of the WHO and African national regulatory authorities regarding their recommendation of the vaccine. If RTS,S is approved for use and eventually hits the market, researchers will likely conduct Phase IV trials to evaluate the vaccine’s long-term effects. Implementation Science and Translational Research The research on RTS,S, will not end there, however. The vaccine may be efficacious, but that does not mean it will be easy or cost-effective to produce and deliver at a scale of millions. Studies that assess how best to get efficacious treatments to the people who need them most fall under the domain of implementation science. Many stumbling blocks face interventions in the process of moving from “bench to bedside.” Practitioners of translational research point to 4 key bottlenecks: T1: Translation from basic science to clinical research T2: Translation from early clinical trials to Phase III trials and beyond with larger patient populations T3: Translation from efficacy trials (i.e., Phase III trials) to real-world effectiveness through implementation science T4: Translation from evidence about delivery at scale to the adoption of new policies Figure 1.4: Source: Medical University of South Carolina, http://bit.ly/2iq2Blv Monitoring and Evaluation Another arena of applied science in global health is monitoring and evaluation, or M&amp;E. Evaluation In the United States, program evaluation became commonplace by the end of the 1950s and grew dramatically in the 1960s as the federal government expanded and introduced new social programs. Lawmakers wanted accountability, and the evaluation of social programs took off (Rossi, Lipsey, and Freeman 2003). But is program evaluation really research? Methods giant Donald Campbell4 thought so: The United States and other modern nations should be ready for an experimental approach to social reform, an approach in which we try out new programs designed to cure specific problems, in which we learn whether or not these programs are effective, and in which we retain, imitate, modify or discard them on the basis of their effectiveness on the multiple imperfect criteria available (Campbell 1969). But not everyone agrees. Some have argued that program evaluation is really designed for program implementers and funders, and that the messy nature of program implementation requires a loosening of research standards (Cronbach 1982) and simply evaluate the evidence, or “learn what you can.” In their introductory text on evaluation, Rossi et al. (2003) strike a balance in views. Their answer is perhaps a bit unsatisfying but is arguably true nevertheless: It depends. Program evaluations should be as rigorous as logistics, ethics, politics, and resources permit—and no less. Surely a lower bound in terms of quality or what is worthwhile exists, but the line is so context dependent that a simple rule is best: “Don’t go beyond the data.” Every organization wants to claim “impact,” but not every evaluation can be based on the design and implementation. Monitoring Program monitoring is concerned with the implementation of programs, policies, or interventions. How are resources being used? Is the program being delivered as intended (or with fidelity)? How many people participate, and does the program reach the intended targets? These are all program-monitoring questions. Accurate monitoring is essential for reporting data to funders, but it is also essential for all good evaluations. The reason is simple: If a program fails—that is, has no impact—the next question is why? Did the program fail because the idea or theory behind the program was wrong (theory failure)? Or was the implementation of the program so troubled that there was never a chance for success (i.e., implementation failure)? Every trial should include ongoing monitoring or a formal process evaluation to assess the impact of the program on a continual basis. Share Feedback Please feel free to add your comments about this chapter. References "],
["research101.html", "2 Research 101 2.1 Scientific Research 2.2 Stages in the Research Process", " 2 Research 101 At its core, global health research is based on common principles of scientific research that each discipline follows to a greater or lesser extent, but different disciplinary traditions and emphases can amplify what is unique over what is shared. This book is presents what is shared while highlighting unique aspects along the way. 2.1 Scientific Research What counts as “scientific research”? King, Keohane, and Verba (1994) offer a useful definition in their book, Designing Social Inquiry. They point to several main characteristics: The goal is inference. The procedures are public. The conclusions are uncertain. 2.1.1 ALL ABOUT INFERENCE By stating that the goal of scientific research is inference, we mean that science goes beyond the collection of facts. Inference refers to the process of making conclusions about some unobserved or unmeasured phenomenon based on direct observations of the world. What is known is used to infer something about things that are not known. This process can be deductive or inductive. In deductive reasoning, we start from general theories and make hypotheses. Then we collect data and make conclusions based on the data. Inductive reasoning flows the other direction, from specific observations to the generation of hypotheses and theories. Remember it this way: Testing a specific hypothesis requires deductive reasoning. Using observations and making more general statements requires inductive reasoning. To say that quantitative research is deductive and qualitative research is inductive is not quite right, but it’s often true.5 For instance, Singla, E. Kumbakumba, and Aboud (2015) reported the results of a cluster randomized trial of a parenting intervention in rural Uganda. This study is an example of deductive reasoning because the authors started with a hypothesis, collected quantitative data, and inferred something about the impact of the intervention: Using quantitative methods, the primary outcomes of this study were cognitive and receptive language development of the children of participating caregivers was measured using the Bayley Scales of Infant Development. The authors hypothesized that the intervention would improve child development. They found effects on cognitive and receptive language but not height-for-age, and they inferred that the difference observed between the treatment and control groups was due to the parenting intervention.6 The Singla et al. trial can be compared with a qualitative study by Sahoo et al. (2015). Sahoo and colleagues used a grounded theory approach to conduct and analyze interviews with 56 women in Odisha, India, about their sources of stress and sanitation practices.7 This study used inductive reasoning because the authors started with the data—their observations—and then looked for themes and patterns. They came to some conclusions about the nature of sanitation-related stress.8 One result of this work was a conceptual framework for thinking about sanitation-related psychosocial stress.9 The point to take away about inference is that, regardless of the approach to reasoning, the goal of scientific research is to use what we observe to make conclusions about what we do not or cannot observe directly. This is sometimes referred to as empiricism, and our systematic observations are empirical evidence. Empiricism is at the heart of scientific research. 2.1.2 RESEARCH AS A PUBLIC ACT Scientific research uses public methods that can be examined and replicated. Replication is a core principle of scientific research. No one study rules the day. If the results of a study are robust, another research group should be able to follow the methods used and replicate the findings. When such findings are replicated, we all have more confidence in the results. Replications are relatively rare, however. First, resources for replicating studies are scant, especially for big field experiments. Second, journal space is limited (especially if there is still a print version), and peer review requires a good deal of time and other resources. Journals want to use their space and resources to publish novel ideas. (Ironically, novel aspects are sometimes small effects that cannot be replicated.) Without the promise of a publication, researchers have little incentive to spend time and money trying to replicate published findings. Publications are a key criterion for tenure and promotion in academia, as well as grant acquisition, so many researchers don’t waste their efforts on studies that have no chance of being published. What happens when researchers attempt to replicate study findings? The short answer is bitterness. Replicators grab more headlines when they “debunk” findings, and the original authors almost invariably call into question the quality of the replication. This process often leads to hard feelings on both sides. Just see #wormwars to learn what happened when a famous de-worming study was re-examined. Or Google social psychology and priming. Yikes! A related issue is reproducibility, the ability to generate a study’s findings given the original data set and sometimes the original analysis code. Surprisingly, reproducible findings are much less than assured. The Quarterly Journal of Political Science found that slightly more than half of their published empirical papers subjected to review had results that could not be reproduced using the author’s own code. On the positive side, authors increasingly share their data and analysis code. Although such sharing has been standard practice in economics for some time, the idea is pretty revolutionary in medicine and public health. We’ll explore why sharing methods and analysis code is so important and easier than ever to do. 2.1.3 LIVING WITH UNCERTAINTY Every method has limitations, every measurement has error, and every model is wrong to some extent. In short, research is an imperfect process. Sometimes researchers make outright mistakes. These mistakes may or may not be detected and corrected in the peer review process, or during post-publication review if authors share their data and analysis code. Other findings are free of obvious mistakes, but they fail the replication test, and over time some results run counter to a growing body of literature that points in the other direction. In these ways, science is said to be self-correcting. However, this ideal can fall short in the face of challenges like publication bias, but the point here is to understand the intrinsic nature of uncertainty in the scientific process. A study of the estimation of maternal mortality provides a good example of the principle of uncertainty (see Figure 2.1). Hogan et al. (2010) published maternal mortality estimates for 181 countries. Some countries, like the United States, have vast amounts of data in vital registries that attempt to track all births and deaths. Countries with vital registries struggle with changing definitions over time, but the uncertainty interval around their estimates is typically small because a lot of good data has been collected over a significant period. In many low-income countries, the situation is very different, however. Figure 2.1 shows only four data points! No wonder the uncertainty interval is so great. The takeaway message is that there is uncertainty in everything. No single estimate can be considered “The Truth.” Instead, we must focus on the origin of estimates and recognize the limitations of what we know or what is being reported. Figure 2.1: Estimates of maternal mortality 1990-2010. LEFT: United States. RIGHT: Afghanistan. Squint and you will see that the confidence intervale for the US estimate is less than 10 out of 100,000, compared to more than 3,000 out of 100,000 in Afghanistan. Source: Hogan et al. (2010), http://bit.ly/1JBCelO So how many women die during pregnancy or within 42 days of delivery? The same research group that published Hogan et al., the Institute for Health Metrics and Evaluation, estimated that there were 292,982 maternal deaths globally in 2013, with a 95% uncertainty &lt;&lt;EG: Do you mean confidence interval?&gt;&gt; interval ranging from 261,017 to 327,792; that’s a range of 66,775 (Kassebaum et al. 2014). Although this number seems high, global statistics include a world population of more than 7 billion people!10 2.2 Stages in the Research Process Just as every story has a beginning, middle, and end, every scientific article has an introduction, methods, results, and discussion (known as the IMRaD format). Follow these steps and you will have all of the pieces you need to write each section. 2.2.1 FIND A RESEARCH PROBLEM Every study begins with a research problem. A research problem represents a gap in our knowledge. In academic research, this is another way of saying a gap in “the literature.” Usually when people speak of “the literature,” they mean scholarly or peer-reviewed journal articles. In addition, a body of work called “grey literature” is more encompassing and harder to search systematically. Grey literature sources are typically disseminated through channels other than peer-reviewed journals. Examples include technical reports or white papers published on the web. Research problems are typically broad. For instance, stakeholders might want to know how to increase the use of mosquito nets for children under 5 years of age or whether all children should receive deworming medication prophylactically. These problems have something in common: they are solvable. In his introductory text on behavioral research methods, Leary (2012) writes that this is another key criterion for scientific research. The problems must be solvable. This does not mean easy; it just means that we can use systematic, public methods to gather and analyze data on the problem. In other words, it is possible to devise a method for studying how to get more parents to ensure that their kids sleep under a mosquito net every night, but we don’t yet have a scientific method for determining whether there is a mosquito afterlife where these pests get to buzz around for all of eternity. 2.2.2 ARTICULATE A RESEARCH QUESTION To study a broad research problem, we must narrow our focus to a more specific research question. D. de Vaus (2001) says there are essentially two types of research questions: Descriptive—What is going on? Explanatory—Why is it going on? For example, to study the uptake or use of bed nets, we might ask a descriptive research question like, “How many children sleep under bed nets?” But this question is too general. Children of what age? Living where? We also need to define what we mean by sleeping under a bed net. In this line of research, it is common to ask about the previous night, as in the night before the survey.11 A better way to phrase the question might be, “What percentage of children under 5 years of age in Kenya slept under an insecticide treated net the previous night?” An explanatory research question on the same topic might be, “What are the predictors of the use of insecticide treated net among children under 5 years of age in Kenya?” Remember this: Good research questions are FINER: feasible, interesting, novel, ethical, and relevant (Hulley, Newman, and Cummings 2007). Feasible Some resarch questions will take a long time to answer, cost too much, require too many participants, require skills or equipment that you do not have, or will be too complex to implement. Interesting Research requires funding and effort. If you do not ask a sufficiently interesting question, you will not get funding. If you manage to get funding but lose interest in the question, you might not finish. Unlike other domains, global health research tends to have long timelines, and it’s important to work on things you will find interesting over the long term. Novel Replication is an important part of science, but the majority of funding goes to research that asks new and interesting questions. Ethical It would be very interesting to create a prison simulation to determine whether charactristics of the people or situation cause abusive behavior, but this would not be ethical because it could lead to the harmful treatment of research subjects. Right? Relevant In addition to being interesting, a research question should also be relevant. The answer should move the field forward in some way. Making this determination requires a thorough review of the literature and conversations with senior colleagues. And here’s another: Good clinical questions include PICO: Patient, population or problem; intervention, prognostic factor or exposure, comparison, and outcome. P Patient, Population, or Problem I Intervention, Prognostic Factor, or Exposure C Comparison O Outcome We could use PICO to develop a research question about the efficacy of mosquito bed nets in preventing malaria. The problem is malaria infections. The population is children under 5 years of age. Because intervention studies tend to be smaller in reach than nationally representative surveys, we might add “living around the Lake Victoria basin in Kenya.” The intervention is the application of an insecticide-treated net. [Prognostic factor refers to covariates that could influence the prognosis of the patient. An exposure would be something that we think might increase the risk of an outcome.] The comparison group might be children living in families who are provided an untreated bed net. One outcome measure could be the rate of parasitaemia after the intervention. Importantly, we combine all of these elements into a single research question: Among children under 5 years of age living around the Lake Victoria basin in Kenya, are insecticide-treated mosquito nets more effective than untreated nets at preventing parasitaemia? Here is some good advice if you are writing qualitative research questions: 2.2.3 IDENTIFY RELEVANT THEORY Leary (2012) defines a theory as “a set of propositions that attempts to explain the relationships among a set of concepts.” In quantitative research, you could replace “propositions” with “hypotheses” and “concepts” with “variables.” Some studies set out to develop a theory (inductive), while others may test a theory (deductive). However, much of applied global health is atheoretical. Many impact evaluations fit the label of “black box evaluations,” meaning that they do not focus on why programs do or do not have an impact. The evaluation is not guided by theory, and the hypotheses are as simple as “the program will have an impact on the outcome.” White (2009) outlines a strategy for changing this and moving to theory-based impact evaluations (White 2009). Chapter 6 discusses developing a theory of change and logic models. A good resource for understanding the (potential) role of theory in global health is the journal Social Science &amp; Medicine. For instance, Green et al. (2015) frame their cluster randomized trial of an economic assistance program for women in terms of the literature on engaging men in efforts to reduce intimate-partner violence (IPV). The rationale for addressing IPV through men’s discussion groups is based on the belief that socially constructed gender norms about inequality are a root cause of violence (Barker et al., 2010). Girls and boys learn gender roles and normative behavior, such as gender-based violence, by watching others and observing rewards and punishments; this is the basis of social learning theory (Bandura, 1973), one of several theoretical etiologies of IPV (for a review see Dixon and Graham-Kevan, 2011). Understanding and addressing the connection between violence and masculinity is also critical, gender theorists argue (Jewkes et al., 2014). ‘Gender-transformative’ programs are therefore designed to change gender norms and to promote gender equality among men and boys, most often by raising awareness and targeting attitudes throughout the social ecology. Search for the words “theory” or “conceptual” in the Introduction or Discussion sections of articles to see how authors frame their work in theoretical and conceptual terms. 2.2.4 DEVELOP HYPOTHESES The logical approach in quantitative research is often deductive. After a theory is formulated, the investigator develops research hypotheses that are tested. A hypothesis is an a priori prediction about what will occur or about how constructs are related. When the hypothesis is supported by the data, the underlying theory is validated. A well-designed study will have a higher impact as other researchers consider the evidence in support of the theory. For a hypothesis to be scientific, it should be falsifiable, or testable. Considering our silly example from earlier, the following statement is not a research hypothesis because it cannot be tested: “if a mosquito is killed, it goes to mosquito heaven.” Although this assertion may be true, we cannot test this hypothesis. Scientific validation requires the possibility of falsification, so hypotheses must be engineered to potentially fail. Not all studies test hypotheses, however. Qualitative research is generally inductive and “hypothesis generating.” Developing hypotheses in the course of planning a study, however, can be daunting. Not all hypotheses are obvious at first. Can you prove a theory? Some people advocate against the free distribution of insecticide-treated nets (ITNs) because they believe that there is a “sunk cost” effect of spending money for a bed net, that is, people will use the net more to justify their purchase (Arkes and Blumer 1985). In this case, the theory is one of sunk costs directing behavior. Cohen and Dupas (2010) designed a study to test the falsifiable hypothesis that people who paid a non-zero price for an ITN would use the ITN more than those who received the ITN for free. Notably, they did not find support for this hypothesis. So the theory of sunk costs is rejected, right? Not necessarily. Leary (2012) offers some helpful advice for thinking about proof and disproof. Proof is logically impossible, whereas disproof is practically impossible. Proof is Not Possible First, state the theory and hypothesis as an if-then statement. For example, “If the theory of sunk cost effects is true, then people who pay for an ITN will be more likely to use it than people who get an ITN for free.” If the theory is true, then the hypothesis will be true. Next consider what happens when you flip this statement. If the hypothesis is true, does it mean that the theory is true? For example, a child has a fever, and my theory is that the fever is a symptom of malaria. My hypothesis is that this child must have been bitten by a mosquito, therefore. I pull back the child’s sleeve and find a few mosquito bites. My hypothesis was supported by data! So therefore, if my hypothesis is correct, my theory is proven, and the child must have malaria, right? Well, no. Although the child was bitten by a mosquito, the child may have been in the eastern United States where we don’t worry about malaria. So in this case, the hypothesis was true, but it didn’t prove the theory. Disproof is Possible, but Uncommon What if the hypothesis was not supported because the child was not bitten by mosquitoes? Could my “theory” be true—could the child’s fever still be malaria? No. If the hypothesis is derived from the theory and the hypothesis is not supported, the logical inference is that the theory is wrong. However, we still shy away from concluding that the theory is wrong. We consider the complexity of the broader picture. A study like that of Cohen and Dupas (2010) could fail to reject the null hypothesis that net use does not differ between free and subsidized clients—thus not supporting the hypothesis of different use rates—but there are many practical reasons that no differences were detected. For example, maybe they used a questionnaire to measure bed net use that was affected by a bias, such as the idea that “Good parents use bed nets,” that skewed the responses. The counting technique itself may have been flawed and hid the difference in net use as a result. The possibilities are endless, which is is partly why journals are hesitant to publish null results. So what do we learn when a study does or does not support a theory? In short, no one study is enough to lead people to discard a theory. But several null results might be. Conversely, no study ever proves a theory, but an accumulation of study results showing support for the theory-derived hypothesis builds confidence in the theory. This is particularly true when the studies are conducted by different researchers across different populations and results are triangulated using multiple methods. How do researchers find out whether several experiments failed to support a certain theory if journals are reluctant to publish null results? And if negative evidence is missing, is positive evidence overrepresented in the literature? Yes. This is the problem of publication bias, or the file drawer problem, and there is not an easy answer. Efforts like AllTrials to register and report the results of all trials, regardless of outcome, represent a step in the right direction. 2.2.5 SELECT A RESEARCH DESIGN As Glennerster and Takavarasha (2013) explain in their practical guide to running randomized evaluations, different research questions require different research designs. The most common designs can be lumped into the following four categories: Descriptive (quantitative or qualitative) Correlational/observational Quasi-experimental Experimental Deciding on the best design to approach a particular research question can be overwhelming. Developing a flow chart can help. Figure 2.2: Research design choose your own adventure. PDF download, https://drive.google.com/open?id=0Bxn_jkXZ1lxuWkhFcTUzdWVkZ0E These designs are introduced in Chapter 5 and Parts V and VI are dedicated to explaining them in detail. 2.2.6 IDENTIFY KEY VARIABLES A variable is a characteristic or aspect that can take on different values (Diez, Barr, and Çetinkaya-Rundel 2015). A variable can be numeric or categorical. Numeric variables can be further classified as continuous or discrete. You can add, subtract, and take the mean of continuous and discrete numeric variables. The distinction between continuous and discrete is that discrete numeric variables cannot be negative and must be whole numbers. For instance, the number of mosquitoes captured in a light trap is a discrete number. Conversely, the blood meal volume observed in trapped mosquitoes is continuous because volume does not need to be a whole number. A continuous variable can also be classified as ‘interval’ or ‘ratio’. The key difference is that ratio variables have a meaningful zero, so it’s OK to compute a ratio. For instance, if you trap 10 mosquitoes and I trap 20, I collected more mosquitoes by a ratio of 2 to 1. Interval variables like temperature don’t have this meaningful zero. I’ve never come across other example of interval variables besides temperature, so suggest an edit if you think of one. The type of mosquito trapped is an example of a categorical variable (e.g., Anopheles, Aedes, Culex). Specifically, type is a nominal or unordered categorical variable. If you ask someone to rate how often they’ve been bitten by any mosquito in the past week—let’s say on a 4-point scale from never to often—their response would be an example of the other type of categorical variable: ordinal. An ordinal variable is what you think: some categories are greater than others. A variable with two levels (yes and no) is often be called binary. Some discipline,s such as economics, also refer to categorical variables as “qualitative” variables, not to be confused with qualitative methods. Variables are also classified as dependent and independent variables, depending on how they are used. In studying the impact of ITN use on parasitaemia, parasitaemia is the dependent variable and ITN use is the independent variable. Or maybe the predictors of ITN use are being studied. In this case, ITN use is the dependent variable and other factors, like education level, cultural background, or income level, are independent variables. Here are some other ways these terms are described in the literature. Dependent Variable (DV) Independent Variable (IV) Response Explanatory Outcome, Endpoint Predictor, Risk Factor Y X Qualitative researchers do not typically talk about the measurement of dependent and independent variables; rather, they talk about concepts and constructs. In qualitative research, the goal is not to test a hypothesis that some independent variable predicts some dependent variable. Instead, the goal is to explore a phenomenon and describe it in as much detail—thick description—as possible. Some researchers quantify qualitative data (e.g., as frequencies), however, so it’s not necessarily a number-free zone. 2.2.7 SELECT APPROPRIATE RESEARCH METHODS If research designs are strategies for answering research questions with the best possible evidence, then research methods are the tactics for obtaining the evidence (Chapter 8). Often methods are divided into three broad categories: Quantitative Qualitative Mixed Investigators use quantitative methods to collect and analyze numerical data. These data may be binary or dichotomous** (e.g., whether a patient is hospitalized or not), they may be categorical (e.g., to which wealth quintile does a person belong), or they may be continuous (e.g., hematocrit level). A good example of a quantitative method is a survey in which people are asked to answer questions with fixed response options or to provide numeric values, such as their monthly income. Lab tests resulting in disease classifications (yes/no) or in a measurement such as the number of white blood cells in a blood sample are also examples of quantitative methods. Qualitative methods focus on nonnumeric data. Participant observations, interviews, and focus-group discussions are common qualitative methods used in global health research. Qualitative methods are well suited for obtaining thick descriptions and for exploring new areas of research. Qualitative methods are often considered less rigorous because they are flexible and do not lead to the same types of hypothesis testing and results as quantitative methods. But this is not true; rigor is a characteristic of how the methods are applied rather than the methods themselves. The choice of methods should be based on the research question. Impact evaluations often use quantitative methods, but there is no 1-to-1 match between research designs and research methods. Many studies incorporate both quantitative and qualitative methods as mixed methods. Sometimes the goal of mixing methods is to triangulate results with respect to the same research question. At other times, qualitative work is used to develop the tools and measures needed in a subsequent trial. When qualitative work follows a quantitative phase, the goal is often to explain or explore results in more depth, which was not possible with the quantitative data alone. Increasingly randomized control trials (RCTs) complement their use of quantitative methods with qualitative inquiry (O’Cathain et al. 2013). Alaii et al. (2003) provide a good example. These investigators incorporated qualitative interviews on nonadherence into a larger randomized trial of the efficacy of ITNs on reducing child morbidity and mortality in Kenya (Phillips-Howard et al. 2003). They wanted to better understand why people, particularly children under the age of 5, were not using their ITNs correctly. Alaii et al. found that more than a quarter of individuals were nonadherent, often due to excessive heat. 2.2.8 SPECIFY AN ANALYSIS PLAN Although we are not specifically studying data analysis in this book, a well-considered analysis plan is an important component of both qualitative and quantitative research proposals. On a practical level, the plan must be carefully evaluated to ensure that the study design will produce the data needed to conduct the desired analysis. More generally, however, prespecified and registered analysis plans promote transparency and confidence in study results. Researchers make tens or hundreds of small decisions during the course of data processing and analysis that can alter the results. Although many of these decisions are overwhelmingly legitimate and defensible, fraudulent skewing of research results is not unheard of. The biggest problem occurs when these decisions are made after seeing the data. For instance, a researcher might run a test and find that a relationship is not statistically significant. The researcher then makes a small change in how a variable is defined and runs the test again. This time the relationship is significant, which greatly increases the likelihood that the study is published. It’s easy to see where the temptations arise. The following video provides a great introduction to some of these thorny issues, which are revisited in a later chapter. For now, we highlight the benefits of pre-registration, or publishing your analysis plan in advance of obtaining the data. With drug trials, for example, the research is regulated by the FDA and registration is required on clinicaltrials.gov. Other areas may not have a regulatory registration requirement, but certain high-impact journals might require that you do so in order to publish the results. 2.2.9 OBTAIN ETHICAL APPROVAL Research involving human subjects must be reviewed and approved by an institutional review board (IRB) prior to commencing. According to the U.S. Department of Health &amp; Human Services, 45 CFR 46, “research” is defined as a systematic investigation, including research development, testing, and evaluation, designed to develop or contribute to generalizable knowledge. As shown in Figure 2.3, several categories of research with human subjects are [exempt]from approval (https://www.hhs.gov/ohrp/regulations-and-policy/regulations/45-cfr-46/#46.101). Figure 2.3: Is the human subjects research eligible for exemption?; Source: http://bit.ly/2brlbKR. Unless a study is exempt from review or meets the requirements for expedited review, the study proposal is expected to be reviewed by the full committee at a regularly scheduled IRB meeting. For international research, sufficient time is needed for the study proposal to be reviewed by an in-country IRB in addition to the IRB of the investigator’s home institution. “Sufficient” could mean 6 weeks or 6 months, so proper planning is imperative. Global health research is collaborative by design and necessity, so the local expertise of partners is often sought with regard to IRB procedures. 2.2.10 RECRUIT A SAMPLE AND COLLECT DATA Unless a secondary analysis of existing data (e.g., medical record review)is being used, a sampling strategy and procedural outline for data collection must be identified. These topics are covered in Chapters 9 and 8, respectively. 2.2.11 ANALYZE THE DATA AND WRITE UP THE RESULTS Complex global health research typically involves specialists in data analysis, most commonly biostatisticians. These experts provide guidance early in the process of study design to ensure that the raw materials needed for the planned analysis are available. Writing manuscripts in global health research is a collaborative process. In some disciplines, like economics, published studies tend to have very few authors. Some medical studies conducted at multiple sites can have dozens of coauthors. The International Committee of Medical Journal Editors suggests that authors be defined by four criteria: Substantial contributions to the conception or design of the work; or the acquisition, analysis, or interpretation of data for the work; AND Drafting the work or revising it critically for important intellectual content; AND Final approval of the version to be published; AND Agreement to be accountable for all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved. Any contributors who do not meet these criteria should be acknowledged. Author order is handled differently in different disciplines. In psychology, for instance, the author listed first is supposed to be the “lead” author who contributed the most. The last author could be the person who contributed the least, or it could be the senior-most member of the “lab” that produced the work. In economics, author order tends to be alphabetical, but not always. For senior researchers, author order may be an afterthought, but junior scholars need to establish a record of first author publications to signal their emergence as independent scientists. 2.2.12 MAKE YOUR RESEARCH HAVE AN IMPACT It’s not uncommon in global health research for an intervention study to span 5 to 10 years from proposal to final publication in the scientific literature. And there is an important distinction between publication and impact. While specific metrics track the impact that journals, articles, and authors have on a field, impact also implies real-world change. Does the research lead to new policies or programs? Does it change the way people think? Is the work utilized by fellow researchers? It is tempting to think that high-quality research and significant, salient results will spread through the discipline and will be built upon in good faith. In reality, however, there is a gap between research and practice/policy. Many worthy ideas for which the data comparisons yielded a p-value less than 0.05 have been shelved prematurely. In response to this stagnation, some have advocated for a research utilization framework to promote the advanced planning needed to engage the potential end-users of research before the study even collects any data. Figure 2.4: Research utilization framework. Source: http://bit.ly/2j1dLfL References "],
["literature.html", "3 Searching the Literature 3.1 Start with Systematic Reviews and Meta-Analyses 3.2 Devising a Search Strategy 3.3 Use a Reference Manager 3.4 Why Does Any of This Matter? Share Feedback", " 3 Searching the Literature The starting point of every research study is a literature review. To know where you are going, you need to know where the field has been. In some ways, technology makes this easier than it has been in the past, but we are swimming in information, and the pool gets deeper every day. A lot deeper, actually. In 2012, Google’s former CEO Eric Schmidt said that we create as much information every 2 days as we did from the beginning of time through 2003. Two days! This is not just a bunch of cat photos either. Global scientific output doubles every 9 years. And with “predatory journals” on the rise, this rate will only speed up. With so much noise around us, good information can be hard to find. This chapter presents a strategy for quickly getting a sense of the state-of-the-art of any health research topic and an outline for searching the literature for primary sources. 3.1 Start with Systematic Reviews and Meta-Analyses Repeat after me: I will not start my research by Googling “malaria.” I will not start my research by Googling “malaria.” I will not start my research by Googling “malaria.” If your research topic is malaria and you are not sure whether the vector is mosquitoes or monkeys, then Wikipedia is probably a good place to start. There is no shame in that. Otherwise, begin by searching for relevant systematic reviews or meta-analyses. A good review is far superior to Googling “malaria” any day. 3.1.1 META-ANALYSIS A meta-analysis is a quantitative approach to reviewing research in which the results from multiple studies are combined to estimate an overall effect size. The results of a meta-analysis are typically summarized in a forest plot like the one shown in Figure 3.1. Let’s take a look at this helpful guide from Ried (2006) that breaks it all down. Figure 3.1: Source: Ried (2006), http://bit.ly/2j9pfSz A forest plot summarizes the results of several studies that measured the effect of the same intervention on the same outcome. One study result is described and plotted per row, and the overall effect (i.e., the “pooled” or “meta” effect) of all the studies is displayed at the bottom. The study sample is divided into an intervention arm and a control arm, presented in the n/N format where n represents the number of participants who experienced a certain outcome and N is the total number of participants in the study arm. For example, 141 were people assigned to the intervention group in Study A. Of these 141 people, 1 person experienced the adverse outcome that the forest plot summarizes. Next, a plot of the effect size and the confidence interval is created. An effect size is a measure of the strength or magnitude of a relationship, such as the relationship between taking a medicine and experiencing a bad outcome. This guide shows a specific type of effect size: relative risk. Each study’s point estimate of the relative risk is plotted around a line of “no effect.” A risk of 1 means that there is no difference between the intervention and control groups. When the outcome is something bad, like death, the intervention should be designed to reduce the risk, which is represented by a risk ratio less than 1. We talk about “estimates” of the effect because research can only approximate the truth. Every estimate has some uncertainty. In a forest plot, uncertainty is represented by confidence intervals. The size of the effect estimate is based on how much the study contributed to the meta-analysis. All studies are not created equal, and the weight parameter lets researchers account for these differences in the analysis. Each estimate point is surrounded by a confidence interval (typically 95%) that is summarized numerically in the final column. Basically, if a study is repeated 100 times, the effect size is expected to be within this interval 95% of the time. When this interval crosses the line of no effect, the effect could be null or could even run in the opposite direction. In this case, the result is not “statistically significant.” Finally, the test for heterogeneity is presented toward the bottom. Heterogeneity means diversity (and is the opposite of homogeneity). Heterogeneity in a forest plot refers to the diversity in effect size estimates across studies. Heterogeneity complicates the interpretation of a meta-analysis; it signals that we might be comparing apples and oranges. For instance, the intervention may work differently in different contexts, and the included studies were gathered from all over the world. In such a case, it might not make sense to attempt to determine one overall meta effect size from a comparison of the studies. The first way to assess heterogeneity is to consider the plots. Do the confidence intervals from each study form a vertical column, even if the point estimates shift between them? If so, heterogeneity is probably low. Heterogeneity can also be summarized numerically. Two estimates of heterogeneity are often presented: chi-square (χ2) and I^2, which is generally preferred. Values greater than 75% may indicate that a change in the meta-analysis method (random vs fixed effects) is needed. If heterogeneity is reported with a high I^2 value, authors should address this in the methods or limitations section of the study. Lewis and Clarke (2001) discovered that the first forest plot was published in 1978, and first used in a meta-analysis in 1982. The name lagged behind, appearing first in 1996, apparently referring to the tree-line optics typical of most forest plots. Example For example, consider the meta-analysis by Radeva-Petrova et al. (2014). The authors reviewed 17 studies of the effects of chemoprevention on pregnant women living in malaria-endemic areas. With their review, they set out to answer this basic question: Do women who take antimalarial medication during pregnancy have a lower risk of getting infected with malaria, and thus a lower risk of experiencing the bad health outcomes associated with malaria? One indicator of malaria infection is parasitemia, or the presence of malaria parasites in the blood. If chemoprevention has some preventive effect, less parasitemia should be observed among women exposed to the medication (i.e., treatment). Few interventions are 100% effective, so scientists often talk about reductions in the risk of bad outcomes like malaria. The forest plot shown in Figure 3.2 displays the results of 10 studies (8 trials) of cases of parasitemia among 3,663 pregnant women who were randomized to an intervention group (n=2,053) that received a preventive antimalarial drug or to a control group (n=1,610) that received a placebo (no drug). Figure 3.2: Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj Details about each study are reported in separate rows in this figure. The study by Shulman et al. (1999) in row 6 found that 30 of the 567 women in the intervention group tested positive for parasitemia (i.e., malaria). Comparing this number to 199 of the 564 woman in the control group yields a risk ratio of 0.15, i.e., (30/567)/(199/564) = 0.15. In other words, the chemoprevention reduced the risk of parasitemia by 85%. This is a huge effect size! The effect size for each study is presented in the far-right column and is depicted graphically in the size of the point estimate square. All point estimates fall to the left of the line of no effect (&lt;1), thus indicating a favorable effect of the chemoprevention intervention, i.e., reduced risk of parasitemia. A risk ratio of 1 would indicate no difference in risk, and a ratio &gt;1 would mean the risk was higher among the intervention group, thus favoring the control group (with no treatment). The overall (pooled) effect size is 0.39, or a 61% reduction in the risk of parasitemia. Calculating this pooled effect size is not as simple as averaging the effects of the 10 studies because the studies were not given equal weight, as shown in the “weight” column. For instance, Greenwood et al. (1989) had a sample size of only 34 children (i.e., 21+13=34). As a result, the effect size estimate is very noisy. The 95% confidence interval is very large and crosses 1. Consequently, the weight of this study is only 6.7%, which is lower than the others. Simply put, studies with weaker research designs, such as this one, have less weight in the pooled analysis. A single forest plot provides a summary of the best available evidence and an estimate of the overall effect size, along with uncertainty intervals. A Google search cannot begin to offer that! Exploring meta-analysis 3.1.2 SYSTEMATIC REVIEW How did Radeva-Petrova et al. (2014) find these studies in the first place? Through a systematic review of the literature. Most, if not all, meta-analyses are completed as part of a systematic review of the literature. Every systematic review is a type of literature review, but not every literature review is a systematic review, even if it is done systematically. Figure 3.3: Literature reviews, systematic reviews, and meta-analyses As shown in the table below, a systematic review requires a number of steps that represent good practice, but they are too time-consuming for an initial general literature review. Nevertheless, the general process of creating a systematic review is important to evaluating the quality of the reviews gathered, and this process provides solid foundation for this part of the research process overall. Table 3.1: Comparing systematic reviews and literature reviews. Systematic Reviews Literature Reviews The goal of a systematic review is to be comprehensive and to include every relevant article. Literature reviews, on the other hand, do not follow such rigid or explicit methods. They are not expected to be exhaustive. For this reason, most systematic reviews are conducted by teams, given the large scope of the data initially collected for most research topics. Literature reviews can usually be conducted by a single person rather than a team Like any other aspect of research, however, systematic reviews must define and follow a method that can be replicated. Literature reviews, on the other hand, don’t have to follow such rigid methods or make the methods explicit. Most systematic reviews preregister the research plan, meaning that the authors submit their planned methods to a registry like PROSPERO prior to conducting the study. Preregistration gives other researchers confidence that the team is not selectively choosing advantageous results at the end to make an interesting paper. This registration informs other researchers that a group is working on a certain area of study, which can discourage duplicate research efforts that may, therefore, fail to be published. Not the case for literature reviews. These preregistration plans include a specific search strategy using specific search terms for individual scholarly databases so other researchers can recreate the search. It’s a good idea to do the same for a literature review, even if not a strict requirement. Importantly, both inclusion criteria and exclusion criteria must be clearly outlined when a systematic review is undertaken. One inclusion criteria might be that assignment to study arms had to be random; an exclusion criteria might be all studies without a control arm that used a placebo. Most systematic searches specify several, if not many, criteria regarding which studies to include or exclude. Team members screen the search results and sort them according to these criteria, beginning with titles and abstract reviews and moving to full-text reviews later. Screening for a literature review is typically less intensive. In systematic reviews, specific details are extracted from every study included, such as numbers of participants, methods, analysis techniques, and key outcomes. An annotated bibliography might suffice for a literature review. In addition, the research team formally assesses the quality of each study, including the potential for bias, and these assessments are considered when the results are synthesized. This process is more ad hoc for literature reviews. Where to find systematic reviews Three excellent sources for finding systematic reviews (and meta-analyses) in global health are the Cochrane Library, the Campbell Collaboration, and 3ie. Many of the reviews in these databases can be accessed by searching within PubMed using the Clinical Queries feature. How to read systematic reviews Abstract and plain language summary Cochrane reviews follow a standard format that can look overwhelming at first, but this format actually makes them quite easy to read and understand. As with most journal articles, Cochrane reviews begin with an Abstract and a Plain language summary, which can be helpful for newcomers to a the topic. For example, Radeva-Petrova et al. (2014) include the following passage in their plain language summary: For women in their first or second pregnancy, malaria chemoprevention prevents moderate to severe anemia (high quality evidence); and prevents malaria parasites being detected in the blood (high quality evidence). It may also prevent malaria illness. We don’t know if it prevents maternal deaths, as this would require very large studies to detect an effect. This paragraph brings us up to speed with the state of the science for preventing malaria and its effects among pregnant women living in malaria-endemic areas (and points to some gaps in the literature). Google does not filter the evidence in this manner, and starting with systematic reviews pays off almost every time. Summary tables Next come the Summary tables, such as the one presented below from Radeva-Petrova et al. (2014). These tables provide enough information to make an initial judgment about the study. Figure 3.4: Malaria chemoprevention for pregnant women living in endemic areas. Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj First, the comparative risk column shows the assumed risk among the control group. For instance, the risk of antenatal parasitemia is 286 events per every 1,000 people. This is the median control group risk across 8 trials of 3,663 women. The relative risk is 0.39. Recall that this is the pooled, or “meta,” effect size. The corresponding risk among the intervention group is 286*0.39=111 per 1,000 people.12 As shown in the final column, the quality of this evidence is rated “high.” Here, the authors are referring to GRADE criteria, a systematic approach to evaluating the quality of empirical evidence: High—Further research is very unlikely to change our confidence in the estimate of effect. Moderate—Further research is likely to have an important impact on our confidence in the estimate of effect and may change the estimate. Low—Further research is very likely to have an important impact on our confidence in the estimate of effect and is likely to change the estimate. Very Low—We are very uncertain about the estimate. Background The abstract contains much important information, such as summary text and tables, and sometimes even forest plots. The abstract is often followed by a Background section, typically a short overview that explains which knowledge gaps the review is intended to fill. Radeva-Petrova et al. (2014) have used this section to present a conceptual framework for malaria prevention during pregnancy. Figure 3.5: Drugs for preventing malaria in pregnancy: conceptual framework. Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj Methods The Methods section details how the review was organized and conducted. The purpose of this section is to provide enough detail to enable other researchers to replicate the review. These are the main components:13 A description of the population and intervention The key outcomes of interest The search strategy and databases Inclusion and exclusion criteria for studies reviewed Procedures for extracting information from each study Procedures for assessing bias and conducting a meta-analysis (if one is included) Results The Results section typically begins with details about how many primary articles were identified, screened, and excluded. This information is typically presented graphically in a flow diagram like the one below from Radeva-Petrova et al. (2014). Figure 3.6: Study flow diagram. Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj Once the included studies are identified, the review authors usually report on the quality of the evidence presented in each study. The nature of these sources of bias are discussed in a later chapter, but the heatmaps are introduced here. Although they may appear complex at first, they provide a useful summary of bias. Basically, the green circles indicate that the authors believe that the research is not affected by bias; conversely, red indicates the likely presence of bias, in the authors’ judgment. The less bias, i.e., the more green circles, the higher the quality of the studies included in the review. Figure 3.7: Risk of bias summary: review authors’ judgements about each risk of bias item for each included trial. Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj Discussion and conclusions The discussion section provides a short summary of the findings, commentary on the quality of the evidence, and thoughts about what the review adds to the existing literature on the topic. The discussion tends to be short relative to the size of the overall review. The discussion section is often followed by a brief statement of the authors’ conclusions. Here, the authors frame the overall results in terms of their implications for practice and research. Radeva-Petrova et al. (2014) conclude: Routine chemoprevention to prevent malaria and its consequences has been extensively tested in RCTs, with clinically important benefits on anemia and parasitaemia in the mother, and on birth-weight in infants. Simply put, the final conclusion of this review is “chemoprevention works.” Appendices Often, appendices include table after table of data included and (sometimes) excluded studies. They are often followed by dozens of forest plots if the systematic review includes a meta-analysis with several outcomes or populations of interest. Reading the appendices provides a sense of the mechanics behind a systematic review. Radeva-Petrova et al. (2014) wrap up on page 120! 3.2 Devising a Search Strategy Not every topic has been the subject of a recent systematic review or meta-analysis. In such cases, a search of the primary literature is needed. The first step to a successful search is establishing a clear definition of the objective. 3.2.1 ASKING A RESEARCH QUESTION Again, the helpful mnemonic PICO, introduced in the previous chapter, can guide the formation of a good clinical question. P Patient, Population, or Problem I Intervention, Prognostic Factor, or Exposure C Comparison O Outcome As an example, PICO can be used to develop a well-focused, searchable research question on treating malaria during pregnancy. The problem is malaria infections. The population is pregnant women living in malaria-endemic areas. Not every clinical question involves testing of a treatment or intervention, but we’ll focus a lot on these types of questions in this book. For the example at hand, the intervention is malaria chemoprevention. [Prognostic factor refers to covariates that could influence the prognosis of the patient. An exposure would be something that we think might increase the risk of an outcome.] Similarly, not every question involves a comparison group.14 In this example, the comparison is no intervention or a placebo in place of the drug being administered. There are many potential outcomes for treating malaria; in this case, the outcome is parasitemia. Combining all of this information yields a research question like this one: Among pregnant women living in malaria-endemic areas, is chemoprevention more effective than a placebo at preventing parasitemia? Remember, not all research questions fit neatly into the PICO format. Chapter 2 included another mnemonic helper: FINER. 3.2.2 APPROACHES With your basic research question outlined, you’re ready to begin searching. In the beginning you might take a Quick and Dirty™ approach to get started. Eventually you’ll need to graduate to a proper search strategy to be more systematic, even if the end goal is not a formal systematic review. Quick and Dirty™ A reasonable initial approach is to find a few recent articles to get a quick sense of what is out there. Possibly, Google Scholar could be helpful here. For instance, an advanced Google Scholar search for PICO terms “malaria pregnant chemoprevention parasitemia” (limited to recent years) identified a paper by Braun et al. (2015) on the use of intermittent preventive treatment in pregnancy (IPTp) with sulfadoxine–pyrimethamine (SP)—a specific type of chemoprevention—on malaria infections among pregnant women in western Uganda. Customize your Google Scholar experience by clicking on the gear icon. Enable use of a bibliography manager, and click on “Library links” to add your library to get links to full text. An article’s keywords provide a good starting point for any search. Not all journals print keywords, however, but if it does, they are usually listed directly following the abstract. Following the abstract and the keywords is the Introduction. Some journals, especially in the fields of medicine and public health, have very brief introduction sections that might not be of much help. The Discussion section, which usually directly follows the Results section, may also hold new search leads. Authors typically use the discussion to link the study results to the existing literature to demonstrate how the results add to what is already known. After the introduction and discussion sections, the references provide a plethora of useful information. First, the names of journals that publish studies in the field are always listed with their article titles. . If a certain journal appears commonly, a scan of the journal’s table of contents for recent issues could be useful.15 Journal Citation Reports is a useful resource about the scholarly journals in a field. This annual report ranks the journals in each field according to impact factors. Impact factors are one metric used to evaluate the importance of a journal in its field. More systematic Planning and documenting the search In both formal and informal systematic reviews, it is important to plan and document the search. Literature reviews do not need to be as thorough as systematic reviews, but the approach can be nicely overlaid. Consider the work of Radeva-Petrova et al. (2014) for some inspiration. Every good systematic review includes a table or appendix like this one to make the method reproducible. In other words, running this search query at the same time on two different computers should yield the same results. Figure 3.8: Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj For the purposes of a literature review, the results do not need to be directly repeatable by others, but you should be certain that you recreate your search if needed. Most databases require that users create an account within the database to log in and save the search approaches. This makes it easier to retrace the search steps later. Also, differences in the design of each database and interface often require distinct ways of customizing a search. To conduct an actual systematic review for publication—as opposed to just searching the literature systematically—research librarians are the “go to” resources for building search strategies. Selecting a database As you can see from the table, Radeva-Petrova et al. (2014) searched five databases. MEDLINE is probably the most well known of this group. PubMed includes the MEDLINE database. PubMed is a very good place to start to find health-related studies. Research librarians are also an excellent resource to determine whether other databases are suitable for the topic under research. Generating search terms Once the proper database is identified, specific search terms are needed. These usually coincide with the keywords published in related articles. MeSH is another resource for identifying articles to include in a literature review or a systematic review. MeSH, which stands for “Medical Subject Headings,” is a controlled vocabulary thesaurus that is used to index articles in PubMed. This thesaurus is helpful because there are often many ways to refer to the same phenomenon. For instance, the MeSH term for “breast cancer” is “breast neoplasm.” When a search for “breast cancer” is conducted in PubMed, the database helps by casting a wider net using the MeSH term automatically. Notably, however, PubMEd does not always do this! What PubMed includes in the search can be viewed by clicking on the search details link after running a search): &quot;breast neoplasms&quot;[MeSH Terms] OR (&quot;breast&quot;[All Fields] AND &quot;neoplasms&quot;[All Fields]) OR &quot;breast neoplasms&quot;[All Fields] OR (&quot;breast&quot;[All Fields] AND &quot;cancer&quot;[All Fields]) OR &quot;breast cancer&quot;[All Fields] Not surprisingly, breast cancer is referred to using many, many terms! The following entry terms are indexed to the MeSH term “breast neoplasms” by humans at PubMed: Breast Neoplasm Neoplasm, Breast Neoplasms, Breast Tumors, Breast Breast Tumors Breast Tumor Tumor, Breast Mammary Neoplasms, Human Human Mammary Neoplasm Human Mammary Neoplasms Neoplasm, Human Mammary Neoplasms, Human Mammary Mammary Neoplasm, Human Mammary Carcinoma, Human Carcinoma, Human Mammary Carcinomas, Human Mammary Human Mammary Carcinomas Mammary Carcinomas, Human Human Mammary Carcinoma Breast Cancer Cancer, Breast Cancer of Breast Mammary Cancer Malignant Neoplasm of Breast Malignant Tumor of Breast Breast Carcinoma Cancer of the Breast Back in the world of mosquitoes, the MeSH term for “malaria” is “malaria,” conveniently, and a search for this term in PubMed actually searches: &quot;malaria&quot;[MeSH Terms] OR malaria[All fields] The following entry terms are indexed to the MeSH term “malaria”: Remittent Fever Fever, Remittent Paludism Plasmodium Infections Infections, Plasmodium Infection, Plasmodium Plasmodium Infection Marsh Fever Fever, Marsh Running a search Once the initial search terms have been identified, a query can be built. Query construction is an iterative process, full of trial and error. Figure 3.9: Boolean operators: AND, OR, NOT Some basic Boolean operators are needed to conduct effective searches: AND, OR, NOT. For instance, consider the search PubMed runs when the terms “malaria OR pregnancy” are entered: (&quot;malaria&quot;[MeSH Terms] OR &quot;malaria&quot;[All Fields]) OR (&quot;pregnancy&quot;[MeSH Terms] OR &quot;pregnancy&quot;[All Fields]) These four terms are combined with OR, meaning we keep results that match any of these terms. At the time of this writing, PubMed returns 922,588 results. Of course, it would make more sense to search for “malaria AND pregnancy,” instead of “malaria OR pregnancy”: (&quot;malaria&quot;[MeSH Terms] OR &quot;malaria&quot;[All Fields]) AND (&quot;pregnancy&quot;[MeSH Terms] OR &quot;pregnancy&quot;[All Fields]) The first two terms and last two terms are combined separately with OR. These combinations are then combined with AND (notice the use of parentheses to segment the operations), dropping the pool of results to 4,203 records. The AND operator will always maintain or decrease the number of results. To limit the results humans, we can add AND &quot;humans&quot;[MeSH Terms] to the end.16 Doing so drops the pool of results to 3,798. (&quot;malaria&quot;[MeSH Terms] OR &quot;malaria&quot;[All Fields]) AND (&quot;pregnancy&quot;[MeSH Terms] OR &quot;pregnancy&quot;[All Fields]) AND &quot;humans&quot;[MeSH Terms] Alternatively, the NOT operator could be used to limit the results to nonhumans, but NOT is not commonly used. Combining the components of the PICO questions and Boolean operators can be very useful. Consider the following research question: Among pregnant women living in malaria-endemic areas, is chemoprevention more effective than a placebo at preventing parasitaemia? Here’s what we could do in plain English: pregnancy OR pregnant women AND malaria endemic AND chemoprevention (to search for specific drugs, string them together with ORs) AND randomized controlled trial AND parasitaemia At this writing, this search returned 513 records in PubMed. After the results have been honed adequately, the same search can be applied in another database, especially if the topic crosses disciplinary boundaries, like economics and health. Research librarians can provide invaluable leads to important databases. Oh hey, here’s one now: Screening results Even the best search queries return some duds, so all search results must be screened. Radeva-Petrova et al. (2014) have used a very thorough approach. Most literature reviews are not as detailed. Often systematic review searches return hundreds or thousands of potential hits, so a study team screens titles and abstracts to exclude obvious outliers. As this process begins, team members commonly screen some of the same records and discuss any differences in their findings to establish reliability (i.e., consistency). Basically, everyone screening records should make the same inclusion/exclusion decision. The Radeva-Petrova et al. (2014) search strategy turned up 179 unique records, and the authors excluded 126 of these records after screening the abstracts. The excluded studies did not meet certain predefined criteria. For instance, the authors only wanted to include studies using RCTs and quasi-experimental designs Fifty-three studies remained for full-text review. Only 17 of these 53 studies still met eligibility criteria after this step.17 Supplemental searches It is customary in a systematic review–and helpful in general reviews—to augment database searches with reference reviews and hand searches to ensure that no key references were missed in the database query. A reference review is nothing more than a scan of an eligible article’s bibliography. In a hand search, eligible articles are plucked from the tables of contents on the website of the journal for each issue published during the search window. If either supplemental method turns up a lot of new results, the systematic review search strategy should be revised to be more comprehensive. Extracting data Depending on the research objectives data can be systematically extracted from a study or set of studies, such as key facts related to study design, methods, and results. If it fulfills the objective, however, an annotated bibliography can be conducted in a much shorter time at a smaller scale. Where a true systematic review is used, a data extraction strategy is also needed. The PICO research question can help guide the identification of the minimum data to extract. Returning to the example of Radeva-Petrova et al. (2014): Among pregnant women living in malaria-endemic areas, is chemoprevention more effective than a placebo at preventing parasitaemia? Some meaningful data items to extract include: Study setting/population Sample size Sample demographics, including parity Study design Intervention details, such as specific medication and dose Primary outcome (e.g., parasitaemia) Effect size Numerous software options exist for storing extracted data, but a simple spreadsheet with rows of studies and columns of study variables is often sufficient. Many teams use this approach for large systematic reviews, and it works well for more modest research and less experienced researchers, too. 3.3 Use a Reference Manager The importance of using a software program for managing references cannot be overstated. The manual collation and assembly of a bibliography is simply a colossal waste of time. Several reference managers are available. Zotero is free and open source. Some other options are not really free. With EndNote, for example, a university may make offer a free download for enrolled students, but the license expires upon graduation or soon becomes obsolete without a paid upgrade. Additionally, in global health, some colleagues may not have access to a program like EndNote, which makes collaboration challenging. For these reasons, Zotero is good choice. Although a tutorial is beyond the scope of this chapter, some features that are common to many reference managers are helpful: Easily imports references from databases like PubMed; moves from the search results to the reference manager instantly Automatically retrieves full-text PDFs Syncs PDFs to tablets and phones Connects to word processing software; inserting references in papers is easy Automatically creates bibliographies based on works cited Instantly reformats in-text citations and references to different styles, such as APA, AMA, or Harvard Shares collections by automatically sync-ing via the Cloud to facilitate collaboration. Easily exports references to other reference managers 3.4 Why Does Any of This Matter? Most of the time, literature searches are not conducted to perform a research-related systematic review. Mostly, literature searches offer the most expedient means of staying current with developments in a specific field and of illuminating gaps in the collective knowledge. Literature searches lead to FINER research questions: questions that are Interesting, and Novel, and Relevant. Literature also provides insight into how researchers of a specific topic or in a particular field conceptualize study designs, plan study measurement, and report results. Searches, quick data evaluations, and rapid integration of study summaries take practice. For researchers early in the process, using effective search strategies can save a great deal of time and energy. Share Feedback This book is a work in progress. You’d be doing me a big favor by taking a moment to tell me what you think about this chapter. References "],
["critical.html", "4 Critical Appraisal 4.1 Be Skeptical of News Reports and Press Releases 4.2 Peer-Reviewed Does Not Mean “Correct” 4.3 How to be a Good Consumer of Research Additional Resources Share Feedback", " 4 Critical Appraisal All health professionals, whether you are a clinician treating patients, a public health official making policy recommendations, or a representative of a donor agency responsible for setting funding priorities, should make decisions based on evidence, not beliefs. The phase “evidence-based” first came into use in the 1990s, nearly two decades after Archie Cochrane highlighted how medical practice lagged behind what science in terms of “what works”. Medical schools soon began teaching their students evidence-based medicine (EBM), defined by Sackett et al. (1996) as: The conscientious, explicit and judicious use of current best evidence in making decisions about the care of the individual patient. It means integrating individual clinical expertise with the best available external clinical evidence from systematic research. Thereafter, the “evidence-based” idea expanded to evidence-based practice more generally, as well as to population-level approaches such as evidence-based public health (Brownson, Fielding, and Maylahn 2009) and evidence-based global health policy (Yamey and Feachem 2011). As in evidence-based medicine, these newer conceptualizations have led to policies and programs that have saved lives and improved health at scale. But before policy makers can make evidence-based decisions, the scientific community must evaluate the strength of the evidence, or conduct a critical appraisal. This topic is the focus of this chapter. 4.1 Be Skeptical of News Reports and Press Releases Much of what we learn about scientific results from the media comes in the form of click bait, such as this article in Discover Magazine titled “Want to avoid malaria? Just wear a chicken.” Not surprisingly, this study did not actually require people to wear chickens? Instead, Jaleta et al. (2016) placed CDC mini-light traps at the foot of a bed in each of 11 homes in an Ethiopian village. They introduced 11 “treatments” over 11 days in a Latin Square experimental design and recorded the number of mosquitoes caught in the traps overnight. One of the treatments indeed used a live chicken, but no human participants were asked to carry it around or let it crawl into bed. Interestingly, a blood meal analysis suggested that Anopheles arabiensis avoids biting chickens, and keeping a live chicken in a nearby cage resulted in a significant reduction in the number of mosquitoes caught in the traps. Odor compounds specific to the air around chickens also achieved this result. So despite the outlandish title, “Just wear a chicken”, the news report got it mostly right. That’s not always the case. If you turn on the news or read a university press release, you’ll often find summaries and claims that go far beyond the conclusions of the original article. Science gives us a small glimpse of the “truth,” but the measured and careful language of scientific articles does not always capture the attention of the public. Another part of the problem is that good science writers are a scarce commodity. One of the best is Ben Goldacre, a British psychiatrist who runs the EMB Data Lab at the Centre for Evidence-Based Medicine at the University of Oxford. He wrote the Guardian’s “Bad Science” column for a decade, and he later published a great book with the same title. His more recent work can be viewed at AllTrials. 4.2 Peer-Reviewed Does Not Mean “Correct” How can published research be flawed if it was peer-reviewed? Peer review is an important component of the scientific process, but it is not a guarantee of “truth” or validation of the results. So what is peer review? Peer review does not mean “correct.” For example, take the Lancet article published by Pronyk et al. (2012) about the impact of the Millennium Villages Project (MVP) on child survival in rural sub-Saharan Africa. The MVP was founded by economist Jeffrey Sachs as a proof of [his] concept(http://amzn.to/2aUF9dJ) that extreme poverty could be solved, and the Millennium Development Goals were met with a big financial push. The basic idea behind a Millennium Village is to intervene across sectors simultaneously—water, sanitation, education, health, etc.—to give communities a chance to escape the poverty trap. Sachs and his colleagues published their first comprehensive report on the MVP in The Lancet—a high-impact, peer-reviewed journal—with the stated aim to “assess progress towards the Millenial Development Goals (MDGs) and child survival over the project’s first 3 years and compare these changes to local trends.” The authors examined child mortality rates across nine Millennium Villages and concluded the following: The average annual rate of reduction of mortality in children younger than 5 years of age was three times faster at Millennium Village sites than in the most recent 10-year national rural trends (7·8% vs 2·6%). When this peer-reviewed article was published online, Bump et al. (2012) challenged the calculation of both of these figures and the resulting interpretation that the MVP had a large impact on child mortality. One of the authors of this criticism, Gabriel Demombynes, explained the errors in this post. First, Pronyk et al. (2012) annualized (i.e., divided) the 21.7% cumulative decline in mortality by 3 for the number of years the intervention was active rather than by 4, the correct time window in this retrospective assessment of mortality. This correction reduced the annual rate of decline among Millennium Villages from a claim of 7.8% to 5.4%. Second, Pronyk et al. (2012) compared their estimated annual decline after 2006 (7.8%) to the results of national surveys in MVP countries from 2001 to 2010. A more appropriate comparison, using all the available post-2006 data, suggests an average annual rate of decline of 6.4% across the 9 MVP countries. Bump et al. (2012) conclude the following: The above observations imply that a key finding of the paper—that child mortality fell at the treatment sites at triple the nationwide rural background rate—is incorrect. Child mortality fell at 5.9% per year at the sites versus 6.4% per year on average across all areas of the countries in question (probably more in rural areas alone) according to the available data that most closely match the project period. This difference is not significant. Pronyk (2012) published a correction that was noted by Retraction Watch. This situation is an example of post-publication peer-review. 4.2.1 GETTING PAST THE GATEKEEPERS When original research upends years of conventional thinking and yields exciting and novel results, researchers (or their institutions) sometimes issue press releases to advertise or create news. Most scientists, however, prefer to reserve judgment or consider the results preliminary pending peer review. They would expect these results to be organized in a manuscript detailing the research design, data, methods, and results, and then submitted to an academic or scholarly journal for publication. Typically, each submission is screened by the journal’s editor, a role often filled by a senior scientist in the journal’s field. If the editor thinks that the research is free of obvious fatal flaws and that the paper will be of interest the journal’s readership, then the editor assigns it to an associate editor with some expertise on the topic to manage the peer-review process. 4.2.2 WHO IS A PEER? To begin the peer-review process, an associate editor finds 3 or more scholars in the research field—peers of the authors—to review the paper and comment on the merits of the study. Some journals allow authors to recommend reviewers who might be a good fit for their research and to request that certain colleagues not be considered. Competition within a field, competition for funding resources, and the race to publish original research are a few of the reasons an author might wish to avoid having their unpublished research under the eyes of certain colleagues. The editorial team does not always respect the authors’ wishes, but finding appropriate reviewers is challenging, and they usually consider these suggestions. A “peer” can be: someone the same level, more junior, or more senior someone who shares the same conceptual framework regarding the topic of study, or someone who takes a different view entirely someone who works on a parallel topic, or someone who is a direct “competitor” someone who is a topic expert, or, when it’s hard to find the right person, someone who does not have much background at all someone who is a technical expert on the study methods, or someone who does not know the first thing about the chosen analytical approach Peer reviews themselves are most often conducted in a blinded fashion, that is, the author and the reviewer are not aware of each other’s identities. However, sometimes reviewers reveal themselves by recommending that the author cite a lot of their work. And sometimes it is easy to determine an authors’ or research group’s identity because their current work builds on previous studies or because various parts of their work have been presented at scientific conferences. The scientific community is a small world, especially in highly specialized fields. 4.2.3 WHAT HAPPENS DURING THE REVIEW PROCESS? Once the manuscript of a research study arrives on a reviewer’s desk, he or she will take a few weeks (or even months!) to recommend that the paper be rejected or accepted with minor or major revisions or no revisions at all. Some reviewers enumerate the perceived flaws in painstaking detail. Others give complicated comments that may be too vague to be helpful to the author or the editor. When the editorial team receives these reviews, they decide how the manuscript will proceed. Most academics are happy to get a “revise and resubmit” letter (also called an “R&amp;R” letter). The editor usually gives an indication that the revised paper will have a good chance of publication when it revised, but there are no guarantees. Even after the original paper is revised it sometimes goes back out for further peer review, or the editor makes the decision to accept the revision without additional input. The editor has a difficult task because reviewers often take different positions regarding a submission. As Smith (2006) suggested, the recommendations of multiple reviews can be in direct opposition: Reviewer A: `I found this paper to be extremely muddled with a large number of deficits’ Reviewer B: `This paper is written in a clear style and would be understood by any reader’. 4.2.4 WHAT DOES NOT HAPPEN? A critical thing to note, however, is that reviewers almost never have access to a researcher’s data or analysis code. They base their decisions on the methods as they are reported, the results as they are presented, and the conclusions reached by the author in the discussion. At this stage, the data sources must be described, but they are not verified. So even if reviewers find possible flaws in the logic of the research, analysis mistakes and fraud go largely unchecked.18 This lack of verification is why it is wrong to conclude that publication in a peer-reviewed journal means validation, or assurance of the correctness of the conclusions drawn. “Published” does not equal “correct.” Journalists can harbor this belief, and defensive authors sometimes promote it when their study’s findings are challenged. Several resources “police” the world of scientific reporting, such as corrigendum and the website Retraction Watch, as well as retractions in the original journal. This lack of verification is why it is wrong for people to conclude that published in a peer-reviewed journal means validation, or assurance of the correctness of the conclusions drawn. “Published” does not equal “correct.” Journalists can harbor this belief, and defensive authors sometimes promote it when their study’s findings are challenged. If it were true that published==correct, then we would have no need for corrigendum and retractions—and the website Retraction Watch would be empty. 4.2.5 PEER REVIEW OF FUNDING PROPOSALS Funding agencies like the National Institutes of Health (NIH) also use a peer-review process to make funding decisions. When a grant application is submitted to an institute at the NIH, a Scientific Review Officer (SRO) checks the proposal for completeness and assigns it to several peer reviewers serving on a Scientific Review Group. The reviewers write up their critiques and assign a score from 1 (exceptional) to 9 (poor). If the preliminary score is too high, the full committee may not discuss the application, which kills the opportunity for funding, at least for that version of the proposal. If an application, i.e., the study proposal, is discussed, the committee assigns a final score (multiplied by 10 for a final rank of 10–90). A summary statement with comments, the overall impact score, and the percentile score is prepared and returned to the applicant. If the proposal exceeds the payline, a percentile score that most institutes set based on the available budget, then the proposal will likely be funded. The overall success rate for research grant applications in 2015 was 18.3%. 4.3 How to be a Good Consumer of Research Here is a nice framework for how to be a good peer reviewer or referee. This information provides a solid basis for learning to be a good consumer of research in general. In this guide, Leek describes a scientific paper as consisting of four parts,19 with some amendments: An introduction that frames the research question A set of methodologies and a description of data A set of results A set of claims Leek offers a helpful recommendation about how to approach a new paper: Your prior belief about [#2-3] above should start with the assumption that the scientists in question are reasonable people who made efforts to be correct, thorough, transparent, and not exaggerate. This book focuses mainly on how to read and write sections 1 and 2, on how to evaluate research results. To develop the ability to critique findings and plan a data analysis confidently, however, further background in analysis will be needed. 4.3.1 INTRODUCTION SECTION A good Introduction explains the aim of the paper and puts the research question in context.20 In public health and medicine, this section is often very short compared to the introductions in other disciplines like economics. Even so, reviewers will often read the Introduction looking for references to key studies that could signal the authors are unaware of developments in the field.21 4.3.2 METHODS SECTION A good Methods section provides enough information to enable the reader to (attempt to) replicate the findings in a new study. Journal space constraints make this challenging, so authors often post supplemental materials online that provide additional details.22 Even with supplemental materials, however, the author is commonly contacted for additional details and materials to actually attempt a replication. The organization of the Methods section varies by discipline, but generally it includes some information about the research design, subjects, materials or measures, data sources and procedures, and analysis strategy. The Equator Network, which awkwardly stands for Enhancing the QUAlity and Transparency Of health Research, is a good resource for understanding modern reporting standards. When preparing a manuscript, most journals expect authors to include all the information outlined in this checklist that is relevant for the research design of the study being reported. Include the completed checklist as an appendix with the article submission to head off reviewers who may complain about missing information that is definitely included. Is the research design well-suited to answer the research question? This book introduces common research designs in global health. There are many different designs that can potentially answer most research questions, but not all designs are created equal. A graphic like Figure 4.1 is commonly used in the EBM literature to convey this point. The meta-analyses and systematic reviews in Chapter 3 are ‘studies of studies,’ and they sit atop the evidence hierarchy. They enjoy this status because they synthesize the best available evidence. No one study is the final word on a research question, so it makes sense that a meta-analysis that pools results and accounts for variable study quality could potentially provide a better answer than any one study alone. Figure 4.1: Levels of evidence However, the Cochrane Handbook for Systematic Reviews (2011) cautions researchers to pay attention to design features (e.g., how participants were selected) rather than labels (e.g., cohort study) because such labels are broad categories. Therefore, this hierarchy is not absolute; these rankings reflect ideals. For example, RCTs can be poorly designed or poorly implemented, and the evidence from such a flawed study is not necessarily better than the evidence from a nonrandomized study just because it carries the label “randomized.” Is there a risk of bias and confounding? Some study designs are better than others (at least in theory) because of their ability to address potential bias when conducted properly. As discussed in Chapter 2, the goal of scientific research is inference, and some error and uncertainty is always inherent. Consumers (and conductors) of research must accept this fact, and they must be willing to assess the extent to which a study’s design and methods might lead away from the “truth.” Error can be either random and systematic. Random error adds noise (i.e., variability) to the data, but it does not affect the average. To revisit our previous example, I might step on a scale and see that I weigh 185.12. I step off and back on, and this time I weigh 185.13.23 This random error results from the limitations of my scale. If I continue taking measurements, this random error will balance out. Random means that the readings are not systematically too high or too low. Systematic error is not random. Systematic error is also known as bias, and it represents a deviation from the “truth.” Let’s imagine that my scale is broken and I do not really weigh 185. I weigh 200. I can worry about the imprecise measurements of 185.12 and 185.13 all day, but I’d be missing the bigger problem that my scale is systematically reading the wrong weight. I can keep taking measurements over and over, but my scale is just wrong. If my goal is 186, I would come to the wrong conclusion that I can stop dieting! Random error can be estimated (as discussed in Chapter 9), but typically, the extent to which bias affects the study results remains unknown. For this reason, this problem is often framed as a “risk of bias.” In a nonrandomized design, the biggest risk of bias comes from potential selection bias (Higgins and Green 2011). Selection bias can take different forms. In the context of intervention research, selection bias represents pretreatment (i.e., baseline) differences between study groups. Webster et al. (2003) conducted a case-control study with a non-randomized ( or observational) study design (discussed in Chapter 13 in Eastern Afghanistan to study the efficacy of bed nets as a tool for preventing malaria. Patients who presented at the study clinic with a fever were tested for malaria. Those who tested positive were classified as “cases,” and the rest were classified as “controls.” The researchers asked cases and controls about their bed-net use, education, income, and several other characteristics. They then compared bed-net users and nonusers on their odds of malaria (i.e., being classified as cases). Webster et al. (2003) wanted to look at potential selection effects with this particular research design, so they also examined patients’ use of chloroquine prior to attending the clinic. If a patient was classified as a control (negative blood film) but tested positive for chloroquine, the patient had received treatment for malaria prior to arriving at the clinic, meaning they really should have been classified as a case. To determine whether this misclassification of cases as controls could introduce selection bias, these researchers investigated chloroquine use in bed-net users and nonusers. They found that the use of chloroquine prior to clinic testing was less common among patients who reported using bed nets than among nonusers. If chloroquine use was less common among bed-net users, the estimated effect of bed nets would have been underestimated. Consider the following example. Panels A and B show cases (those who tested positive for malaria) and controls by their reported bed-net usage. In Panel A, 4 patients were misclassified as controls, meaning that they tested negative for malaria but only because they treated themselves with chloroquine prior to the test. The panel also shows that chloroquine use is less common among net users. Still in Panel A, the odds of malaria (cases) among bed-net users is 12/18, and the odds of malaria among nonusers is 12/6. This is an odds ratio of 0.34, suggesting that bed nets protect against malaria (cf. a value of 1 would indicate no effect). However, Panel B shows that this effect might be an underestimate. If the misclassified control patients are moved to the case group where they belong, the odds change. Now the odds of malaria among bed-net users is 13/17 and the odds among nonusers is 15/3. This is an odds ratio of 0.15, which suggests an even greater protective effect of bed nets. In Panel A, the effect was biased toward the null, meaning that the effect looked smaller than it probably is. This bias results in confounding, and chloroquine use is a confounding variable. Confounding variables are correlated with both the “treatment” (i.e., bed-net use) and the outcome (i.e., malaria). Exploring selection bias An experimental design typically overcomes the risk of bias and confounding through random assignment. If the sample size is large enough, potential confounding variables like chloroquine use from the example above should be equally likely for all groups. The key word here is “typically.” Many aspects of an experimental design can result in a risk of bias. For this reason, every Cochrane systematic review assesses several types of known risks of bias in RCTs (Higgins and Green 2011): Selection bias Performance bias Detection bias Attrition bias Reporting bias These sources of bias are further discussed in Chapter 11. The takeaway at this point should be that every study has a potential for bias and, research consumers should assess the risks of bias that might challenge the validity of the reported results. This type of validity—asking, “are the study results ‘correct’?”—is typically referred to as internal validity (Higgins and Green 2011). (External validity is covered near the end of this chapter.) Who (or what) was the subject of study and how were these subjects recruited and/or selected? Typically, a subsection of the Methods section describes participant recruitment and selection. What made someone eligible or ineligible to participate? Who was excluded, intentionally or not? Exclusion and inclusion criteria define the population of interest and inform the study’s generalizability. Furthermore, how were participants invited or selected? Was this process random, or did the researchers invite participants base on availability? The methods of sampling and selection have implications for the inferences that are possible regarding the population. This concept is covered in greater detail in Chapter 9. What materials and/or measures were used in the course of the study? Almost every study uses some type of materials or measures. Diagnostic studies, for instance, evaluate a diagnostic test or a piece of hardware that analyzes the test samples. Environmental studies often use sophisticated instruments to take atmospheric measurements. Studies like these provide specific details in the Methods section about the materials and equipment used. Study variables also need to be precisely defined. For instance, hyperparasitemia describes a condition of many malaria parasites in the blood. But what constitutes “many”? The World Health Organization (WHO) defines it as “a parasite density &gt; 4% (~200,000/µL)” (WHO 2015b). Does the study use this definition or another one? In studies measuring social or psychological constructs such as anxiety, a definition of the concept of “anxiety” should be provided. Is an anxiety disorder diagnosed by a psychiatrist? If so, what is the basis for this diagnosis? Or is anxiety inferred from a participant’s self-reported symptoms on a checklist or screening instrument? If so, what are the questions and how is the instrument scored? Such measurement issues are addressed in Chapter 8. How was the study conducted and how were the data collected? The data collection part of a Methods section should describe what happened after participants were recruited and enrolled. What happened first, second, third? If the study is observational, the procedures might be limited to data collection. Who collected the data, and how were they trained? Where were the data collected? For intervention studies, the data collection procedures describe how participants were randomized to study arms and what happened (or did not happen) in each arm. Were the participants, data collectors, and/or patients blind to the treatment assignment? How was the data analyzed? If the study uses a hypothesis-testing framework (and not all do), then details about the study hypotheses are located in the Introduction or Methods section, depending on the journal. The Methods section should also detail how the analysis will be carried out. For example, in an intervention study, how was the effect size estimated? Did the study use ordinary least squares regression or logistic regression? The list goes on and on. When preparing a manuscript, variables should be defined and analyses specified in the Methods section . In the Results section, data and findings are reported without a restatement of the methods or analysis approaches. Was the study pre-registered and approved by an ethics board? The US Federal Policy for the Protection of Human Subjects (i.e., the “Common Rule”) defines research as “a systematic investigation, including research development, testing and evaluation, designed to develop or contribute to generalizable knowledge…” If the research involves human subjects, it must be reviewed and approved by an institutional review board (IRB) before any subjects can be enrolled. Most studies fall under IRB oversight, but some, such as retrospective studies or quality control interventions, may qualify as exempt. Increasingly, researchers are taking the additional step of registering their study protocol prior to the study launch in a study clearinghouse like https://clinicaltrials.gov/. This registration is a requirement for drug investigations regulated by the FDA, and it is expected by many journals.24 Preregistration does not ensure trustworthy results, but the practice fosters a welcome increase in research transparency. If the analysis described in an article deviates from the planned analysis, the authors are expected to provide a compelling justification. Studies often measure a number of outcomes, sometimes in a number of different ways, and it can be tempting to consider only certain results that are most applicable to the topic under investigation. . Sometimes researchers deviate from the pre-registered protocol and present different results when the pre-registered plan does not work out. This is called outcome switching, and some medical journals do not seem to care, but the COMPare Trials Project thinks they should consider this diversion from the norms of scientific research reporting methods more seriously. Figure 4.2: Is outcome switching a problem in medical trials?; Source: http://compare-trials.org/. 4.3.3 RESULTS SECTION Can each finding be linked to data and procedures presented in the Methods Every finding in the Results section should be linked to a methodology and source of data documented in the Methods section. Articles in medical journals are some of the shortest, so supplemental materials posted online may be needed to obtain a clearer sense of what the authors did and found. Is the analysis correct? Without access to the data and any analysis code, which is still the norm for most publications, results cannot be independently verified. Even with such access, some analyses are so complex that only people with extensive training feel qualified to question the accuracy of the results. 4.3.4 DISCUSSION SECTION Is each claim linked to a finding presented in the Results? Each claim should be supported by results that are reported in the paper. If there is no link between a claim in the Discussion section and a finding in the Results section, the author may be “going beyond the data.” For example, if results on the efficacy of a new treatment for malaria but do include any data on cost, then it would be inappropriate to claim that the treatment is cost-effective. Although it is legitimate to speculate a bit in the Discussion section based on documented findings, authors should be careful to label all speculation as such—and these hypothetical forays should never be included the article’s Abstract. Is each claim justified? Once a link is established between a claim and a set of results, the claim must be validated as representing a correct interpretation of these findings. In addition, the conclusions should be carefully assessed to make sure that the authors did not “go beyond the data” by making conclusions that are not supported by the analysis. For instance, if only weak or mixed evidence that a new program works has been provided, are the authors recommending a massive scale-up of the program? Or do the authors claim that a program is cost-effective without presenting data on actual costs? Are the claims generalizable? Most [studies] are highly localized and paticularistic…Yet readers of [your study’s] results are rarely concerned with what happened in that particular, past, local study. Rather, they usually aim to learn either about theoretical constructs of interest or about a larger policy. That’s Shadish et al. (2003) writing about the importance of generalizability of research findings and claims. When a study is so highly localized that the results are unlikely to generalize to new people and places, we’d say that the study has low external validity. One approach to promoting generalizability is to use formal probability sampling, which is discussed in Chapter 9. Randomly sampling participants from the population of interest is one way to increase the external validity of a study. For example, Wanzira et al. (2016) analyzed data from the 2014 Uganda Malaria Indicator Survey, a large national survey, and found that women who knew that sulfadoxine/pyrimethamine is a medication used to prevent malaria during pregnancy had greater odds of taking at least two doses than women who did not have this knowledge. Because the UMIS is nationally representative, the results could apply to Ugandan women who did not participate in the study. Would the results be generalizable to women in Tanzania? An argument could be made that they would. Would the results be generalizable to women in France? No, probably not; among other things, malaria is not an issue there. Are the claims put in context? A good Discussion section puts the study findings in context by suggesting how the study adds to the existing literature. Do the results replicate or support other work? Or do the findings run contrary to other published studies? What are the limitations? No study is perfect, and almost all studies include a paragraph or two outlining the shortcomings recognized by the authors. Indeed, many journals require it. Such limitations span all aspects of the study design and methods, from sample size to generalizability of results, to data validity and approaches to statistical analysis. In addition to knowing how the results fit into the bigger research landscape, communicating shortcomings can provide a valuable resource for future researchers in terms of caveats and research directions. Additional Resources Critical appraisal worksheets from the Centre for Evidence-Based Medicine BMJ Series on “How to Read a Paper” Critical appraisal resources from Duke Medicine Share Feedback This book is a work in progress. You’d be doing me a big favor by taking a moment to tell me what you think about this chapter. References "],
["causeeffect.html", "5 Cause and Effect 5.1 Fundamental Challenge of Causal Inference 5.2 Threats to Internal Validity 5.3 Research Designs to Estimate Causal Impact Share Feedback", " 5 Cause and Effect Do bed nets prevent malaria? Do vouchers increase access to treatment? Do cash transfers improve mental health? What these research questions share is a focus on causal impact—a difference in outcomes that can be attributed to a treatment, intervention, policy, or exposure. In many ways, as an applied field global health is the study of causal impact. 5.1 Fundamental Challenge of Causal Inference PDF slide deck In an ideal research world, the question, “Do bed nets prevent malaria?” could be answered by cloning a study participant—let’s call her Lucy—and simultaneously giving Lucy a bed net while withholding a net from Lucy’s clone. In this way, we could determine what really happens in the absence of the intervention because the only difference between Lucy and her clone would be that one of them received the intervention.25 Of course, we cannot clone study participants, and we cannot simultaneously both provide and not provide a bed net to Lucy. We only get to observe what happens to Lucy, not her clone who did not receive the intervention. Therefore we ask hypothetically what would have happened if Lucy had not been given a bed net. This hypothetical situation—what would have happened in the absence of the intervention—is referred to as the counterfactual (or “potential outcome” in the language of the Neyman–Rubin causal model). Not being able to observe the counterfactual directly is known as the “fundamental challenge of causal inference.” This is the primary reason that research design requires thought and effort. Much of what follows in this book deals with strategies to deal with causal inference in the absence of a true counterfactual. 5.1.1 UNDERSTANDING CAUSAL RELATIONSHIPS Humans have a pretty decent understanding of cause and effect: hand touch fire, fire hot, fire burn hand. Philosophers, on the other (unburnt) hand, have spent centuries explaining that causality is actually much more complicated than it seems on the surface. As a result, causal inference is a vibrant field of study today, and researchers continue to develop new techniques for drawing causal inferences from experimental and non-experimental data. Causes In his book Causal Inference in Statistics, computer scientist Judea Pearl (2016) provides a simple definition of causes: “A variable X is a cause of a variable Y if Y in any way relies on X for its value.” The phrase “in any way” is a reminder that most of the causal relationships we investigate in global health are not deterministic and effects can have more than one cause. For example, an experimental treatment is given to 100 people suffering from a disease, and only 60 get better. If the causal relationship between the drug and disease state were deterministic, all 100 patients would have recovered. This is not what happened, however. The causal relationship only increased the probability that the effect would occur. Effects Causal impact is the difference in counterfactual outcomes (i.e., potential outcomes) caused by some exposure, program, intervention, or policy. Although this sounds simple, it leads back to a fundamental problem: only one counterfactual outcome can be observed for an individual; we cannot observe someone in two states simultaneously (i.e., the treatment and the control). Therefore, it is not possible to observe an effect of the program on an individual. Instead, groups of individuals are observed both under the intervention and without the intervention. Thus, we an infer the counterfactual by comparing some people who get some treatment to other people who do not.26 Most often, we compare two different sets of people in a treatment (or intervention) group, but the logic also extends to two or more groups (i.e., study arms) or to a single group of people observed at different time points. “But I thought you said an individual like me can’t exist in two states at once!” That’s correct. Although an individual cannot be observed under two conditions at the same time, an individual can be observed at two different times, for example, before the individual receives a treatment and after the individual receives a treatment. This is a “pre–post” or “before/after” comparison. The most common estimate of causal impact is the average treatment effect (ATE). Although we cannot observe an effect of X on Y for any specific individual (who can only exist in one state at a time), we can determine whether X causes Y on average among individuals in a study cohort. This is possible because the average difference in potential outcomes (which we cannot observe) is equal to the difference of averages. The following graphic might help.27 Figure 5.1: Average causal effects can be estimated even though individual effects cannot be observed. Panel A shows hypothetical results when all subjects are assigned to treatment Y(1) or control Y(0). There are two data points for each person corresponding to their hypothetical potential outcomes. In reality, however, we can only observe one point per person because it is not possible to be in two states at once. Both sets of points have an average value, depicted by the dashed lines in Panel A and Panel C. In the middle Panel B, the differences between each pair of outcomes in A are plotted as individual effect sizes (e.g., 1.0-(-1.5) = 2.5 for person 6). The dashed green line represents the average causal effect. Importantly, this average of individual differences in B is equal to the difference in averages in Panel C. The point is that it is not possible to estimate the effect on individuals since we cannot measure both potential outcomes for any one person at the same time. However, the average causal effect can be estimated by comparing the average value of a group of people who receive the intervention to the average value of a group of people who do not. Causal relationships Although it is clearly possible to estimate the average difference between two groups, can this difference be interpreted as an estimate of the causal impact of X on Y? In other words, are X and Y causally related? Shadish et al. (2003) point to useful three characteristics of causal relationships: The cause is related to (i.e., associated with) the effect. The cause comes before the effect. There are no plausible alternative explanations for the effect aside from the cause. Condition #1 is easy to establish. Is X correlated with Y? In fact it’s so easy to establish that someone came up with the maxim, “correlation does not prove causation,” to remind us that the burden of proof is greater than the output of correlate x y or cor(x, y), or whatever command a statistical software package runs. But it is a start. Condition #2 is a bit harder to demonstrate conclusively because X and Y might be correlated, but the causal relationship may run in the opposite direction—maybe Y causes X. Correlations do not conclusively indicate which comes first, X or Y. Consider malaria and poverty as an example. Jeffrey Sachs and Pia Malaney (2002) published a paper in Nature in which they wrote: As a general rule of thumb, where malaria prospers most, human societies have prospered least…This correlation can, of course, be explained in several possible ways. Poverty may promote malaria transmission; malaria may cause poverty by impeding economic growth; or causality may run in both directions. Condition #3 is the trickiest of all: ruling out plausible alternative explanations. As Sachs and Malaney note, the literature on poverty and malaria has not found a way to do so conclusively. They write that it is “possible that the correlation [between malaria and poverty] is at least partly spurious, with the tropical climate causing poverty for reasons unrelated to malaria.” The authors are proposing that climate is a potential cause of both poverty and malaria. If true, that would make climate a confounding (or lurking) variable that accounts for the observed relationship between poverty and malaria. 5.2 Threats to Internal Validity The possibility of plausible alternative explanations keeps researchers up at night, particularly non-experimentalists. Threats to internal validity, i.e., mistakes in causal inferences, can lay waste to months or even years of research. The “randomistas”, (i.e., researchers whose designs count heavily on random group assignment and the inclusion of control groups), rest easy with the knowledge that random assignment generally makes plausible alternative explanations implausible.28 Internal validity is Campbell’s (1957) notion about whether an observed association between X and Y represents a causal relationship. If X comes before Y, and if there are no other plausible explanations for the covariation between X and Y, then the causal inference about X and Y is valid. Threats to causal inference are threats to internal validity. Shadish, Cook, and Campbell (2003) outlined nine primary reasons why it might not be valid to assume that a relationship between X and Y is causal. Table 5.1: Threats to internal validity. Source: Shadish et al. (2002), http://amzn.to/2cBaAM1. Threats Definitions Ambiguous temporal precedence Lack of clarity about which variable occurred first may yield confusion about which variable is the cause and which is the effect. Selection Systematic differences over conditions in respondent characteristics that could also cause the observed effect. History Events occurring concurrently with treatment could cause the observed effect. Maturation Naturally occurring changes over time could be confused with a treatment effect Regression When units are selected for their extreme scores, they will often have less extreme scores on other variables, an occurrence that can be confused with a treatment effect. Attrition Loss of respondents to treatment or to measurement can produce artifactual effects if that loss is systematically correlated with conditions. Testing Exposure to a test can affect scores on subsequent exposures to that test, an occurrence that can be confused with a treatment effect. Instrumentation The nature of a measure may change over time or conditions in a way that could be confused with a treatment effect. Additive and interactive effects The impact of a threat can be added to that of another threat or may depend on the level of another threat. 5.2.1 AMBIGUOUS TEMPORAL PRECEDENCE Correlational studies can establish that X and Y are related, but often it is not clear that X occurred before Y. Uncertainty about the way a causal effect might flow is referred to as ambiguous temporal precedence—or simply “the chicken and egg” problem. Sometimes, the direction is clear because it is not possible for Y to cause X. For instance, hot weather (X) might drive ice cream sales (Y), but ice cream sales (Y) cannot cause the temperature to rise (X). Most relationships of concern in global health are not so clear, however. Take bed-net use and education as an example. Does bed-net use prevent malaria and allow for greater educational attainment? Or does greater education lead to a better understanding and appreciation of the importance of preventive behaviors like bed-net use?29 5.2.2 SELECTION The fundamental challenge of causal inference is that the counterfactual cannot be observed directly. In health research, we often compare a group of people who were exposed to the potential cause to a group of people who were not exposed. No matter the effort to make sure that these two groups of people are equivalent before the treatment occurs, there may be observable and unobservable ways in which these groups differ. These differences represent selection bias, which is a threat to internal validity. For instance, Bradley et al. (1986) compared parasite and spleen rates among bed-net users and nonusers in The Gambia and concluded that bed nets had a “strong protective effect” against malaria. However, the authors also observed that bed net use and malaria prevalence were also associated with ethnic group and place of residence. Thus, ethnic group and place of residence are confounding variables, that is, plausible alternative explanations for the relationship between bed net use and malaria. Identifying selection bias and trying to account for it in the analysis can be frustrating because not all biases are visible. The same applies to selection threats. Although some may be discernible, many confounding variables often go unnoticed. The only way to be certain that such threats have been minimized is to randomly assign people to conditions (i.e., study arms). 5.2.3 HISTORY History threats to validity begin where selection threats end. Whereas selection threats are reasons that the groups might differ before the treatment occurs, history threats occur between the start of the treatment and the posttest observation. Before-and-after studies (i.e., pre–post studies) are particularly susceptible to history threats. In these designs, researchers assess the same group of people before and after an intervention without a separate control or comparison group. The assumed counterfactual for what would have happened in the absence of the intervention is simply the pre-intervention observation of the group. Okabayashi et al. (2006) provide a good example. In this study, the researchers conducted a baseline survey and then began a school-based malaria control program. Nine months later, they conducted a postprogram survey with the same principals, teachers, and students. On the basis of the before-and-after differences they observed, they concluded that the educational program had a positive impact on preventive behaviors. For example, student-reported use of bed nets (“always”) increased from 81.8% before the program to 86.5% after the program. It is possible that the program changed behavior, but without evidence to the contrary, it is also possible that something else was responsible for the change. Maybe another program was active at the same time. Maybe there was a marketing campaign for a new type of bed net just entering the market. Maybe the posttest occurred during the rainy season when people know the risk of malaria is greater. The examples of possible history threats illustrate how causal inference can invalidate the impact of a study that includes behavior change as an outcome. 5.2.4 MATURATION Single-group designs like Okabayashi et al. (2006) are also subject to maturation threats. The basic issue is that people, things, and places change over time, even in the absence of any treatment. For example, all children grow and change over the course of a school year. Therefore, comparing children at the end of the year to their younger selves a year earlier and making a causal inference about some program or intervention is problematic because kids gain new cognitive skills as they age. Changes observed can be due to a specific program or intervention, or they may simply be related to the passage of time. Without a comparison group (i.e., control group) of similar-aged children, it can be hard to determine the difference. 5.2.5 REGRESSION ARTIFACTS Certain study designs are susceptible to regression artifacts. Sometimes, people are selected for a study because they have very high or very low scores on some outcome. Often, these scores are less extreme at retest, independent of any intervention. This statistical phenomenon is called regression to the mean, and it occurs because of measurement error and imperfect correlation. 5.2.6 ATTRITION Attrition occurs when study participants are lost to the cohort, for example, when they do not participate in outcome assessments. Attrition that is uneven between study groups is described as systematic attrition. Whereas selection bias makes groups unequal at the beginning of a study, attrition bias makes groups unequal at the end of the study for reasons unrelated to the treatment under investigation. For example, researchers recruit depressed patients to take part in an RCT of a novel psychotherapy that is delivered over the course of 10 weekly sessions. If the most depressed patients in the treatment group drop out because the schedule is too demanding, then the analysis would compare the control group (with the most depressed patients still enrolled) to a treatment group that is missing the most depressed patients.30 The data would show that the treatment group got better on average, but part or all of the observed treatment effect would be due to attrition of the most depressed patients from the treatment group, not due to the treatment. 5.2.7 TESTING Repeated administrations of the same test can influence test scores, independent of the program that the test is designed to evaluate. For instance, practice can lead to better performance on cognitive assessments, and this improved performance can be mistaken as a treatment effect if there is not a comparison group. Testing threats decrease as the interval between administrations increases. 5.2.8 INSTRUMENTATION Testing threats describe changes in how participants perform on tests over time due to repeated test administrations. When the tests themselves change over time, an instrumentation threat occurs. For example, if a study uses different microscopes or changes measurement techniques for the posttest assessment, differences in blood smear results could be incorrectly attributed to an intervention. 5.2.9 ADDITIVE AND INTERACTIVE EFFECTS Unfortunately, a study can be subject to more than one of these threats to internal validity. Interestingly, threats can work in opposite directions, or they can interact to make matters worse. For example, if Okabayashi et al. (2006) had decided to compare students who went through the depression program to students from another part of the country who did not go through the program, their study might have been subject to both selection and history threats. The two groups of students might have been different to begin with (selection), and they might have had different experiences over the study period unrelated to their treatment or nontreatment status (history). 5.3 Research Designs to Estimate Causal Impact Figure 5.2: Research design choose your own adventure. PDF download, https://drive.google.com/open?id=0Bxn_jkXZ1lxuWkhFcTUzdWVkZ0E 5.3.1 EXPERIMENTAL DESIGNS If given the choice, many (if not most) researchers would choose an experimental design to estimate causal impact. Experiments are subject to bias when things do not go as planned (e.g., systematic attrition), but a good experiment is subject to fewer threats to internal validity for two main reasons: The cause always comes before the effect in an experiment (and quasi-experiment) because the treatment is “manipulated”; some people get the treatment but others do not. After the treatment is administered to some people, outcomes are observed. Cause precedes effect. Random assignment makes plausible alternative explanations implausible. The importance of this design element cannot be overstated. Whereas other designs require stronger assumptions about selection threats, experiments dismiss them by distributing observable and unobservable differences approximately equally across study arms. Figure 5.3: Basic experimental design Example An important global health policy question that has been studied using experimental and quasi-experimental methods is the impact of user fees on the adoption of health goods, such as bed nets. Advocates of fees argue that free distribution is not sustainable and leads to waste when people who do not need or want the goods are recipients. Also, there is an argument that people only value what they pay for, so removing fees makes people less likely to use goods like bed nets. Conversely, the provision of some health goods, in the language of economics, creates “positive externalities” and should therefore be financed with public dollars. In other words, some interventions have spillover effects whereby people who are not treated still experience some indirect effect. A good example of a spillover effect is vaccines and the resulting herd immunity. Hawley et al. (2003) showed a similar protective effect of ITN use on child mortality and other malaria-related outcomes among households without ITNs located within 300 meters of households with ITNs. Evidence that ITNs have direct (Phillips-Howard et al. 2003) and indirect benefits has been established. The research problem is how to increase geographic coverage and the use of the nets. Is free distribution the best strategy, or should users have to spend something to get a bed net that might retail for a price that is out of reach for many poor households? In other words, should ITNs be free or subsidized? Cohen and Dupas (2010) used an experimental design to study this question in Kenya, where malaria is the leading cause31 of morbidity and mortality. They randomly assigned 20 prenatal clinics in an endemic region to 1 of 5 groups: a control group that did not distribute ITNs, a free distribution group, a group that charged 10 Ksh per ITN (i.e., a 97.5% subsidy), a group that charged 20 Ksh (i.e., a 95% subsidy), and a group that charged 40 Ksh or approximately $0.60 USD (i.e., a 90% subsidy). When units like clinics, schools, and villages are randomized, the design is a cluster-randomized trial, or CRT. The authors followed a subset of pregnant women over time and found that those who paid a subsidized price were no more likely to use the bed nets than women who received one for free. They also found that the increase in price from $0 to $0.60 USD reduced demand for ITNs by 60%. These findings indicate that the cost-sharing model of having women pay something for ITNs reduces coverage. The women who forgo a net purchase are at higher risk for malaria because of the direct prevention effects of ITNs, but the research of Hawley et al. shows that the community also suffers because ITNs have spillover effects. Cohen and Dupas conclude that free distribution would ultimately save the lives of more children. 5.3.2 QUASI-EXPERIMENTAL DESIGNS Although experiments aim to mimick the counterfactual, it is not always logistically possible, politically feasible, or ethically justified to run an RCT. Most often, researchers must infer causal inference from non-experimental data. This procedure is shaped in part by disciplinary traditions. For instance, psychologists trained in the tradition of Campbell tend to focus on design choices made before a study is launched to improve causal inference by ruling out alternative explanations (Shadish, Cook, and Campbell 2003). This primacy of control by design aims to prevent confounding or at least investigates the plausibility of alternative explanations by adding design elements like more pretest observations and comparison groups. Economists have a similar preference for strong designs, but their approach to causal inference tends to focus more on the analysis after data collection. Whereas psychologists might ask about threats to internal validity, economists are more likely to ask, “What’s the identification strategy?” Econometricians Angrist and Krueger (1999) defined identification strategies as “the combination of a clearly labeled source of identifying variation in a causal variable and the use of a particular econometric technique to exploit this information.” For example, economists often analze the returns on the costs of education. The most common identification strategy to estimate the impact of schooling (the proposed causal variable) uses regression to control for potential confounds. In addition to regression, the econometrics tool kit for nonexperimental data also includes instrumental variables, regression discontinuity, and differences-in-differences (Angrist and Pischke 2015). Interrupted time series may also be included. Psychologists (and others) label these quasi-experimental designs because they involve a manipulable cause that occurs before an effect is measured but lacks random assignment. Figure 5.4: Common quasi-experimental designs Example Agha et al. (2007) used a quasi-experimental design to estimate the impact of a social marketing intervention on ownership and use of ITNs in rural Zambia. Nets that commonly sold for USD $27 were subsidized and sold for $2.50 at public health clinics. Neighborhood health committees were established, and 600 volunteer “promoters” were trained to teach residents about malaria and to encourage them to purchase the nets. To estimate the impact of the intervention, the authors analyzed data from postintervention surveys in three intervention and two comparison districts. This study design was quasi-experimental because the districts were not randomized to the intervention or control arms. Figure 5.5: Source: Agha et al. (2007), http://bit.ly/1MkO5a0 Agha and colleagues reported that ITN ownership and use was higher in intervention districts according to the postintervention data, but they were careful to avoid going ‘beyond the data’ to claim evidence of a causal relationship. Several design limitations should be explored here, which will become more apparent as we explore more facets of study design. Briefly, the authors did not randomize districts to study arms and no baseline (i.e., pretreatment) data were collected. Experimental studies benefit from, but do not require, baseline (or preintervention) data because randomization usually ensures that the treatment and comparison groups are similar at the start, that is, if enough units are randomized. But a nonrandomized study like this one leaves itself open to criticism because it lacks baseline data to show that the intervention and comparison districts were similar before the intervention was introduced. The results suggest that the districts were different after the intervention period, but these differences may or may not have been caused by the intervention itself. Given the limitations, how should we view the results? If this was one of the first studies on the topic, we would consider it a starting point that would encourage more rigorous investigations. As part of a larger body of evidence, however, it would probably be passed over in systematic reviews and meta-analyses (i.e., studies of studies) because of the limitations of the design with regard to causal inference. 5.3.3 OBSERVATIONAL DESIGNS Epidemiologists are typically associated with observational designs such as cross-sectional surveys, case-control studies, and cohort studies, though many epidemiologists design and implement RCTs.32 Figure 5.6: Common observational designs Observational studies can yield important insights about cause and effect, but they have limitations. Foremost among them is “correlation does not equal causation.” For example, there is a nearly perfect correlation between the per capita consumption of cheese and the number of people who have died by becoming tangled in their bedsheets! All studies have limitations and tradeoffs. Designing a good study is a process of weighing scientific objectives with logistical constraints, ethical considerations, time, money, and a host of other factors. Some common observational designs are described below. Cross-Sectional Descriptive research The goal of descriptive research is to characterize the population. Often, this means estimating the prevalence of a phenomenon or disease: 20% are illiterate; 36% have an unmet need for contraception; 9% are HIV positive. Description can also be qualitative in nature (e.g., thick description). Nearly every study has a descriptive element, and many studies are primarily descriptive. The Demographic and Health Surveys, more commonly referred to as DHS surveys, are a good example. Every student of global health should explore what the DHS program has to offer. The program is funded by the U.S. Agency for International Development (USAID), and registered users can request access to data from more than 300 surveys conducted in more than 90 countries. A DHS survey is also an example of a cross-sectional study. These are typically one-off surveys, but they can include other forms of data collection. The survey provides a snapshot. The overall goal of the survey is often description, but it might also include correlation. Cross-sectional studies are differentiated from panel or longitudinal studies by their participants; the latter include the same research participants (sample) over time in multiple studies, whereas cross-sectional studies include a sample only once. The DHS program conducts a new survey in a country every 5 years or so, and they always recruit a new sample of participants (i.e., successive independent samples). Thus, the DHS surveys are cross-sectional rather than panel or longitudinal in design. DHS surveys provide a good example of demographic research. Demographers contribute to and use data sources like DHS surveys and national population and housing censuses to understand more about population size, structure, and change (e.g., birth, death, migration, marriage, employment, education). Many countries strive to conduct a census, an enumeration of all citizens, every 10 years. The United Nations Statistics Division and the United Nations Population Fund (UNFPA) provide technical support to countries preparing for, conducting, and analyzing a national population and housing census. These two organizations, in partnership with the United Nations Children’s Fund (UNICEF), maintain CensusInfo, a database of global census data. The 2014 Kenya DHS Key Indicators Report describes the prevalence of ITN use.33 This is a typical DHS cross tabulation (or crosstab) of the results. In this example, the percentage of children under the age of 5 years old that slept under an insecticide-treated bed net the previous night in Kenya was 54.1%. This descriptive data is further disaggregated by residence and wealth quintiles, which is typical for DHS tables.34 Figure 5.7: Source: Kenya 2014 DHS Key Indicators Report, http://bit.ly/1g4NYS5 The data summarized in this table describe the problem of bed-net use. Descriptive questions are well suited for needs assessments. Before designing a program or policy to increase bed net usage, for instance, the need must be clearly understood. In Kenya, almost half of children under 5 years of age are not sleeping under insecticide-treated nets, according to the DHS. This exposure is a particular concern for children living in areas of high risk for malaria. Correlational research This descriptive information sheds light on program and policy priorities, but it goes beyond describing the problem in an effort to alleviate it. Descriptive insights often serve as the basis of subsequent attempts to predict or explain the behavior or phenomenon. For instance, Noor et al. (2006) asked a correlational research question about the factors associated with bed-net use among children under the age of 5: Are wealth, mother’s education, and physical access to markets associated with the use of nets purchased from the retail sector among rural children under five years of age in four districts in Kenya? Correlational research asks questions about the relationship (i.e., the association) between two or more variables. In this case, these variables are ITN use and a variety of potentially influential factors, such as household wealth and a mother’s education level. Noor and colleagues reported that only 15% of children in the rural study sample slept under a net the previous night, a much lower percentage than the national prevalence reported by DHS surveys. As shown in the table below, several factors were associated with higher odds of bed-net use, including greater household wealth, living closer to a market center, not having older children present in the household, having a mother who is married and not pregnant, being younger than 1 year old, and having an immunization card. Figure 5.8: Source: Noor et al. (2006), http://bit.ly/1HoltVo Cohort A cohort is a group of people recruited because they share something in common. In a prospective cohort study, participants without the outcome of interest are recruited and followed for a certain period to observe their respective outcomes. For instance, Lindblade et al. (2015) conducted a prospective cohort study in Malawi to test the efficacy of ITNs in an area of moderate resistance to pyrethroids, a common class of insecticide. They followed a cohort of 1,199 healthy children (i.e., no malaria) aged 6–59 months for 1 year and found that the incidence of malaria infection over this time was 30% lower among ITN users than nonusers. The exposure in this study was bed-net use the night before the study team visited the household and was measured in three possible responses: ITN user, untreated net user, or no net. This is a bit confusing because the use of a bed net is protective. Often, the exposures being studied is harmful, like smoking or lead in the drinking water. The research question is the same in both cases, however; it seeks to determine whether the outcome differs by exposure status. Although this study design is promising, it has limitations. One important limitation is that the children were not randomized to ITN access. The children who used the ITNs may have been somehow different from the children who did not use the ITNs. This possibility represents a potential selection bias, a threat to internal validity, which is discussed further in a later chapter. The basic challenge for causal inference is that the design does not rule out the possibility that something other than ITN use accounted for the reductions in malaria infections. A retrospective cohort study is similar except that, by the time the researcher is involved, the cohort has already been recruited and the data have already been collected. Researchers often use medical records to explore the characteristics of groups of people who share many aspects (e.g., a canger diagnosis) but differ in terms of some exposure (e.g., whether or not they smoke). These data from these groups are then compared to see who developed the outcome. For instance, Fullman et al. (2013) used birth history information contained in DHS and MIS micro-data (i.e., survey data about individuals and households, not just the summary reports) to construct retrospective cohorts of children aged 1 to 59 months. They were interested in estimating the effect of ITNs and indoor residential spraying (the exposures) on child mortality (the outcome) within 59 months of birth, so they used survey data to determine whether and when households had been “exposed” to these prevention tools. Notably, everything about this study was retrospective; the researchers did not collect their own data or follow any participants over time. In the end, they found that bed nets and spraying reduced malaria-related morbidity but not child mortality. Case-control SSometimes it is not possible to recruit a group of healthy people and wait to see who gets sick, as in a prospective cohort study. Imagine having to wait a decade or more to see who develops a rare disease like glioma. The study of a rare disease that takes time to emerge would be very expensive and would need to involve thousands of people . A case-control study might be a better fit. In this design, researchers identify people with the disease (cases) and without the disease (controls) and ask them about past exposures. Obala et al. (2015) conducted this study in Kenya with 442 children hospitalized with malaria and healthy matched controls without evidence of malaria. They sought to determine why the malaria burden was high despite a high level of ITN coverage. The research team visited the homes of all case and control participants and collected data about ITN coverage and recent use, along with measuring the parasite burden of family members, mapping nearby potential vector breeding sites, and assessing neighborhood ITN coverage. Obala and colleagues found that ITN coverage was not correlated with hospitalizations but that consistent ITN use decreased the odds of hospitalizations by more than 70%. As with prospective cohort designs, selection bias is a risk. In this case, the matching process was less than perfect. The matching was performed on the basis of age, gender, and village. However, the cases and controls may have differed in other ways that were not considered, which may have undermined the results. A case-control study looks like a retrospective cohort, but flipped: In a retrospective cohort you look to see if people with different exposures have different outcomes. In a case-control study you look to see if people with different outcomes had different exposures. Another difference is that, in a case-control study, the researcher recruits participants in the present day and asks them about historical events. In a retrospective cohort study, all data collection has already taken place. Share Feedback This book is a work in progress. You’d be doing me a big favor by taking a moment to tell me what you think about this chapter. References "],
["logic.html", "6 Theory of Change 6.1 All Models are Wrong, But Some Are Useful 6.2 Theory of Change 6.3 Logic Model Share Feedback", " 6 Theory of Change Underneath any good claim of causal inference is theoretical model of how we think X actually causes Y. Few studies set out to test a specific mechanism of impact, but most impact evaluations are designed around a theory about how the world works. We call this a theory of change. A theory of change articulates how an intervention—or a policy, program, or treatment—is expected to impact an outcome. It explains how X causes Y, and what is needed for this to happen. You might see this referred to as a theory of change, logic model, logical framework, causal model, results chain, pipeline model, results framework, program theory, or one of several other combinations of these words. Developing a good theory of change is an essential part of designing effective programs, but I like to teach students about these models because they help us think through measurement issues. Before you can articulate how you will measure key study variables, you need to think through what these variables are in the first place. A theory of change can help you approach this task. 6.1 All Models are Wrong, But Some Are Useful Before I describe how to create a theory of change, I want to frame this discussion in a broader context. A theory of change falls under the larger umbrella of what are known as conceptual models. A model is a simplified representation of a more complex reality. A plastic replica of the human heart is a model. So is an epidemic model of Ebola tranmission. Neither model is perfect, but we can probably learn something from each one. As the mathemetician and statistican George Box famously wrote, “Essentially, all models are wrong, but some are useful” (Box and Draper 1987). Conceptual models are used to propose how constructs are related—without necessarily claiming that the relationships are causal. You’ll come across many different types of conceptual models in the literature. Let’s review a few examples before turning to theories of change. 6.1.1 IN PSYCHOLOGY AND RELATED DISCIPLINES Conceptual models are very common in psychology, public health, and related fields. One of the most commonly used conceptual models in the study of health behavior change is the health belief model (Figure 6.1). The health belief model was developed in the 1950s as researchers in the U.S. Public Health Service were trying to understand why people were reluctant to engage in preventive health behaviors (Rosenstock 1974). This model suggests that health behaviors are explained by a person’s perception of the benefits vs. risks of action, perceived threat of the health issue, self-efficacy for change, and cues to action. Figure 6.1: Health belief model. Source: http://bit.ly/2i9Lw0Ehbm.jpg Another example is the information-motivation-behavioral skills (IMB) model proposed by Fisher and Fisher (1992) to explain HIV-related risk behaviors (Figure 6.2). Figure 6.2: Information-motivation-behavioral skills model Conceptual models like this one can be tested empirically (i.e., with data). For instance, Zhang et al. (2011) explored the fit of the IMB model to data on condom use among female commercial sex workers in China. Figure 6.3 is a path diagram that shows the results of a structural equation model that was fit to the data. Figure 6.3: Information-Motivation-Behavioral Skills structural equation model. Source: Zhang et al. (2011), http://bit.ly/2i2SrWb Structural equation modeling (SEM) is a multivarite technique that combines a sturctural model represented by the path diagram and a measurement model that specifies the relationships between latent variables and their indicators. In path diagrams like the one shown in the figure above, directly observed variables are called manifest variables and are represented as rectangles, and latent constructs are represented as elipses. For instance, condom use is a manifest variable that is directly measured, whereas self-efficacy is a latent construct that is measured indirectly by asking people several questions that are combined to make a composite indicator of this thing called self-efficacy. In the language of SEM, variables are labeled endogenous and exogenous rather than independent and dependent. In this example, knowledge and perceived risk are exogenous variables because they are not “caused by” another variable. Condom use is an endogenous variable. 6.1.2 IN EPIDEMIOLOGY Directed acyclic graphs A related type of causal diagram are directed acyclic graphs, or DAGs. This idea from mathematics and computer science has been applied to epidemiologic (observational) research in an effort to identify potential confounding variables that need to be addressed to make valid causal inferences (Greenland, Pearl, and Robins 1999). in Figure 6.4 shows two example DAGs. In this example, Suttorp et al. (2015) show that, at a minimum, it is necessary to control for age when estimating the relationship between chronic kidney disease and mortality (a). If cancer is directly related to kidney disease (b), it will also be important to control for cancer. Drawing out these relationships helps us to see that it may not be necessary to control for dementia because there is no direct relationship between dementia and kidney disease. Figure 6.4: Graphical presentation of confounding in directed acyclic graphs. Identification of a minimal set of factors to resolve confounding. In (a), the backdoor path from CKD to mortality can be blocked by just conditioning on age, as depicted by the box around age. However if we assume that cancer also causes CKD (b), the backdoor paths can only be closed by conditioning on two factors, either age and cancer (as depicted) or cancer and dementia. Source: Suttorp et al. (2015), http://bit.ly/2i5ZZwq Epidemic model Epidemic models are used to explain or predict the spread of an epidemic. Kermack and McKendrick (1927) proposed a deterministic compartmental model called SIR that consists of the number of uninfected people susceptable to the disease (S), the number of infected (I), and the number of people removed (R) through death or immunization. Rivers et al. (2014) used this basic framework to create a compartmental model of the 2014 Ebola outbreak in Liberia and Sierra Leone (Figure 6.5): Susceptible (S) Exposed (E) Infectious (I) Hospitalized (H) Funeral (F; handling bodies) Recovered/Removed (R) Figure 6.5: Compartmental flow of a mathematical model of the Ebola Epidemic in Liberia and Sierra Leone, 2014. Source: Rivers et al. (2014), http://bit.ly/2jkpl5G Rivers et al. (2014) published this model in November 2014, concluding: The forecasts for both Liberia and Sierra Leone in the absence of any major effort to contain the epidemic paint a bleak picture of its future progress, which suggests that we are in the opening phase of the epidemic, rather than near its peak. The key phrase is in the absence of any major effort to contain the epidemic. It turned out that November 2014 was actually the peak of new cases thanks to a coordinated effort respond to the outbreak. As the authors wrote at the time: Of the modeled interventions applied to the epidemic, the most effective by far is a combined strategy of intensifying contact tracing to remove infected individuals from the general population and placing them in a setting that can provide both isolation and dedicated care. This intervention requires that clinics have the necessary supplies, training and personnel to follow infection control practices. Although both of these interventions in isolation also have an impact on the epidemic, they are much more effective in parallel. 6.1.3 IN STATISTICS Statistical models are non-deterministic and thus incorporate stochastic variables. In other words, some of the variables being modeled have probability distributions rather than constant inputs that you might see in physics or mathmatics. Statistical models are typically communicated as a set of equations and visualized as a set of results. An exception is the path diagram represented visually in Figure 6.3. 6.1.4 IN ECONOMICS Economic models can be stochastic or non-stochastic. The field of econometrics shares a lot in common with statistics, including a focus on stochastic models. Economics more broadly, however, also uses non-stochatic models. For instance, Figure 6.6 shows the hypothesized mathmatical relationship between mortality and income (Wildman and Shen 2014). This graph is based on theory and does not plot actual empirical data collected in a particular setting. Figure 6.6: Effect of increased inequality on population mortality. Source: Wildman et al. (2014), http://bit.ly/2iUFqA9 6.2 Theory of Change Theory of change diagrams come in a variety of flavors. There is no RIGHT WAY™ to create one, as long as you can convey the fundamentals of how you think X leads to Y. If people understand your diagram, it is a good diagram. People are more likely to understand your diagram and your theory of change if you include a few common elements. The United Kingdom’s Department for International Development, commonly known as DFID or UK Aid, commissioned a report on the uses of theories of change in international development that identified several common components:35 influence of context discussion of long-term change process/sequence of change explained underlying assumptions presented as a diagram and narrative summary 6.2.1 TEMPLATE My preferred approach to outlining a theory of change is to follow this template from the W.K. Kellogg Foundation. Here is an editable PowerPoint version. Figure 6.7: Theory of change template. Source: W.K. Kellogg Foundation, http://bit.ly/1My75Ay Start with the (research) problem statement (Box 1). This box gets at the heart of the reason your intervention exists. What are you trying to solve? You’d be surprised how often program design is disconnected from the problem that led to the program in the first place. This planning process will help you prevent you from ending up in this situation. Next think about what assets already exist and what needs remain (Box 2). There is always something to build upon, which is why it’s important to look for strengths in addition to challenges. This process is best done in collaboration with people impacted by the problem so that the proposed solution is grounded in their reality. If available, descriptive data sources like the DHS can help to outline (1) and (2). For a more local perspective, it might be beneficial to conduct a brief needs assessment in partnership with the local community if resources permit. Box 3 jumps to the desired results. If the program works, what will change? We’ll come back to what is meant by “outputs, outcomes, and impact” in a moment. With the results articulated, you can go back to the beginning and consider what factors might affect your program’s success positively or negatively (Box 4). The next step is to outline strategies for achieving your desired results that take into account potential barriers and facilitators (Box 5). This is where you articulate what your program will actually do. Finally, Box 6 reminds you that every theory of change is built on a set of assumptions. Be thorough and transparent as you think through the hidden beliefs that underlie your ideas about how your program will achieve its results. 6.2.2 EXAMPLE: KENYAN “SUGAR DADDIES” In 2014, an estimated 1.4 million people in Kenya were living with HIV. This is a prevalence rate of 5.3% among adults aged 15 to 49. Without a cure for AIDS, prevention remains critical to ending the epidemic. Starting in 2001, Kenya integrated HIV/AIDS education into the primary school curriculum as a new prevention strategy (JPAL 2007). At the time, the focus of this program—and many others programs across sub-Saharan Africa—was complete risk avoidance (Dupas 2011). Abstinence. Information on risk reduction was limited. In particular, students were not learning about the differential prevalence of HIV infection by age and gender. Girls were not learn that the older “sugar daddies” who provide nice things like phones and airtime in return for sex are more likely than the girls’ goofy age mates to be infected. An organization called ICS Africa set out to change this by rolling out a “Relative Risk Information Campaign” in Kenya. The intervention was brilliant in its simplicity. A program staffer would talk with students for 40 minutes. During this time, the staffer showed the class a 10-minute video on sugar daddies and led a discussion about cross-generational sex. During the session, the staffer reviewed results of recent studies and wrote facts about the distribution of HIV prevalence on the chalk board. JPAL researchers tested ICS Africa’s risk reduction strategy in a randomized experiment in Western Kenya. In the first phase (2003), 328 schools were randomized to teacher training on the national HIV prevention curriculum (Duflo et al. 2006). In the second phase (2004), 71 of these schools were stratified and randomized to receive the sugar daddy intervention (Dupas 2011). In total there were four study arms: (i) teacher training only, (ii) sugar daddy only, (iii) teacher training and sugar daddy, and (iv) nothing. The results were shocking. Teacher training was a bust. Sure, the training led to a change in teaching practices—notably that trained teachers mentioned HIV in class more often than non-trained teachers—but it had little effect on HIV knowledge or childbearing. In contrast, the 40-minute sugar daddy discussion and video reduced childbearing with men at least five years older by 65%—and not because girls started having babies with males their own age. The overall incidence of childbearing fell by 28%. With a cost of $28.20 USD per school and $0.80 per student, the cost per childbirth averted was $91 (JPAL 2007). Returning to our template, let’s piece together the theory of change for the relative risk reduction program. Figure 6.8: Sugar daddy awareness theory of change Increasing knowledge about HIV makes intuitive sense as an outcome for a study about HIV prevention. But why childbearing? Well, babies don’t come from storks. They come from unprotected sex. It’s harder to lie about birthing a baby than it is to lie about your private sexual behavior, so childbearing is thought to be a more objective measure of unprotected sex. Unprotected sex is a main driver of HIV transmission, so measuring childbearing is a proxy for HIV risk from unprotected sex. 6.3 Logic Model While a theory of change tends to be a high-level depiction of the “why”, a logic model—or logframe—is more detailed and focuses on the “how”. Logical models are a useful tool for program planning, monitoring program implementation, and program evaluation and reporting. You’ll often see them presented in the “results chain” or “pipeline” format shown in Figure 6.9. In a logic model, inputs and activities represent your planned work. Outputs, outcomes, and impact are your intended results. Figure 6.9: Logic model. Source: W.K. Kellogg Foundation, http://bit.ly/1My75Ay Table 6.1: Components of a logic model. Component Description Input The resources needed to implement the program. People, money, time, etc. Activities What your program will do. Trainings, events, distribution of goods, etc. Outputs What your program did. Number of people trained, number of events held, number of goods delivered and number of people who benefitted. Outcomes Short- and medium-term results of your program. Increased knowledge, decreased risky behavior, improved functioning, etc. Impacts Long-term effects of the program outcomes. Lower HIV prevalence, reduced morbidity and mortality, etc. Figure 6.10 shows what a logic model might look like for the Kenyan relative risk reduction program introduced earlier. Figure 6.10: Sugar daddy awareness logic model As with a theory of change, there is no RIGHT WAY™ to create a logic model, but you will often see the same basic components (inputs, activities, outputs, outcomes, impacts) included. Figure 6.11 shows an example logic model for evaluating reproductive health programs. Figure 6.11: Reproductive health logic model. Source: MEASURE Evaluation, http://bit.ly/2iwphA7 The next step is to think through how to measure key variables throughout your conceptual model, theory of change, or logic model. Donors like USAID commonly require project staff to create results frameworks to monitor progress toward achieving the stated goals. Here’s an example results framework from the Feed the Future strategic plan for Kenya. You can read this diagram from the top down. The overall goal reflects the mission of the broader FTF program: reduce poverty and hunger. The plan for attaining this goal is to reach two objectives: inclusive agricultural sector growth and improved nutritional status. If the goal represents the ultimate desired impact, objectives are long range strategies. You know you are on track to reaching your objectives if you hit several intermediate results, or IRs. For instance, IR6, “Improved utilization of MCH and nutrition services”, is expected to improve the nutritional status of women and children. This IR has three sub IRs, including 6.3 “Strengthened MCH nutrition surveillance”. The rationale for sub-IR 6.3 is that better monitoring and data will enable earlier identification of at-risk individuals, which in turn should mean earlier initiation of nutrition interventions. Every IR has a set of indicators for measuring progress. In the case of IR6, FTF lists the following illustrative indicators: prevalence of maternal anemia number of children under five years of age who received vitamin A from USG-supported programs number of people trained in child health and nutrition through USG-supported health area programs (disaggregated by gender) number of clients who received food and/or nutrition services (PEPFAR indicator) Share Feedback This book is a work in progress. You’d be doing me a big favor by taking a moment to tell me what you think about this chapter. References "],
["indicators.html", "7 Measurement 7.1 Terminology 7.2 Identify Constructs 7.3 Select Good Indicators 7.4 Constructing Indicators 7.5 Indicators Throughout the Causal Chain Additional Resources on Indicators Share Feedback", " 7 Measurement In the last chapter you learned that outcomes are the intended results of your program. In this chapter you’ll learn how to go from outcomes to indicators and how to specify a measurement plan for all parts of your logic or conceptual model. 7.1 Terminology Regardless of what research design you are using, if your goal is to estimate the impact of a program/intervention/treatment/policy on [blank], [blank] is your outcome. But what is the difference between an outcome and an indicator? Between an indicator and an instrument? Glennerster and Takavarasha (2013) provide helpful definitions of the terms you’ll come across in discussions of measurement and data collection. I expand on their list in the table below. Term Definition Example Construct A characteristic, behavior, or phenomenon to be assessed and studied. Often cannot be measured directly. depression Outcome In an impact evaluation, ‘constructs’ will be referred to as outcomes—the intended results of your program. Also referred to as endpoint in a trial. decreased depression Indicator Observable measures of outcomes or other study constructs. depression severity score on a depression scale Instrument The tools used to measure indicators. Also referred to as a measure. a depression scale, made up of questions/items about symtoms of depression Variable The numeric values of the indicators. Respondent The person (or group) that we measure. Let’s work through an example to highlight the terms. Patel et al. (2016) designed a RCT in India to test the efficacy of a lay counsellor-delivered brief psychological treatment for severe depression. The hypothesized outcome was a reduction in severe depression. In a theory of change or logic model, outcomes take on the language of change: increases and decreases. But you’ll also see the word “outcome” used more generally, and synonymously with “indicator”, particularly in articles reporting study results. For example, Patel et al. (2016) write: Primary outcomes were depression symptom severity on the Beck Depression Inventory version II and remission from depression (PHQ-9 [Patient Health Questionnaire] score of &lt;10) at 3 months in the intention-to-treat population, assessed by masked field researchers. Using the language in the table above, we could also say that the primary outcome was severe depression and the team measured two indicators of severe depression: (i) a depression symptom severity score on the Beck Depression Inventory version II (BDI-II) and (ii) a score of less than 10 on the PHQ-9. They used two instruments to measure depression: the PHQ-9 (pdf) and the BDI-II. Outside of the impact evaluation literature, the word “outcome” will often be replaced with “dependent variable” or “response variable”. Additional constructs of interest might be called “covariates”, “independent variables”, or “exposure variables”. To make matters simple, I recommend asking yourself the following questions when you are planning a study: What is the key construct you want to study? What other constructs do you need to measure at the same time to fully understand your key construct? What are indicators for these constructs? In other words, how will you quantify these constructs? How will you measure these quantities? What instruments will you use? (This is the topic of the next chapter.) Fundamentally, there should be a logical flow from your research problem to your measurement of primary study outcomes/constructs. Figure 7.1 demonstrates this using Patel et al. (2016) as an example. Figure 7.1: From research problem to study instruments The language of qualitative studies is a bit different. There is an emphasis on study constructs, but not on indicators or measures. Quantification is not the goal. 7.2 Identify Constructs Most studies are designed to provide the best evidence possible about one or two primary outcomes linked directly to the main study objective. Secondary outcomes may be registered, investigated, and reported as well, but these analyses could be more exploratory if the study design is not ideal for measuring these additional outcomes. For instance, Patel et al. (2016) included the following secondary outcomes in addition to depression severity and remission from depression: Secondary outcomes were disability on the WHO Disability Assessment Schedule II and total days unable to work in the previous month, behavioural activation on the five-item abbreviated Activation Scale based on the Behavioural Activation for Depression Scale-Short Form, suicidal thoughts or attempts in the past 3 months, intimate partner violence (not a prespecified hypothesis), and resource use and costs of illness estimated from the Client Service Receipt Inventory. These outcomes were labeled secondary because the study was powered on the primary outcomes (a topic of a later chapter): …we aimed to recruit 500 participants to detect the hypothesised effects (a standardised mean difference of 0·42), with 90% power for the primary continuous outcome of depression severity and 92% power to detect a recovery of 65% in the HAP group for our primary binary outcome of depression remission. The basic idea is that one study cannot definitively answer every possible research question. There are tradeoffs in terms of the time, money, and resources, and investigators must prioritize among all possible outcomes. 7.3 Select Good Indicators To define a study construct in terms of an indicator and to specify its measurement is to operationalize the construct. Indicators should be DREAMY™: Defined clearly specified Relevant related to the construct Expedient feasible to obtain Accurate valid measure of construct Measurable able to be quantified customarY recognized standard 7.3.1 DEFINED It is important to clearly specify and define all study variables, especially the indicators of primary outcomes. This is a basic requirement for a reader to critically appraise your work, as well as a building block for future replication attempts. For instance, the construct of interest in Patel et al. (2016) was severe depression, and the two indicators were (i) depression symptom severity and (ii) remission from depression. The authors pre-registered the trial and defined these outcomes as follows: Mean difference in total score measured by the Beck’s Depression Inventory (BDI-II), at 3 months, a 21-item questionnaire assessment of depressive symptoms; each item is scored on a Likert scale of f 0 to 3. It measures depression severity based on symptom scores. Remission, defined as a score of &lt;10 measured at 3 months by the Patient Health Questionnaire (PHQ-9), a nine-item questionnaire for the detection and diagnosis of depression based on DSM-IV criteria. It is scored on a scale of 0 to 3 based on frequency of symptoms. 7.3.2 RELEVANT Indicators should be relevant to the construct of interest. In Patel et al. (2016), scores on the BDI-II and PHQ-9 are clearly measures of depression severity and remission. An example of a non-relevant indicator would be scores on the Beck Anxiety Inventory, a measure of anxiety. While anxiety and depression are often co-morbid, anxiety is a distinct construct. 7.3.3 EXPEDIENT It should be feasible to collect data on the indicator given a particular set of resource constraints. Asking participants to complete a 21-item questionnaire and a 9-item questionnaire as in Patel et al. (2016) does not represent a large burden on study staff or participants. However, collecting and analyzing biological samples such as hair, salavia, or blood might. 7.3.4 ACCURATE Accurate is another word for “valid”. Indicators must be valid measures of study constructs. In other words, we need to be sure that scores on the BDI-II and PHQ-9 measure this thing we’re calling depression. I’ll discuss this in more detail shortly. 7.3.5 MEASUREABLE Indicators must be quantifiable. Psychological constructs like depression are often measured through the use of scales such as the BDI-II and the PHQ-9. Other constructs require more creativity. For instance, Olken (2005) measured corruption in Indonesia by digging core samples of newly build roads to estimate the amount of materials used in construction and then compared cost estimates against reported expenditures to get a measure of corruption (i.e., missing expenditures). 7.3.6 CUSTOMARY Whenever possible, it’s smart to use standard indicators and follow existing definitions and calculation methods. One way to learn about standards and customs is to keep up with the literature and find articles that measure the same constructs. If you want to publish the results of your impact evaluation of a microfinance program in an economics journal, read other papers by economists. How do they measure outcomes like income, consumption, and wealth? Study measurement is not a good opportunity to show your creative side unless you are explicitly trying to overcome limitations to the standard methods. If you are studying population-level issues, it’s likely that you can find your indicator in the World Health Organization’s Global Reference List of the 100 core health indicators (WHO 2015a): Figure 7.2: WHO 100 core health indicators. Source: http://bit.ly/1NgGeLh If 100 is not enough indicators for you, try the United Nations Sustainable Development Goals. There are 230 indicators to measure 169 targets for 17 goals! The SDG indicators even get their own website. Figure 7.3: Sustainable Development Goals. Source: http://bit.ly/2cuDpWN. 7.4 Constructing Indicators 7.4.1 SINGLE ITEM INDICATORS Some indicators are measured with responses to a single item (or a short series of items) on a survey. For instance, in Malaria Indicator Surveys the “proportion of households with at least one ITN” is the “number of households surveyed with at least one ITN” (numerator) divided by the “total number of households surveyed” (denominator). The numerator for this indicator is obtained from asking the household respondent if there is any mosquito net in the house that can be used while sleeping and from determining whether each net found in a household is a factory-treated net that does not require any treatment (an LLIN) or a net that has been soaked with insecticide within the past 12 months. The denominator is the total number of surveyed households. To determine if a household has an ITN, enumerators ask the following sequence of questions. Number Question 119 Does your household have any mosquito nets? 120 How many mosquito nets does your household have? 121 ASK THE RESPONDENT TO SHOW YOU ALL THE NETS IN THE HOUSEHOLD 122 How many months ago did your household get the mosquito net? 123 OBSERVE OR ASK BRAND/TYPE OF MOSQUITO NET 124 Since you got the net, was it ever soaked or dipped in a liquid to kill or repel mosquitoes? 125 How many months ago was the net last soaked or dipped? The end result is a binary indicator (yes/no) of whether the household has a bednet that has been dipped in the past 12 months or is factory-treated. In theory it is possible to ask this in one question—“Does your household have any factory-treated mosquito nets or nets that have been dipped in a liquid to kill or repel mosquitoes in the past 12 months?—but this is a long and complicated question. It’s more effective to break up the question. Sometimes more abstract constructs can also be measured with just one item. For instance, Konrath et al. (2014) ran 11 studies and found that you can measure narcissism with one question: To what extent do you agree with this statement: “I am a narcissist.” Response options range from “not very true about me” (1) to “very true of me” (7).36 Most often, however, constructs like narcissism and depression are measured with multiple items that are combined into indexes or scales. These two terms are often used interchangeably, but they are not synonyms. While sharing in common the fact that multiple items or observations go into their construction—making them composite measures—the method and purpose of combining these items or observations is distinct. 7.4.2 INDEXES Indexes combine items into an overall composite, often without concern for how the individual items relate to each other. For instance, the Dow Jones Industrial Average is a stock market index that represents a scaled average of stock prices of 30 major U.S. companies such as Walt Disney and McDonald’s. The Dow Jones is a popular indicator of market strength and is constantly monitored during trading hours. Every index has its quirks, and the Dow Jones is no exception. Companies with larger share prices have more influence on the index. An index popular with the global health crowd is the DHS wealth index. As a predictor of many health behaviors and outcomes, economic status is a covariate in high demand. Failing to measure economic status in a household survey would be like failing to note a respondent’s gender or age, but measuring economic status is not nearly as easy.37 In an ideal data world every survey would include accurate information on household income and consumption as measures of household wealth. Income is volatile, however, and consumption is very hard to measure quickly. So in the late 1990s, researchers first proposed creating an index of household assets as a measure of a household’s economic status (Rutstein and Johnson 2004). Data for the wealth index come from DHS surveys conducted in a particular country. Indicator variables include individual and household assets (e.g., phone, television, car), land ownership, and dwelling characteristics, such as water and sanitation facilities, housing materials (i.e., wall, floor, roof), persons sleeping per room, and cooking facilities. The figure below shows a snapshot of the DHS Household Questionnaire. Figure 7.4: DHS Round 7 Household Questionnaire. A key decision when creating indexes like the wealth index is whether to weight the individual components. Should owning a car be given the same weight as owning a phone? In other words, in constructing an index that measures someone’s wealth, should owning a phone contribute as much to the index as owning a car? Most readers would probably say no, so the next question is how to assign differential weights to the components. Filmer and Pritchett (2001) first proposed assigning weights via principal components analysis, or PCA. PCA is a data reduction technique in which indicators are standardized (i.e., transformed into z-scores) so that they each have a mean of 0 and a variance of 1. If there are 10 items, the total variance is therefore 10, and there are 10 principal components. A principal component (aka eigenvector) is a linear combination of the original indicators, so every indicator (e.g., yes/no to owning a phone) has a factor loading that represents the correlation between the individual indicator and the principal component. The first principal component always explains the most variance, and each component after the first explains a smaller and smaller amount of total variance. In constructing the wealth index, we assume that the first component measures this thing called “wealth”, so we use the factor loadings on the first principal component to create a score for each household. Let’s use the 2014 Bangladesh DHS survey as an example. Figure 7.5: Example wealth index construction (abbreviated from 2014 Bangladesh DHS). As shown above, the factor loading for water piped into a dwelling (i.e., indoor plumbing) was 0.056 in the PCA run on the 2014 Bangladesh DHS data. In order to create the index, this loading gets converted into a score for whether the household has or does not have the asset, and these indicator scores are summed to get an overall index score for each household. Once every household has an index score, it’s possible to assign every participant to of 1 of 5 wealth quintiles reflecting their economic status (relative to the sample). This makes it possible to examine the relationship between health outcomes and wealth. 7.4.3 SCALES In an index, indicators “cause” the concept that is being measured. For instance, a household’s wealth is determined by the assets it owns (e.g., livestock, floor quality). Conversely, in a scale, the concept “causes” the indicators. Figure 7.6: Scale vs index. Let’s take an example like depression. There is not a blood test for depression, so depression is a construct that needs a definition. According to the Diagnostic Criteria for Major Depressive Disorder and Depressive Episodes, currently the DSM-V, the criteria for Major Depressive Disorder are as follows, A-E: A. Five (or more) of the following symptoms have been present during the same 2-week period and represent a change from previous functioning; at least one of the symptoms is either (1) depressed mood or (2) loss of interest or pleasure. Depressed mood most of the day, nearly every day, as indicated by either subjective report (e.g., feels sad, empty, hopeless) or observation made by others (e.g., appears tearful). Markedly diminished interest or pleasure in all, or almost all, activities most of the day, nearly every day (as indicated by either subjective account or observation.) Significant weight loss when not dieting or weight gain (e.g., a change of more than 5% of body weight in a month), or decrease or increase in appetite nearly every day. Insomnia or hypersomnia nearly every day. Psychomotor agitation or retardation nearly every day (observable by others, not merely subjective feelings of restlessness or being slowed down). Fatigue or loss of energy nearly every day. Feelings of worthlessness or excessive or inappropriate guilt (which may be delusional) nearly every day (not merely self-reproach or guilt about being sick). Diminished ability to think or concentrate, or indecisiveness, nearly every day (either by subjective account or as observed by others). Recurrent thoughts of death (not just fear of dying), recurrent suicidal ideation without a specific plan, or a suicide attempt or a specific plan for committing suicide. B. The symptoms cause clinically significant distress or impairment in social, occupational, or other important areas of functioning. C. The episode is not attributable to the physiological effects of a substance or to another medical condition. D. The occurrence of the major depressive episode is not better explained by schizoaffective disorder, schizophrenia, schizophreniform disorder, delusional disorder, or other specified and unspecified schizophrenia spectrum and other psychotic disorders. E. There has never been a manic episode or a hypomanic episode. If someone meets criteria A-E, they are diagnosed with Major Depressive Disorder. A diagnosis by a trained mental health professional like a psychiatrist would usually be considered the gold standard measure of depression. Gold standards are in short supply in many places, however, and we need more feasible methods of measuring this thing called depression. A reasonable alternative is to develop a set of questions—a scale—that we could administer to someone as a way of measuring their symptom severity. Presumably, if a person scored high enough on this scale, we’d classify them as depressed. In this example, depression is the latent variable that we can’t measure directly. To get an indicator of depression, we need to measure a combination of manifest variables that are ‘caused’ by the latent variable depression. Patel et al. (2016) used the Beck Depression Inventory (Beck, Steer, and Brown 1996) to measure depression severity. The BDI-II consists of 21 groups of statements, such as: Sadness I do not feel sad I feel sad much of the time I am sad all the time I am so sad or unhappy that I can’t stand it Pessimism I am not discouraged about my future I feel more discouraged about my future than I used to be I do not expect things to work out for me I feel my future is hopeless and will only get worse Each item is a manifest variable—something that we measure directly by asking the question. The latent variable depression is measured indirectly by summing a person’s responses to all 21 manifest variables to create the BDI-II scale score. Determining the factor strucutre of scales Exploratory factor analysis Typically when developing a new scale, researchers will start with a large pool of potential items—many more than they could ever use an applied context where administration time is a relevant constraint—and use exploratory factor analysis or some other method of data reduction to shrink the pool. Exploratory factor analysis (EFA) looks a lot like PCA, but they are conceptually and computationally distinct. Whereas PCA results in a linear combination of indicators that maximized total variance, factor analysis maximizes the common or shared variance. Factor analysis helps us to understand the structure of the data. For instance, the BDI-II consists of 21 items that are thought to measure the latent construct of depression, but many studies have examined whether these items can be grouped into subfactors—different domains of depression. Manian et al. (2013) administered the BDI-II to 953 new mothers “from a large East coast metropolitan area” and then conducted EFA on data from half of the sample.38 They looked for 2- to 4-factor solutions and found that a 3-factor model made the most sense empirically (based on data) and theoretically (based on their knowledge of the literature). Their model suggested that the latent variable of depression is composed of three subfactors as shown in the figure below: cognitive symptoms, affective symptoms, and somatic symptoms. Confirmatory factor analysis Manian et al. (2013) then used the holdout data (i.e., data from their sample not used in the EFA) to test the fit of their 3-factor model through confirmatory factor analysis (CFA). It fit! The model is shown below. If you are using an existing scale in a new population or setting, CFA is a good technique to determine if the original factor structure generalizes to your context. A typical case is one in which the original scale is developed in a high-income setting and research suggests that it makes sense to construct an overall scale score (of some latent variable like depression) AND 2 or 3 subscale scores that correspond to subfactors like cognitive symptoms and affective symptoms. However, let’s say that you recruit a sample in a completely new context and your CFA suggests that the original 2-factor model does not fit. Maybe in a different cultural context depression is not manifested along the same dimensions of cognitive and affective symptoms. Such a finding should make you question whether it makes sense to use the scale as is without further evaluation of its applicability. Figure 7.7: Final 3-factor model of the BDI-II with standardized path coefficients. Source: Manian et al. (2013). Constructing scale scores An under-appreciated question when it comes to scales is how to actually construct scale scores. Let’s say you come up with a bunch of items that you think measure this thing—this latent construct—called depression, administer the survey with these items to a few hundred people, and conduct EFA and CFA to determine the factor structure. Now what? Well, you have a few options. DiStefano, Zhu, and Mindrila (2009) lump them into two buckets: refined and non-refined. Non-refined methods are most commonly used because they are simple to compute and easy to compare across samples. Sum raw scores. If there are 21 items each with a possible range of 0 to 3, you just add up the scores on each item. This is what the BDI-II does, which gives it a possible range of 0 to 63. Average raw scores. Same idea as summing, but averaging keeps the possible range consistent with the response scale. For instance, if you average 21 items with response options that range from 0 to 3, the possible scale scores will range from 0 to 3. Some people think this makes more intuitive sense when presenting results. Sum standardized scores. With this method you would first standardize each item to have the same mean and standard deviation. It might be a good idea if the standard deviations of the items vary quite a bit. Refined methods may produce more exact scores since items are weighted empirically (vs equal weighting in non-refined methods) and relationships between factors are reflected in the scoring, but they are more complex and require the analyst to make a number of decisions along the way that can produce very different results. 7.4.4 EVALUATING PSYCHOMETRICS When it comes to evaluating scales like the BDI-II, we often look at several psychometric properties that we can group generally into two buckets: reliability and validity. Here’s a simple example that highlights the basic difference between these terms. Imagine your bathroom scale. If you stepped on, then off, then on again, and the scale read 210 lbs and then 180 lbs, you would realize that you are the owner of a broken, unreliable scale. So you head to the store and pick up a new scale. You step on and off your new scale, and it reads 400.12 lbs then 400.15 lbs. It’s very reliable (good precision), but unfortunately very wrong (poor accuracy, invalid) since you actually weigh something closer to 195 lbs. Reliability A reliable instrument is a consistent instrument. Consistent over repeated use (as in the bathroom scale example), and consistent among it’s component parts. There are several methods for assessing the reliability of an instrument. Here are a few common approaches. Test-retest reliability This is sometimes referred to as stability. Participants complete your survey today and then again in after a short period of time, maybe a few days or a week. If each person’s score is the exactly the same the second time, your instrument would be perfectly reliable. It won’t be, but you’ll hope for a high correlation coefficient (conventionally higher than 0.70). Beck, Steer, and Brown (1996) assessed the test-retest reliability of the BDI-II by giving the screening to 26 outpatients in Philadelphia at their first and second therapy sessions, approximately 1 week apart. They reported a test-retest correlation was 0.93. The tricky thing with test-retest reliability is knowing when to conduct the retest. Wait too long and scores will change because people change. Don’t wait long enough and you just get people repeating their answers from memory. Interitem reliability This is when responses to items in your instrument are consistent. If not, you have to wonder if they are measuring the same underlying construct of depression. 1. One approach to finding unreliable items in your instrument is to calculate item-total correlations. It’s easy: correlate responses on each item with the total scale score. Generally item-total correlations exceeding 0.30 are sufficient. Beck, Steer, and Brown (1996) reported that item-total correlations for the 21 BDI-II items ranged from 0.39 to 0.70 in the outpatient sample. 2. Another approach is Cronbach’s alpha, a measure of internal consistency. Beck, Steer, and Brown (1996) reported a coefficient alpha value of 0.92 for the outpatient sample. To understand Cronbach’s alpha, you have to understand that instruments are imperfect, even the BDI-II. Every person’s observed score—e.g., their total score on the BDI-II—is actually a function of their ‘true’ score (which we can’t know) plus some amount of measurement error. Cronbach’s alpha gives you an estimate of how much variance in people’s scores is measurement error. When you calculate Cronbach’s alpha in a program like R or Stata, behind the scenes the program does the equivalent of splitting the dataset into two halves over and over again and calculating the correlation between total scores for the first half with total scores for the second half. Cronbach’s alpha is the average of all possible correlation coefficients. Things to note about alpha: Alpha can range from 0 to 1 0.70 is a rough guide for the low-end of acceptable A value of 1 would indicate complete redundancy suggesting that the items are too similar! Alpha is sensitive to the number of items so a high alpha might just reflect that there are a lot of items included in the scale Alpha is not a property of the test, rather a characteristic of the test when used in a particular sample Alpha should not be used when a scale might tap different latent constructs—only use alpha when the scale is unidimensional Dunn, Baguley, and Brunsden (2014) and others reviewed the limitations of alpha and suggest coefficient omega as an alternative Interrater reliability This is another type of reliability that indicates whether two observers are consistent in observational ratings. Instead of using self-report instrument like the BDI-II, we might want to have two observers watch a video of a parent and child interacting and ‘code’ the parent’s behaviors using a depression rating system that we developed. If the observers agree a lot in their video ratings, they would be reliable. Things to note about interrater reliability: Percent agreement is one method of evaluating two raters when the rating is binary, but it does not account for agreement that can happen by chance Cohen’s kappa coefficient does account for agreement by chance; generally want a value greater than 0.40 Weighted kappa is good when the rating scale is ordinal (e.g., good &lt; better &lt; best) and you need to account for the fact that good vs best represents more disagreement than better vs best Intraclass correlation is a good option when you have 2 or more raters Validity Ask this three times: does your measure measure what you intend to measure with your measure? Or more simply: is the BDI-II an accurate measure this thing called ‘depression’? If not, it’s not a valid measure of depression. There are several types of validity that you can use to determine whether your instrument is valid. Face validity This is the weakest form of validity. The basic idea is: if it looks like a duck, quacks like a duck, it’s a duck. If people think your depression instrument asks about depression, then it has face validity as a measure of depression. This is a weak standard, however. A great looking instrument can perform very poorly in practice, and an instrument that appears to lack face validity might perform very well. The bottom line is that if you read an article and the only mention of validity is face validity, it’s a lame duck. Construct validity Depression is a hypothetical construct. If your new depression instrument has construct validity, it will be more strongly related to other instruments that are also thought to measure depression (convergent validity) and less strongly (or not at all related) to other instruments that claim to measure something other than depression (discriminant validity). For instance, Beck, Steer, and Brown (1996) reported that the BDI-II was more positively correlated with the Hamilton Psychiatric Rating Scale for Depression (0.71; convergent) than the Hamilton Rating Scale for Anxiety (0.47, discriminant). If your instrument has both convergent and discriminant validity, you have more confidence that it measures the construct you think it measures. Content validity This form of validity asks whether the components of an instrument—e.g., each question in a questionnaire—is relevant to the measurement of the larger construct. For instance, a question about difficulty sleeping is relevant to the measurement of depression since insomnia is a common symptom of depression. Conversely, a question about compulsive behaviors is probably not relevant since compulsive behaviors are not a typical symptom of the syndrome. Content validity also accounts for missing dimensions of a construct. If the BDI-II lacked a question about A5, “Psychomotor agitation”, we might question its content validity. Criterion-related validity An even more robust form of validity is criterion validity. Does an index test correctly classify people by their true disease status as determined by some gold standard criterion reference? Does a new rapid diagnostic test correctly identify evidence of malaria parasites in human blood samples? Do scores on the BDI-II correctly predict which people will be diagnosed with depression when evaluated independently by a mental health professional–the gold standard? Let’s use an example to explore criterion-related validity in more depth. Kim et al. (2014) conducted a study with 562 HIV-positive adolescents in Malawi in which they had the adolescents complete the BDI-II and then participate in a separate clinical interview with clinicians trained to use a structured interview tool called the Children’s Rating Scale, Revised (CDRS-R). Index test: With 21 items on the BDI-II—each with possible response values from 0 to 3—BDI-II scale scores can range from 0 to 63. Higher scores represent more severe depression. One goal of a validity study is to find a cutoff score on the index test that maximizes diagnostic accuracy. For instance, if the cutoff score is 15, anyone who scores greater than 15 is classified as depressed and everyone else is classified as not depressed. Criterion reference: In this study, independent clinician classification of depression was the gold standard. Every adolescent was classified as depressed or not depressed following a clinical interview. Taken together, there are four possible combinations of index test and gold standard results that we can display in a confusion matrix: true positive (a): both the test result and the gold standard indicate that the person is depressed true negative (d): both the test result and the gold standard indicate that the person is NOT depressed false positive (b): the test result suggests depression, but the gold standard disagrees false negative (c): the test result suggest no depression, but the gold standard disagrees Figure 7.8: Confusion matrix based on Kim et al. (2014). From this base set of numbers we can calculate a number of useful metrics about the index test (see the STARD guidelines for reporting in studies of diagnostic accuracy): Metric Details Prevalence This is the proportion of the sample (with the right methods we can infer to the population) who have (or had) a certain characteristic such as depression. We will distinguish between point, period, and lifetime prevalence, as well as introduce important issues related to sampling and prevalence, in a later chapter. Accuracy This is the total correct classification rate for a particular cutoff point. How often does the test correctly classify people according to their ‘true’ disease state measured by the criterion reference? 1-misclassification. [Area under the curve, or AUC, is another useful metric of test accuracy. See below for more details.] Sensitivity Sensitivity is also referred to as the true positive rate. Kim et al. (2014) reported an estimate of 0.75, which indicates that 75% of adolescents with depression correctly screened positive with a cutoff value of 15 False Negative Rate The flip side of sensitivity (true positives) is the false negative rate. In this example, 25% of depressed adolescents were misclassified by the test as not depressed. Specificity Specificity is also referred to as the true negative rate. Kim et al. (2014) reported an estimate of 0.77, which indicates that 77% of non-depressed adolescents correctly screened negative with a cutoff value of 15. False Positive Rate The inverse of specificity (true negatives) is the false positive rate. In this example, 33% of non-depressed adolescents were misclassified by the test as depressed. Positive Predictive Value If someone tests positive, you want to know how likely it is that the person is actually positive. This is what PPV indicates, but there’s a catch: the PPV (and NPV, below) depend on the prevalence of the condition in the sample, so they should only be used with representative samples obtained by probability sampling. Kim et al. (2014) report PPV, but this is questionable given their use of convenience sampling. If this were a probability sample, we could interpret a PPV of 0.43 to mean that the probability of depression when testing positive with the BDI-II using a cutoff of 15 is 43 percent. How is this different from sensitivity? Sensitivity does not depend on prevalence, but PPV does. A test can be sensitive (i.e., high true positive rate) but still produce many false positives if the prevalence of the condition is low, resulting in a low PPV. False Discovery Rate This is the inverse of PPV. Whereas PPV is the probability of disease if the test is positive, the FDR is the probability of a false positive (a false discovery) if the test is positive. Negative Predictive Value Similar to the PPV, the NPV asks if a person tests negative, what is the probability that they are actually negative. In the example shown, a NPV of 0.93 can be interpreted to mean that the probability of not being depressed when testing negative with the BDI-II using a cutoff of 15 is 93 percent. False Omission Rate This is the inverse of NPV. Whereas NPV is the probability of no disease if the test is negative, the FDR is the probability of a false negative (a false omission) if the test is negative. Positive Likelihood Ratio This is a measure of how much more likely a positive test result occurs in people with the condition compared to people without the condition. A LR+ of 1 would mean that the result is equally likely, therefore the test is not very helpful. In Kim et al. (2014) the LR+ is 3.3, which means that a positive test result is 3.3 times more likely if the person is actually depressed. This is fairly modest. As a clinician, you’d hope to see that the test had a LR+ ratio greater than 10 as a general rule. Negative Likelihood Ratio This is a measure of how much more likely a negative test result occurs in people with the condition compared to people without the condition. Again here a LR- of 1 signifies that the test is not useful. A ratio less than 1—particularly less than 0.1—is a good indicator that a negative result is diagnostically accurate. In the sensitivity and specificity calculations shown above, the example cutoff score for depression on the BDI-II was set to 15. To determine if 15 is the best cut point, we could calculate the sensitivity and specificity again for several different cutoff points and plot the relationship in a receiver operating characteristic (ROC) curve. ROC curves plot sensitivity (the true positive rate) against 1 minus specificity for a range of different cutoff points. For instance, the figure below shows the ROC curve presented in Kim et al. (2014). Receiver operating characteristic (ROC) curve for the BDI-II and CDI-II-S as compared to the CDRS-R. Source: Kim et al. (2014). The best cutoff is typically the one that maximizes sensitivity and specificity.39 In a ROC curve, this is the point that is closest to the top left of the graph—sensitivity of 1 and (1-specificity) of 0. In Kim et al. (2014), the optimal cutoff did indeed turn out to be 15. To evaluate the overall accuracy of the test, we can calculate the area under the ROC curve (AUC). In Kim et al. (2014), the AUC for the BDI-II is 0.82, generally considered to be an accurate benchmark (1=perfect; 0.5=worthless). The BDI-II performs better than an alternative screening instrument also assessed in this study called the Children’s Depression Inventory-II-Short (CDI-II-S); AUC=0.75. What is the difference between overall accuracy ((true positives + true negatives) / total) and AUC? Recall that the numbers you can calculate in a confusion matrix are dependent on the threshold you set for what counts as a positive test. Different thresholds (cutoffs) result in different patters of misclassification. AUC, on the other hand, takes into account sensitivity and specificity associated with all possible cutoffs. 7.5 Indicators Throughout the Causal Chain Indicators that define inputs, activities, and outputs in a logic model can be classified as process indicators. Process indicators capture how well a program is implemented. In short, the “M” (monitoring) in M&amp;E. As researchers we care about collecting good process/monitoring data in order to develop a better understanding why programs do or don’t work. For instance, we might want to track program costs so that we can estimate cost-effectiveness. Or we might want to know if the intervention was delivered according to plan or not. Researchers often rely on program partners to deliver the intervention under investigation, so issues like fidelity to treatment and compliance with study protocols are important to track closely. Let’s refer once more to Patel et al. (2016) to review a few examples of process indicators that intervention researchers often care about. Figure 7.9: Indicators throughout the causal chain. 7.5.1 INPUTS As you’ll recall from the previous chapter, inputs are the resources needed to implement the program. The most basic input of all is money, therefore one indicator is program cost. Impact evaluations produce estimates of the effectiveness of a program or intervention. Does the program “work”? For some public health and behavioral health nerds, evidence of impact is enough because they are narrowly focused on developing and testing new interventions. Not true for policymakers who are thinking about delivering programs at scale with limited public funding; they want to know whether the intervention is cost-effective, not just effective.40 A cost-effectiveness analysis requires close tracking of the cost of all program inputs. Patel et al. (2016) indicate that the HAP program costs $66 per person, or $181 per remission from depression at 3 months. 7.5.2 ACTIVITIES Treatment fidelity is a measure of how closely the actual implementation of a treatment or program reflects the intended design. The consequence of low treatment fidelity is usually an attenuation (aka, shrinking) of treatment effects. This is a threat to internal validity. If the study shows no effect but treatment fidelity is low, we can’t be confident in the null result. Implementation failure rather than theory or program failure could be to blame. Low fidelity is also a threat to external validity because it isn’t possible to truly replicate the study. Patel et al. (2016) measured fidelity in several ways, including external ratings of a random 10 percent of all intervention sessions. An expert not involved in the program listened to recorded sessions and compared session content against the HAP manual. 7.5.3 OUTPUTS Treatment compliance is a measure of the extent to which people (or units) were treated or not treated according to their study assignment. Sometimes people assigned to the treatment group don’t take up the treatment, or only complete part of the planned intervention. It’s also possible for members of the control or comparison group to be treated accidentally. Both are examples of broken randomization. When there is only non-compliance to randomization on the treatment side, we call it one-sided non-compliance. When some members of the control group are also non-compliant with randomization, it’s called two-sided non-compliance. Patel et al. (2016) randomly assigned 495 eligible adults to the HAP plus enhanced usual care condition (247) or the enhanced usual care condition alone (248). No one in the EUC only condition was treated with HAP, but 31 percent of the HAP group had an unplanned discharge and did not complete the treatment. We’ll discuss analysis strategies for one- and two-sided non-compliance in a later chapter. Additional Resources on Indicators Topic Resource Malaria Roll Back Malaria (2013). Household Survey Indicators for Malaria. Measure Evaluation (2016). Monitoring and Evaluation of Malaria Programs. HIV/AIDS WHO (2015). Consolidated Strategic Information Guidelines for HIV in the Health Sector. TB WHO (2015). A Guide to Monitoring and Evaluation for Collaborative TB/HIV Activities: 2015 Revision. Family Planning FP2020 (2015). Measurement Annex. Share Feedback This book is a work in progress. You’d be doing me a big favor by taking a moment to tell me what you think about this chapter. References "],
["datacollection.html", "8 Data Collection Methods 8.1 Quantitative Methods 8.2 Qualitative Methods 8.3 Mixed Methods Share Feedback", " 8 Data Collection Methods Now that you know what indicators you want to measure, it’s time to decide how to measure them. Sometimes it’s possible to use existing data sources, such as administrative or medical records, but most likely part or all of your data will come from original data collection efforts. In this chapter we’ll review common quantitative and qualitative methods that you can use. 8.1 Quantitative Methods An instrument is a tool for measuring indicators (Glennerster and Takavarasha 2013). Many studies in global health rely on survey instruments, so we’ll begin our tour with surveys before turning to non-survey instruments. 8.1.1 SURVEYS The most common type of data collection instrument in global health is surveys. Surveys are relatively cheap and easy to administer compared to some methods like biomarker testing, but care must be taken to pre-test the instrument, train enumerators, and monitor the administration. I’ll use the term ‘survey’ throughout this section, but some people would use the more specific term ‘questionnaire’ instead. This is because survey can represent the larger category of data collection that includes interviews. When interviews are structured or semi-structured, they look a lot like a written questionnaire that is read aloud to participants. Designing a survey instrument Start with standard tools Whenever possible, begin with well-known survey instruments. A great option for basic demographic and health questions is the DHS model questionnaires. There are four types plus several optional modules: household, woman, man, and biomarker. Household Woman Man Biomarker Optional Household schedule Background Background Anthropometry Domestic Violence Household characteristics Reproductive behavior and intentions Reproduction Anemia Female Genital Cutting Contraception Knowledge and use of contraception HIV Maternal Mortality Antenatal, delivery, and postnatal care Employment and gender roles Fistula Breastfeeding and nutrition HIV and other sexually transmitted infections Out-of-pocket Health Expenditures Children’s health Other health issues Status of women HIV and other sexually transmitted infections Husband’s background Other topics DHS questionnaires and analysis code are available for download, along with materials used in the AIDS Indicator Survey (AIS), Malaria Indicator Survey (MIS), Service Provision Assessment (SPA), and Key Indicators Survey (KIS). Writing good survey questions Sometimes you have to create your own surveys. Start early! Writing good survey questions is an art. It takes a lot of practice and trial &amp; error to get it right. Common problems include: Use of confusing or complex language Unclear meaning Use of double-negatives Embedding more than one question in a question (double-barreled) Use of leading statements Hard to answer The solution to all of these problems is pre-testing. Cognitive interviewing is a good technique for pre-testing in which you ask a question and record a response, but only as a way of inquiring about the respondent’s understanding of the question. For instance, let’s say the survey item is: Over the past 2 weeks, how often have you been bothered by feeling tired or having little energy? Not at all, several days, more than half the days, nearly every day. In cognitive interviewing, you would ask the respondent to explain the meaning or the purpose of the question. What do you think I am asking you to tell me? What does it mean to experience something for ‘more than half of the days’ in the past two weeks? If the respondent’s answers reflects a clear understanding of the question and response options, it’s probably a good item. If not, it might be worth exploring alternative phrasings that can be pre-tested with other members of your target population. These tests can take place one-on-one or in a group format. Some researchers advocate mixing positively and negatively worded questions to limit acquiescence bias, which happens when respondents get in the pattern of just agreeing (or disagreeing) when question after question follows a similar format. The potential downside of mixing directions is that respondents might have a harder time understanding the meaning of each question. Pilot-testing should limit this concern, however. Selecting response options Survey items are typically closed-ended rather than open-ended, meaning that the respondents are asked to provide a specific answer such as a number (e.g., age) or date, or are asked to pick from a set of possible options. When an item has more than one option, it is called a categorical variable. One type of categorical variable is a dichotomous variable (aka binary) that has two options, usually “yes” or “no”. When respondents pick from mutually-exclusive categories that don’t have any particular order, we call this a nominal variable (e.g., cow, pig, sheep). If the options are ordered, it’s called an ordinal variable (e.g., never, rarely, sometimes, often). One type of ordinal variable is a Likert-type item in which response options take a discrete value along a continuum with qualitative anchors. For instance, you might ask a respondent if they agree or disagree with a statement on a 4-point Likert-type scale: Strongly agree (0) Agree (1) Disagree (2) Strongly disagree (3) Without a “neutral” middle option, this response set would be referred to as forced choice because the respondent has to decide to be closer to ‘agree’ or ‘disagree’. This is often an advantageous survey design decision because you avoid having participants clump around this neutral middle point. Participants must be free to refuse to answer questions, but advertising the neutral or ‘don’t know’ option on questions about attitudes or beliefs tends to increase it’s use because it’s always easier not to make a decision.41 It’s also possible to position the two extremes of a scale on either end of a horizontal line and ask respondents to draw an intersecting vertical line somewhere along the line. This is called a visual analogue scale. Web or tablet administration makes it possible to ask the respondent to move a virtual slider to the desired position between the two anchors. Such scales blur the lines between ordinal and continuous measurement. Translating the survey It’s essential to prepare a high quality translation when materials are presented to participants in a different language. There are various approaches to translation, but one that seems to work well is forward translation by a skilled translator and then blind back-translation by a second skilled translator. The key word here is blind; the back translation is worthless if the translator has access to the original version. This process makes it possible to look for problems by comparing the original version and the back-translated version. Whenever a potential loss or change of meaning is detected, it’s necessary to review all three versions (original, translated, back-translated) and determine the cause. Once discrepancies are resolved, it is ideal to have translated instrument reviewed by language experts and subject matter experts. Avoid plans to translate ‘on-the-fly’. Know your participants and find the right translator. For instance, if you want to survey women in a rural area without much formal education, make sure that the translator does not introduce complex language and sentence structure that might be technically correct by utterly confusing for participants. Administering the survey There are two basic approaches to survey administration: Participants complete the survey on their own Trained enumerators read each question aloud and record answers Surveys conducted in low-income countries are often administered in person by trained enumerators who read each question aloud and record answers on paper42 or an electronic device (via computer-assisted personal interviewing, or CAPI). This is done primarily to ensure that illiterate people are not excluded. When survey need to ask about sensitive topics, researchers might employ audio or video computer-assisted self-interviewing tools (CASI) to allow the participant to complete the survey on their own. Format Administration Mode Data Capture Enumerator Present Label Paper Participant Read Participant Maybe Paper Enumerator Read Enumerator Yes Paper Enumerator Read Participant Yes Secret ballot Electronic Participant Read Participant Yes Computer-assisted personal interviewing (CAPI) Electronic Enumerator Read Enumerator Yes Computer-assisted personal interviewing (CAPI) Electronic Participant Read Participant No Computer-assisted self interviewing (CASI) Electronic Participant Listen Participant No Audio computer-assisted self interviewing (ACASI) Technology makes some things easier other things harder. On the positive side, electronic administration eliminates the time, cost, and errors associated with manual data entry. The ability to incorporate survey logic also prevents skip pattern errors. Some downsides include hardware costs and maintenance woes. The combination of rough use in field surveys and rapid developments in software and hardware mean that you might find yourself in a regular cycle of evaluating new options for collecting data. See here for more thoughts about how to answer the paper vs. digital question (which is increasingly becoming a question of “how to do digital data collection”). Online surveys are not commonly used in global health, but as Internet access grows platforms like Amazon’s Mechanical Turk are becoming more feasible for certain research questions. See here for a discussion of how researchers are using Mechanical Turk in mostly high-income settings to recruit study participants. Adapting instruments for new settings The BDI-II appeared to be an accurate measure of depression among HIV-positive adolescents in Malawi according to Kim et al. (2014). This is not always the case, however, when exporting scales developed and validated in one cultural context to another. Whenever you are considering using or adapting a survey instrument like the BDI-II for use in a new context, you should determine if the instrument is a valid measure of the construct in that new context. Translation alone does not make an instrument a valid tool for measuring a construct in a new socio-cultural setting. This advice from Kohrt et al. (2011) just about says it all: …instruments developed and validated with children in high income countries with Western cultural settings cannot simply be translated with the expectation they will have the same psychometric properties in other cultural contexts. Cutoff scores established with Western child populations are not necessarily comparable in other settings and may lead to misclassification and distortion of prevalence rates. Moreover, the instruments may not capture the constructs they are intended to measure in other cultural contexts where the meaning, clustering, and experience of symptoms often differs. You should be skeptical when you read that an instrument has been validated in a particular setting. Kohrt et al. (2011) give us six questions to appraise cross-cultural validity of instruments (with an emphasis on global mental health): Question Details 1. What is the purpose of the instrument? Just as an instrument developed in the U.S. might not be a valid measure of a construct in Japan, an instrument developed to assess prevalence of this construct might not be a valid measure of response to treatment. The validity of instruments should be evaluated based on purpose and context. As Kohrt et al. note: ‘Validity is not an inherent property of an instrument.’ 2. What is the construct to be measured? Construct validity refers to the extent to which an instrument measures the construct it is intended to measure. Construct validity is traditionally described as consisting of two parts: convergent and discriminant validity. Establishing construct validity in cross-cultural settings is more complex, however. In global mental health, we might examine three types of constructs through qualitative and ethnographic inquiry: local constructs (aka, idioms of distress or culture-bound syndromes), Western psychiatric constructs, and cross-cultural constructs. 3. What are the contents of the construct? Content validity is more specific and says that the components of an instrument—e.g., each question in a questionnaire—is relevant to the measurement of the larger construct. Content validity also accounts for missing dimensions of a construct. Is the instrument comprehensive? 4. What are the idioms used to identify psychological symptoms and behaviors? Language matters. The words we use to describe behaviors and inner states—idioms—can take on different meanings in different contexts. Semantic equivalence indicates that ‘the meaning of each item is the same in each culture after translation into the language and idiom (written or oral) of each culture’. 5. How should questions and responses be structured? As discussed above, it is hard to write good questions and structure response options that promote accurate measurement. Technical equivalence is demonstrated when, ‘the method of assessment… is comparable in each culture with respect to the data that it yields’. 6. What does a score on the instrument mean? When we talk about validating measures, we often mean establishing criterion validity: evidence that the instrument is an accurate measure of some outcome. Two subtypes of criterion validity are concurrent validity and predictive validity. Predictive validity assesses whether an instrument used today predicts some outcome measured in the future (e.g., standardized test scores and future performance in graduate school). Concurrent validity takes a present focus: Is the instrument a proxy for some gold standard, like a diagnosis of major depressive disorder by a mental health professional. This particular example might also be referred to as diagnostic validity. Of course it’s often impossible—or at least highly impractical—to establish a gold standard criterion in low-income settings where there are relatively few trained professionals, so researchers often try to establish that a new measure is correlated with existing validated measure, but there is a chicken and egg problem here. The bottom line is that it’s challenging to validate measures across cultures and typically involves a lot of detailed work. Essential work. As Kohrt et al. note: ‘The misapplication of instruments that have not undergone diagnostic validation to make prevalence claims is one of the most common errors in global mental health research.’ “So what if the instrument I want to use has not been validated?” It might be fine to use. Or it might not. The problem is that it can be impossible to tell the difference. If you don’t make an effort to prospectively answer these six questions and instead choose to just translate the instrument and move forward—or someone gives you a dataset to analyze from a completed study—then you might be out of luck. Sorry. Please keep this in mind: reliability and validity are established in particular samples. If your sample is representative of the population, reliability and validity should hold across new samples from this population. If you move to a new population, however, this becomes a very strong assumption. 8.1.2 NON-SURVEY INSTRUMENTS Surveys are widely used in global health, but researchers are constantly on the lookout for creative new low-cost and easy to use instruments that will obtain more valid measurements. Here are some examples that you will find in global health.43 Direct observation One alternative to participant recall and self-report is to observe behavior directly. Direct observation is often described as a qualitative method, but there are several approaches where the goal is primarily to quantify behavior. Random spot checks Random spot checks can be a good technique to use when participants might want to hide a certain behavior, such as not showing up for work. For instance, Chaudhury et al. (2006) studied teacher absenteeism in India by having study enumerators show up at schools unannounced to check whether the teacher was present, whether students were in class, and whether any classroom instruction was taking place. Mystery clients and incognito enumerators Mystery clients mask their identity and purpose to have an authentic experience and data collection opportunity. Kaur et al. (2015) used mystery clients in Nigeria to update a sampling frame of public and private facilities offering artemisinin-based combination therapies for the treatment of malaria. Mchome et al. (2015) took a more qualitative approach, training a dozen Tanzanian youth to visit health facilities and evaluate “youth-friendly” reproductive health services by requesting condoms and information on sexually transmitted infections and family planning. An incognito enumerator does something similar but does not participate as a client. Glennerster and Takavarasha (2013) give the example of going on a ‘ride along’ with drivers to count the number and amount of bribes they are compelled to pay along their route. Physical/environmental tests Physical tests are objective measures of materials and environmental substances. For instance, Rosa et al. (2014) conducted a trial of a combined intervention that involved distributing free water filters and improved cookstoves to hundreds of households in Rwanda. In addition to measuring use of these devices through self-report and spot-checks, the research team assessed (i) levels of fecal contamination (presence of thermotolerant coliforms in drinking water samples) and (ii) average 24-h concentrations of particulate matter in the main cooking area. Biological samples This is a large category of assessment that includes diagnostic tests and the analysis of specimens such as hair, saliva, toenail clippings, urine, and tissue biopsies, to name a few. These tests can determine the presence of disease (or markers of disease), predictors of disease, and consequences of disease (Jacobsen 2016). One of many examples comes from Obala et al. (2015) who used a rapid diagnostic test for malaria to confirm infection and caseness in a case-control study in western Kenya. Anthropometric measures Anthropometric measures quantify characteristics of the human body, such as height, weight, and mid-upper-arm circumference. Research on obesity and malnutrition features anthropometric measurement prominently, but you’ll find use cases in every area of global health. Use care to follow standard data collection techniques and methods of indicator construction (Cogill 2003,Blossner and Onis (2005)). Vital signs Vital signs include physiological measurements such as body temperature, blood pressure, pulse, and respiratory rate. Such measurements are typically easy to obtain. Some et al. (2016) demonstrated in Kenya that management of non-communicable diseases such as hypertension, diabetes mellitus type 2, epilepsy, asthma, and sickle cell, could be effectively shifted from clinical officers (roughly nurse practitioners in the U.S. healthcare system) to nurses to improve overall access to care by reducing the workload of more skilled providers. Clinical examination Medical personnel can also be trained to collect reliable data through a clinical examination. A clinician might listen to a patient’s heart, breath, and bowel sounds, or she might examine the patient’s eyes, ears, and hair. When used in a research context, examination procedures and data recording should be standardized, and data collectors should be trained until reliable. One example comes from Liu et al. (2016) who used the Structured Clinical Interview for Diagnostic and Statistical Manual for Mental Disorders (SCID) to assess depression among a sample of elderly participants in Hunan Province, China. The SCID is structured in the sense that the examination has detailed steps and includes an algorithm for making a clinical determination about depression. This standardization makes it easy to train data collection personnel and reduce measurement error. Tests of physiological function Some studies use tests that measure physiological function, such as electrocardiography (ECG) to measure heart function and electroencephalography (EEG) to measure brain function. Lelijveld et al. (2016) measured lung function with spirometry in a cohort of Malawian children treated for severe acute malnutrition. Medical imaging Medical imaging techniques create visual representations of internal body structures. Examples include radiography (X-rays), computed tomography (CT) scans, magnetic resonance imaging (MRI), and ultrasound (Jacobsen 2016). For instance, Rijken et al. (2012) studied the effects of malaria infections early in pregnancy on fetal growth by using ultrasound to measure fetal biparietal diameter and comparing outcomes among infected and uninfected women. Tracking devices This category of instruments is growing rapidly. From radio-frequency identification (RFID) to track attendance to wearable digital health technology to monitor activity, researchers have a large selection of consumer and professional tracking devices for measuring individual behavior. Vanhems et al (2013) used wearable proximity sensors (RFID) to measure contacts (frequency and duration) among patients and healthcare workers in a geriatric unit of a hospital in France. GIS and remote sensing In recent years, researchers in global health have found many applications for geographic information systems (GIS), global positioning systems (GPS), and remote sensing (e.g., satellite imagery). A particularly active area of research has been the spatial epidemiology of malaria. For instance, Sewe et al. (2016) used satellite imagery to measure the Normalized Difference Vegetation Index, day Land Surface Temperature, and precipitation, and examined the relationship between these environmental variables and malaria mortality. Standardized tests As a measure of knowledge, standardized tests are useful instruments to estimate the immediate impact of a program or intervention designed to teach people new ideas or skills. For example, HIV prevention programs often include some measure of HIV knowledge (Hughes and Admiraal 2011). As many studies have demonstrated, however, increasing knowledge does not always translate to changing behavior. Vignettes Another method of assessing knowledge (and attitudes and skills) is to present participants with hypothetical scenarios called vignettes. Corneli et al. (2015) used vignettes to study women’s differential likelihood of engaging in risky sexual behavior if taking pre-exposure prophylaxis (PrEP) for HIV. Discrete choice experiments (DCE) are a particular type of vignette commonly used in healthcare policy and economics. In a DCE, participants are presented with the same basic vignette that combines variations in attributes to elicit the factors that have the greatest influence on preferences [mangham:2009]. For instance, Michaels-Igbokwe et al. (2015) designed a DCE with variations on six attributes to measure youth’s preferences for family planning service providers in Malawi. The six attributes included distance to the provider, hours of operation, waiting time, provider attitude, stock, and price. Given the large number of combinations possible, the authors used a fractional factorial design to present participants with a limited choice set. Provider attitude and stock appeared to be important drivers of uptake. Behavioral games A related idea is to design “games” in which participants are asked to make decisions with real money to measure constructs like time preferences, trust, risk aversion, and prosocial behavior. Blattman et al. (2016) used games in a randomized trial of a microenterprise development program in Uganda to measure how future orientation moderated program impacts. List randomization It is challenging to obtain accurate and honest answers to sensitive questions, such as questions about sexual behavior. One approach to promoting more honest responding is the unmatched count technique, aka list randomization. In this technique, researchers create two lists of questions or statements. The lists are identical, except that one list has an additional question to measure the sensitive issue. Participants are randomly assigned to get the shorter or longer list. Everyone is asked to indicate the total number of correct or truthful items rather than providing an answer to each item. The difference in the mean number of endorsed items per list can be interpreted as the proportion of participants who endorsed the sensitive issue. For instance, if one list has 5 items and the other list has 6 items, mean counts of 3.4 and 3.9, respectively, would indicate that 50 percent of participants endorsed the sensitive item. Karlan and Zinman (2012) used this technique in Peru and the Philippines to measure the proportion of loan recipients who used loan proceeds for non-enterprise purchases. A limitation of this approach is that estimates are only available at the level of randomization. Purchasing decisions Another alternative to self-report is measuring purchasing decisions. For instance, Dupas (2014) returned to a sample of Kenyan households who were randomly assigned to receive a new type of bednet at prices that ranged from $0 to $3.80. When the research team returned a year later, all households were offered an opportunity to purchase another bednet at the subsidized price of $2.30. They examined how partial or full subsidies in the first phase affected a household’s willingness to purchase a bednet in the second phase. By measuring purchasing decisions, the researchers were able to avoid relying on household self-report about whether they would purchase bednets. Social networks Social relationships influence the spread of disease and offer opportunities for health interventions. Researchers measure these relationships as social networks. Kelly et al. (2014) review examples of how to collect and analyze social network data in low-income settings. 8.2 Qualitative Methods Whereas a good survey requires uniformity and structure, a good interview is flexible and probing. This flexibility is what makes qualitative methods well-suited for exploratory and descriptive research. Flexibility to examine questions of how and why. The three most common qualitative methods are in-depth interviews, focus groups, and participant observation (Mack et al. 2005). Dr. Leslie Curry at the Yale School of Public Health produced six video modules on qualitative research that readers might find useful. You can watch all six videos here. 8.2.1 IN-DEPTH INTERVIEWS An in-depth interview is a method of eliciting a person’s views and stories. The interviewee is the expert, and it’s the interviewer’s role to learn from this expertise. Typically interviews are conducted as a private conversation between two people. Some interviews are highly structured with a pre-determined set of questions and follow-up probes. Other interviews are designed to explore a general research topic and do not follow a specific format. All interviews need to be managed, however. The interviewer must guide the participant enough to find the desired narrative within the time constraints. Data generated by interviews can include audio and/or video recordings that become transcripts, interviewer field notes, and analysis memos. Data quality typically hinges on the skills of the interviewer. To obtain rich, thick description of events and perspectives, the interviewer must keep the participant talking. This takes a substantial amount of practice to do well. Mack et al. (Mack et al. 2005) point to three key skills that interviewers should master: Rapport-building: An interviewer must quickly make the participant feel at ease and free to talk openly and honestly. Emphasizing the participant’s perspective: This requires an interviewer to mask their own perspectives and treat the participant as the expert. People talk when they believe that the person across from them is truly listening and engaging in what they are saying. Adapting to different personalities and emotional states: Every interview is different. The interviewer must be able to adapt his or her interview style to match the needs and style of the participant. Interviewers must also learn to ask clear, open-ended questions and probe responses effectively. Common interviewing mistakes include asking multiple questions at once and asking closed-ended questions that signal the need for a yes/no or short answer. New interviewers also make the mistake of moving on to the next question too quickly, or failing to listen to the participant’s response because they are thinking of the next question to ask. There are two main strategies for eliciting more information from participants: asking follow-up questions and probing. Sometimes interview guides will specify sub-questions that should be explored if a participant’s initial response does not address all of the important issues. Other times follow-up questions are ad hoc and intended to clarify a participant’s statement or pursue an interesting idea. Specifying follow-up questions in advance can be a useful technique, but it can also create a tendency for new interviewers to move into “survey mode” where the goal is to get some answer for every question, rather than to explore the topic in a more open-ended fashion. Often, the better approach to keep people talking is to use probes. Probes can be direct questions or indirect expressions or visual cues for a participant to say more. Effective direct probes include: Can you say more about that? I’m not sure I understand. Can you explain? Can you give me an example? Why do you think…? Examples of indirect probes include: Verbal expressions that indicate you are an active listener, such as “uh huh”, “ok”, and “I see”. Restating the participant’s perspective, e.g., “So you believe that bednets are really only useful during the rainy season”. Reflecting the participant’s feelings, e.g., “And that made you feel disrespected”. Non-verbal gestures to signal that you are listening, such as nodding. Combining direct and indirect probes with clarifying questions is typically a winning combination, but it takes a substantial amount of practice to do well. And it’s only one interviewing skill to master. The other main skill is “managing” the interview. This means watching the clock and deciding how to balance depth and breadth of responding, redirecting participants when the conversation gets too far afield, and taking good notes. Doing this while sustaining rapport, monitoring a participant’s experience, and maintaining an awareness of what ground the interview has and has not covered is incredibly difficult. It is relatively easy to teach someone to be a good survey enumerator—to ask specific questions and record discrete answers. It is much harder to teach someone to be a good interviewer. Whereas you might be able to run a survey training over the course of a few days or a week, training qualitative interviewers could easily take a month or more to allow enough cycles of practice and feedback. 8.2.2 FOCUS GROUPS While interviews are a good method for understanding individual perspectives, focus groups are a better choice if the goal is to quickly gain insight into group norms and the range in group opinions. Focus groups can explore individual experiences to learn about these issues, but private stories are best left for individual interviews where confidentiality and privacy are under the interviewer’s control. It’s often most effective for two people to tag team a focus group with one playing the role of moderator and the other taking notes and preparing materials. The moderator has a difficult role to play. Just as an interviewer has to manage the interview process, the moderator has to manage the focus group. This sometimes means redirecting the conversation away from certain topics and dominant participants to achieve certain goals within a set time period. This job becomes harder as groups grow in size as it can be a challenge to involve everyone and make sense of too many voices. There is no cap on how many participants can join a focus group, but more than a dozen would likely be too hard to manage. It can be helpful to begin a session by laying some ground rules for the discussion that encourage respect for all participants. This gives the moderator something to refer back to when someone in the group is having a negative impact on the discussion. Once the rules are established, it’s the moderator’s responsibility to ensure that these rules are followed. Focus group discussions are usually most interesting when the moderator is able to encourage productive crosstalk between participants. Typically discussions begin as a series of 2-way exchanges between the moderator and a participant. To break out of this pattern and encourage participants to respond to each other, the moderator can follow a participant’s contribution with a question like “Who takes a different view?” or “What do you think of this idea?” and use non-verbal cues to signal that other people should weigh in. If you have permission to capture an audio recording of the session, it is helpful to identify participants when they speak. For instance, the notetaker might create a basic matrix of participant demographics (e.g., age, gender, education) and give each participant an identification number. By placing cards with ID numbers in front of each participant during the discussion, the moderator is able to say something like “Go ahead #3” to link voices to demographics in the audio recording. This pattern can be cumbersome to maintain, so a notetaker should also capture ID numbers along with viewpoints. Focus group activities do not need to be limited to discussion of questions posed by the moderator. Sometimes it can be helpful to design short exercises to stimulate conversation. For instance, free listing is a technique where participants brainstorm on a particular issue and the notetaker records ideas in rapid succession. The moderator might ask the group to think of what “depression” looks like among pregnant women and new mothers in that particular community. A potential follow-up activity is card sorting in which the group ranks the free listing ideas and/or sorts them into conceptual piles. As each activity unfolds, the moderator finds opportunities to probe and engage participants in a discussion. 8.2.3 PARTICIPANT OBSERVATION Another approach is participant observation—making observations while participating to some extent in community life. This is not the same as some of the ‘observation-for-quantification’ approaches we discussed earlier, such as random spot checks. Participant observation typically happens over a much longer time—weeks, months, or years—and is designed to result in ‘thick description’ of context, attitudes, and behaviors. Sometimes participant observation is used as a formative research step to inform the development of interview and focus group guides or to generate hypotheses. This method is also used after quantitative data collection to understand more about why an intervention might not have worked, or to triangulate quantitative findings. If your study aims are more ethnographic in nature, you might find yourself engaging in participant observation as a primary method of data collection. Mack et al. (2005) remind us that observers always remain “outsiders” to some extent, thus it is important to document observations without the filter of interpretation, which can often be wrong. The authors give the example of observing two men holding hands in a place like Uganda—where men will often hold hands as a sign of friendship—and making the incorrect conclusion that the men are homosexual. Interpretation and questioning of one’s observations is better documented in research memos (analysis) rather than field notes. If you are training team members to become participant observers, it is important to discuss strategies for how they will blend in with the community. This involves thinking about things like dress, mannerisms, and behavior. Attempting to participate in community life can lead to an authentic experience, but sometimes full participation is not wise. For instance, there are lines that should not be crossed when it comes to illegal behavior or sexual relationships with participants. Having regular supervision or mentoring meetings with study staff can help make sense of grey areas. It’s common to find references to “Grounded Theory” in qualitative work. Grounded theory is an iterative methodology for collecting and analyzing data. Data collection is iterative in the sense that the focus can shift over time as you reach saturation, the point at which you begin hearing the same themes over and over. Analysis involves coding data (e.g., text, photos, videos, observations, etc) based on emergent themes. Sampling procedures in grounded theory tend to follow gaps in the data, rather than a random process. As you collect and analyze data, you write research memos that themselves become sources of data. Hypotheses inductively emerge from this process of collection and analysis, and some grounded theorists attempt to close the loop by testing hypotheses with additional data collection and analysis. 8.3 Mixed Methods In mixed methods research, qualitative and quantitative methods are like peanut butter and jelly. Better together. Mixed methods guru John Creswell defines mixed methods research as: An approach in the social, behavioral, and health sciences in which the investigator gathers both quantitative (closed-ended) and qualitative (open-ended) data, integrates the two, and then draws interpretations based on the combined strengths of both sets of data to understand research problems. In this formulation, mixing is the key characteristic of mixed methods research. A study that uses qualitative and quantitative methods to collect data but never brings the data together is probably not a mixed methods study. A mixed methods research design is often a good choice when: qualitative or quantitative methods alone are insufficient to fully answer the research question you need to develop quantitative tools through exploratory research you need to triangulate results you need to better understand results Creswell outlines three basic mixed methods designs: convergent, explanatory sequential, exploratory sequential. Label Details Convergent qualitative and quantitative data are collected separately and then merged in the analysis phase so that the results reflect joint interpretation Explanatory sequential begin with quantitative work and then later turn to qualitative inquiry to help explain the quantitative results Exploratory sequential begin with qualitative work to explore an issue, create new instruments, or develop new interventions, and then later apply this learning in a quantitative study of the issue A nice example comes from Bass et al. (2008), a mixed methods study of post-partum depression in the Democratic Republic of Congo. The authors conducted a round of qualitative research to develop a pool of screening items and then conducted a quantitative validation study to evaluate the criterion validity of the new instrument. Share Feedback This book is a work in progress. You’d be doing me a big favor by taking a moment to tell me what you think about this chapter. References "],
["glossary.html", "9 Glossary", " 9 Glossary Term Definition applied research research focused on specific problems or applications basic research pursuit of fundamental knowledge of phenomena; forms the basis of clinical research behavioral research Research that focuses on the most effective ways to change people’s behaviors in their daily life causal inference determining whether or not there is a relationship where one variable causes another; seeks to establish that X causes Y clinical research a broad field that encompasses patient-oriented research, epidemiological and behavioral studies, and outcomes research and health services research correlational research A type of descriptive research that asks questions about the relationship (a.k.a association) between two or more variables; builds upon descriptive insights by attempting to predict or explain the behavior or phenomenon correlational research cross-sectional deductive reasoning reasoning that makes conclusions about some unobserved or unmeasured phenomenon based on direct observations of the world demographic research Research that seeks to understand more about population size, structure and change (e.g., birth, death migration, marriage, employment, education) dependent variable Also called the response variable or descriptive inference goes beyond basic description (or collection of facts) to say something about how indiviudal experiences and opinions illuminate something more universal about the research problem at hand descriptive research Research that seeks to answer the question “What is going on?” empirical evidence the systematic observations that are used in the process of empiricism empiricism to use what observations to make conclusions about what cannot or will not be observed directly; at the heart of scientific research experimental design a design where researchers manipulate some independent variable and examine changes to some dependent variable that result (considered the “gold standard” of designs) explanatory research Research that seeks to answer the question “Why is it going on?” implementation failure The failure of a program to be effective because the program was not executed properly implementation science Studies that assess to how to best get efficacious treatments to the people who need it most inductive reasoning reasoning from specific observations to the generation of hypotheses and theories inference the process of making conclusions about some unobserved or unmeasured phenomenon based on our direct observations of the world longitudinal mixed methods designs that utilize both inductive and deductive reasoning needs assessments panel peer review an important compnent of the scientific process where a scientist’s peers evaluate a work before it is published in a journal Phase I In clinical research, a trial that utilizes a small sample size in order to find a safe dosing range and look for side effects. Phase II In clinical research, studies that demonstrate the efficacy of the medication against several endpoints (a.k.a. outcomes). Includes Phase IIa and Phase IIb. Phase III In clinical research, typically a large trial to show that a treatment is efficacious. Phase IV In clinical research, trials that evaluate a medication’s long-term effects. preclinical research The clinical research phase in which testing is performed in non-human subjects with the goal of collecting data on how well the medication works (efficacy), how much damage it can do to an organism (toxicity), and how it is affected by the body (pharmacokinetics). prevalence Prevalence is the number of existing cases out of the total population at a point in time. In contrast to incidence, which is a measure of disease occurrence, prevalence is a measure of existing disease. Prevalence is unit-less. program evaluation program monitoring Concerned with the implementation of programs, policies or interventions. Necessary for good evaluations. quasi-experimental design replication refers to the ability of another research group to be able to follow the methods of a research study and replicate the results; relatively rare representative reproducibility the ability to generate a study’s findings given the orginal dataset and sometimes the original analysis code research problem A gap in the academic world’s knowledge research question sample In statistics, a sample refers to a set of observations drawn from a population. scientific research Resarch where 1) the goal is inference, 2) the procedures are public, and 3) the conclusions are uncertain stakeholders Refers to a wide range of people and organizations that have some sort of vested interest in the outcome of a program’s implementation theory failure The failure of a program to be effective because the idea or theory behind the program was incorrect translational research Studies that focus on getting interventions from “bench to bedside” "],
["references.html", "10 References", " 10 References "]
]
