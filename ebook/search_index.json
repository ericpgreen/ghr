[
["index.html", "Global Health Research: Design and Methods Preface About this Book Organization Icons Acknowledgements Colophon", " Global Health Research: Design and Methods Eric P. Green 2016-05-23   Preface Does the world really need another book about research methods? I think so. But I spent a fair amount of time writing down the ideas in this book, so I’m biased. But here’s my rationale. I went to graduate school for clinical psychology, and my classmates and I read all of the classic psychology texts on research design and methods. Books like “Experimental and Quasi-Experimental Designs for Generalized Causal Inference” by Shadish, Cook, and Campbell (2003). I still remember staying up late trying to memorize all of the different threats to internal validity outlined by Donald Campbell and colleagues. Meanwhile, across campus, my econ colleagues were reading the ideas of another Donald—Donald Rubin and what is now known as Rubin’s causal model. But I didn’t know this at the time. When I set off for Uganda in 2007, determined to learn more about this field called global health, I met some of these mini Donald Rubin’s in the wild. I tried communicating with them, but they had a strange dialect that I couldn’t quite understand. And they did not understand me and my Campbellian drawl. We were usually trying to say the same thing, just in the language of our peoples. But I couldn’t put all of the blame on the economists and the disciplinary gap between us. There was a lot I didn’t know that went far beyond differences in jargon. I was a psychologist trained in clinical research, and nearly every applied example I read about came from the U.S. or Europe. The young field of global mental health was still an infant when I was in school. The first Lancet series on global mental health that really put the field on the map was published in September 2007 as I was getting on a plane to fly back home. I really knew nothing about global health. Fortunately, students entering university today have many more opportunities to learn about global health through interdisciplinary studies. Duke University launched the first liberal arts global health major in the U.S. in 2013, and other universities have followed suit. The Duke program is unique because it requires global health students to co-major in another discipline, such as biology, economics, psychology, or public policy. I started teaching at Duke around the time the new co-major started, and I found myself in the position of needing to pick a textbook for a course called “Research Methods in Global Health”. I reviewed a lot of excellent books that covered the basics, but none integrated examples from the very diverse and interdisciplinary field that is global health. I saw this as a real limitation. So I decided to write my own book.  About this Book A guiding principle of this book is that a student of global health needs to be a student of medicine, biology, statistics, economics, psychology, public policy, and the list goes on. Just take a topic like malaria. A literature search will return articles about the spread of the disease (epidemiology), the impact of illness on future productivity (economics), the merits of free or subsidized bed nets (public policy), mosquito habitats (ecology), the efficacy of vaccines to prevent the disease (medicine and statistics), rapid diagnostic tests (biomedical engineering), the adoption and use of bed nets (psychology), and the list goes on. No one book or author could ever hope to provide full disciplinary coverage of even one topic like malaria, so my goal was much more modest. I wanted to create a resource that would teach the basics of research design and methods by exposing readers to real world global health examples from different disciplines. Another guiding principle is openness. Whenever possible, the examples come from open access sources. Every reader should be able to access 90% of the references in this book.   Organization The book is organized as follows. One objective of my course on global health research—and thus this book—is to make students better consumers of research. I wrote the first three chapters with this end in mind. Chapter 1 reviews the fundamentals of scientific research, but with a global health spin. In Chapter 2, you’ll learn how to search the literature for existing evidence. Better yet, you’ll learn to let someone else do that for you in the form of a systematic review or meta-analysis. You’ll also learn how to begin asking your own research questions. Chapter 3 on critical appraisal will teach you how to read and evaluate scientific evidence. One of the most important claims you’ll need to assess as a consumer of research is causality. Chapter 4 examines strategies for building up causal arguments. A second objective of my course is to give students the tools they need to begin careers as producers of research. The next few chapters lay this foundation. Chapter 5 gives you practical advice on developing a theory of change to guide program development, monitoring, and evaluation. It also helps to organize your approach to measurement, the topic of Chapter 6. This chapter explains how to define the measurement of your key study outcomes and covers fundamental psychometric concepts such as reliability and validity. In Chapter 7, you’ll learn strategies for determining how many research subjects you should recruit and methods for selecting them. The next two chapters introduce methods for collecting data. Chapter 8 covers quantitative methods, and Chapter 9 covers qualitative and approaches to mixing qualitative and quantitative methods. With the basics out of the way, we turn to designs that you can use to answer your research questions. Chapters 10 and 11 cover non-experimental designs. Chapter 10 focuses on the observational designs you typically find in epidemiology (e.g., cohort, case-control) and psychology (e.g., correlational). Chapter 11 introduces several “quasi-experimental” designs that manipulate a cause that comes before an effect, but without the benefit of randomization, the topic of Chapter 12. All of these chapters will refer back to the foundation in cause and effect we set in Chapter 4. The book concludes with several chapters to help you use your new knowledge to make an impact. In Chapter 13 you will learn how to prepare a study protocol. Chapter 14 will introduce you to the publication process and other opportunities for disseminating your work, such as professional conferences. Chapter 15 pushes you to think beyond your study results to make an impact on policy and practice. One limitation of this book is that it does not teach statistics. Statistical concepts are discussed throughout, but not in great detail. Every statistician will tell you that you need to think through your analysis at the study design stage. Listen to this advice. While this book will not give you the technical tools you need to plan your analysis, I hope you will come away with more appreciation for the gaps in your knowledge that you need to fill with further study. A great resource for learning applied statistics is OpenIntro Stats. If you are fortunate to be at a university with an applied statistics department, chances are you could get excellent consultation on your study protocol. This book WILL prepare you to narrow your options and have a smart conversation about how to meet your study objectives.   Icons I’ve sprinkled several types of asides throughout the book. If you are a student enrolled in my course, I recommend that you actually read them. Everyone else can flip the page and feel productive.   Help piecing together the global health puzzle     Extended discussion of a special topic     Tips     Videos     Review questions     Application exercises     Acknowledgements I’d like to thank some folks for their helpful feedback at various points throughout my writing process. My graduate student teaching assistants, Kaitlin Saxton, Kathleen Perry, and Jenae Logan, read and commented on the initial drafts. This could not have been fun, so thanks! Thanks to Duke librarians Megan Von Isenburg and Hannah Rozear for setting me straight on literature searches. I still have a lot to learn! Liz Turner, biostatistican extraordinaire, kept me from making too many mistakes on technical details here and there. I’d also like to thank students in my undergraduate and graduate global health research courses for test driving the book before all the parts were in place. Your feedback was [placeholder], and the book would have been [placeholder] without you. Special shoutout to the following students for sharing written feedback: Kelsey Sumner, Karly Gregory, Qian Yudong, and Christina Schmidt. Despite everyone’s best efforts to help me catch mistakes, I’m certain errors remain in the book. My bad.   Colophon This book is a work in progress. If you find errors (gasp!), please create an issue on Github, email me, or shame me on Twitter (@ericpgreen). I’m writing the book in R Markdown within RStudio. The `bookdown’ package from the makers of RStudio does most of the heavy lifting to compile the book. The source code for the book is available on Github.   "],
["science.html", "1 Science Fair Redux 1.1 Scientific Research 1.2 The Fundamentals", " 1 Science Fair Redux     Figure 1.1: Example of intended style of overview video.       Believe it or not, you already know the basics of the research process. You probably have a yellowing, tri-fold piece of cardboard tucked in the back of the closet in your parents’ house that would prove me right. Just like Jimmy.1    Figure 1.2: What you see here is the scientific method in action. Jimmy asked a question, made a hypothesis, collected and analyzed data, and ultimately, made some conclusions based on the results. Science!. Source: http://bit.ly/1HbluM4   Even if you have not been as productive as Jimmy, I’m certain that you’ve had years of practice consuming research. We’re exposed to popular press accounts of research every day on TV, the radio, and the internet. Much of it might be wrong—“new study proves that eating chocolate prevents all cancer”—but it’s a start. So it’s a safe bet that just about every reader has some foundation to build upon. The goal of this chapter, therefore, is to re-introduce familiar concepts about scientific research from a global health perspective. We’ll come back to these fundamentals throughout the book and explore them in more detail. By the end you’ll be ready to move your work from primary school cafeteria to academic conferences, policy debates, and real world program design and delivery.  1.1 Scientific Research Let’s start with what we mean by “scientific research”. King et al. (1994) offer a useful definition in their book, “Designing Social Inquiry”. They point to several main characteristics:  The goal is inference The procedures are public The conclusions are uncertain   1.1.1 All about inference By stating that the goal of scientific research is inference, we mean that science goes beyond the collection of facts. You’ve probably seen the word inference used in several different contexts in your global health studies. At the most basic level, when we talk about inference, we are referring to the process of making conclusions about some unobserved or unmeasured phenomenon based on our direct observations of the world. We use what we know to infer something about the things we don’t know. This process can be deductive or inductive. In deductive reasoning, we start from general theories, make hypotheses, collect data, and make conclusions based on the data. Inductive reasoning flows the other direction, from specific observations to the generation of hypotheses and theories. Remember it this way: if you are testing a specific hypothesis, you are using deductive reasoning. If you are starting with your observations and making more general statements, then you are using inductive reasoning. To say that quantitative research is deductive and qualitative research is inductive is not quite right, but it’s often true.2 For instance, Singla et al. (2015) report the results of a cluster randomized trial of a parenting intervention in rural Uganda. This study used quantitative methods; the primary outcomes of this study were cognitive and receptive language development of the children of participating caregivers measured with the Bayley Scales of Infant Development. The authors hypothesized that the intervention would improve child development. As you can see in the following table from the article, the program increased cognitive and receptive language scores, but did not have an effect on height-for-age, thus partially supporting the hypothesis.    Figure 1.3: Source: Singla et al. (2015), http://bit.ly/1UcVtoZ   Later in the book we’ll get into the nitty gritty details of how you read and interpret results like you see here. For now, let’s focus on the approach to reasoning. Singla et al. is an example of deductive reasoning. The authors started with a hypothesis, collected quantitative data (i.e., scores on a measure called the Bayley), and inferred something about the impact of the intervention.3 We can contrast the Singla et al. trial with a qualitative study by Sahoo et al. (2015) that exemplifies inductive reasoning. Sahoo and colleagues used a grounded theory approach to conduct and analyze interviews with 56 women in Odisha, India about their sources of stress and sanitation practices.4 This study is an example of inductive reasoning because the authors started with the data—their observations—looked for themes and patterns, and came to some conclusions about the nature of sanitation-related stress.5 One result of this work was a conceptual framework for thinking about sanitation-related psychosocial stress, as shown below.6    Figure 1.4: Source: Sahoo et al. (2015), http://bit.ly/1JB6nSs   The point to take away about inference is that, regardless of the approach to reasoning, the goal of scientific research is to use what we observe to make conclusions about what we do not observe directly. This is sometimes referred to as empiricism, and our systematic observations as empirical evidence. Empiricism is at the heart of scientific research.   1.1.2 Research as a public act Scientific research uses public methods that can be examined and replicated. Replication is a core principle of scientific research. No one study rules the day. If the results of your study are robust, another research group should be able to follow your methods and replicate the findings. When findings are replicated, we all have more confidence in the results. Replications are relatively rare, however. For one, there are often few resources for replicating studies, especially when it comes to big field experiments. Second, journal space is limited (especially if there is still a print version) and peer review takes a lot of resources. Journals want to use their space and resources to publish novel ideas (ironically, novelty can sometimes mean small effects with a lot of noise that might fail to replicate). Without the promise of a publication, researchers have little incentive to spend time and money trying to replicate published findings. Publications are a key criterion for tenure and promotion in academia, so many researchers don’t waste their efforts on studies that won’t get published. What happens when replications are attempted? Well, that’s a topic for a later chapter. The short answer is bitterness. Replicators grab more headlines when they “debunk” findings, and the original authors almost invariably call into question the quality of the replication. Just see #wormwars to learn what happened when a famous de-worming study was re-examined. Or Google social psychology and priming. Yikes! A separate but related issue is reproducibility, the ability to generate a study’s findings given the original dataset and sometimes the original analysis code. Think irreproducible findings are rare? Think again. The Quarterly Journal of Political Science found that slightly more than half of their published empirical papers subjected to review had results that could not be replicated with the author’s own code. The positive part of this story is that it’s becoming more common for authors to share their data and analysis code. This has been standard practice in economics for some time, but the idea is revolutionary in medicine and public health. We’ll explore why this is so important and easier than ever to do.   1.1.3 Living with uncertainty Every method has limitations, every measurement has error, and every model is wrong to some extent. In short, research is an imperfect process. Sometimes researchers make outright mistakes. These mistakes may or may not be detected and corrected in the peer review process, or during post-publication review if authors share their data and analysis code. Other findings are free of obvious mistakes, but fail to be replicated, and over time run counter to a growing body of literature that points in the other direction. In this way science is said to be self-correcting. We’ll discuss how this ideal can fall short in the face of challenges like publication bias, but the point here is to get comfortable in the short term with the idea of uncertainty. A good example of uncertainty comes from the estimation of maternal mortality. Hogan et al. (2010) published estimates for 181 countries. Some countries like the United States have vast amounts of data; vital registries that attempt to track all births and deaths. Countries with vital registries struggle with changing definitions over time, but the uncertainty interval around their estimates is typically tight, as shown in the figure below from the Hogan et al.’s supplementary webappendix, because there is a lot of good data.    Figure 1.5: Source: Hogan et al. (2010), http://bit.ly/1JBCelO   In many low-income countries the situation is very different. Here is the estimate and uncertainly interval for maternal mortality in Afghanistan. There are only four data points! No wonder the uncertainty interval is so great.7    Figure 1.6: Source: Hogan et al. (2010), http://bit.ly/1JBCelO   So how many women die during pregnancy or within 42 days of delivery? The same research group that published Hogan et al., the Institute for Health Metrics and Evaluation, estimated that there were 292,982 maternal deaths globally in 2013, with a 95% uncertainty interval ranging from 261,017 to 327,792; that’s a range of 66,775 for everyone who struggles with mental math (Kassebaum et al. 2014). This might seem like a lot, but remember that we’re talking about global statistics for a world population of more than 7 billion people.8 The takeaway message is that there is uncertainty in everything. Don’t take any single estimate as the “Truth”. Instead, try to learn about the origin of estimates and recognize the limitations of what we know.    1.2 The Fundamentals Before we get too far along, we need to establish a common understanding of some fundamental concepts and terms. We’ll do so in the context of research on malaria.9  1.2.1 The nature of research Research can be classified as basic or applied. Basic research—or “pure” research—is the pursuit of fundamental knowledge of phenomena. An example would be the bench science to understand the parasitic life cycle and how parasites interact with humans at different stages. Basic research can be contrasted with applied research which is focused on specific problems or applications. For instance, an applied research question is how to increase the coverage and use of bed nets that prevent malaria transmission. Applied science takes many different forms, including clinical research. Clinical research is a broad field that encompasses patient-oriented research, epidemiological and behavioral studies, and outcomes research and health services research.10 Basic research is the foundation of clinical research.  Clinical trials One type of clinical research is a clinical trial. Drugs and vaccines have to pass through different phases of clinical trials before regulatory bodies will approve their use with humans:  Preclinical research Phase I Phase II Phase III Phase IV  Let’s take the development of a vaccine for malaria as an example of the clinical trial life-cycle. A vaccine candidate called RTS,S, or Mosquirix™, recently made news for getting one step closer to becoming a licensed vaccine after a successful Phase III trial. This moment was more than 30 years in the making. Development of RTS,S began in 1984 through a partnership between the pharmaceutical company GSK and the Walter Reed Army Institute of Research. The vaccine candidate was created in 1987 and entered preclinical research. During the pre-clinical phase, testing is performed in non-human subjects with the goal of collecting data on how well the vaccine works (efficacy), how much damage it can do to an organism (toxicity), and how it is affected by the body (pharmacokinetics). Clinical research on humans began in 1992. To obtain regulatory approval, the vaccine had to complete three phases of testing. Doherty et al. (1999) conducted a Phase I safety and immunogenicity trial with 20 adults in The Gambia in 1997. This small sample size is typical of Phase I trials where the objective is usually to find a safe dosing range and look for side effects. The authors reported that the vaccine did not have any significant toxicity but did produce the expected antibodies. Several Phase II studies conducted over a decade (Phase IIa and Phase IIb) demonstrated efficacy of the vaccine against several endpoints (a.k.a. outcomes) (Moorthy and Ballou 2009). A Phase IIb trial began in Mozambique in 2003 with more than 2,000 children aged 1 to 4 (Alonso et al. 2004). Children were randomly assigned to receive three doses of RTS,S or a control vaccine. At 6-months, the prevalence of malaria was 37% lower in the treatment group compared to the control group. A follow-up study with 214 infants also showed partial protection (Aponte et al. 2007). This was an important proof-of-concept. Final results of a large Phase III trial with more than 15,000 infants and young children in seven African countries were published in The Lancet in 2015 (RTS,S Clinical Trials Partnership 2015). Children in the study were randomly assigned to 1 of 3 arms: 3 doses of RTS,S and a booster dose at month 20; 3 doses of RTS,S and a booster dose of a comparator vaccine at month 20; or 4 doses of a comparator vaccine. The study reported that RTS,S reduced clinical malaria cases by 28% and 18% among young children and infants, respectively, over a 3 to 4 year period. This is the goal of a Phase III trial—to show that a treatment is efficacious. On the basis of these results, the European Medicines Agency issued a “European scientific opinion”, which could help inform the decision of the WHO and African national regulatory authorities. If RTS,S is approved for use and eventually hits the market, researchers will likely conduct Phase IV trials to evaluate the vaccine’s long-term effects. This will not be the end for research on RTS,S, however. The vaccine may be efficacious, but that does not mean it will be easy or cost-effective to deliver at scale to millions. Studies that assess how to best get efficacious treatments to the people who need it most fall under the domain of implementation science. There are many stumbling blocks from getting interventions from “bench to bedside”, so to speak. Practitioners of translational research point to four key bottlenecks:  T1: translation from basic science to clinical research T2: translation from early clinical trials to Phase III trials and beyond with larger patient populations T3: translation from efficacy (Phase III) to real-world effectiveness—the domain of implementation science T4: translation from evidence about delivery at scale to new policy  Behavioral research (e.g., development and evaluation of parenting interventions) does not follow the same exact phases of vaccine and drug development, but the broad principles are the same.   Monitoring and evaluation Another arena of applied work in global health is monitoring and evaluation, or M&amp;E. Let’s start with the “E”, program evaluation. In the U.S., program evaluation became commonplace by the end of the 1950s and grew dramatically in the 1960s as the federal government expanded and introduced new social programs. Lawmakers wanted accountability, and the evaluation of social programs took off (Rossi, Lipsey, and Freeman 2003). But is program evaluation considered research? Methods giant Donald Campbell thought so (Campbell 1969):  The United States and other modern nations should be ready for an experimental approach to social reform, an approach in which we try out new programs designed to cure specific problems, in which we learn whether or not these programs are effective, and in which we retain, imitate, modify or discard them on the basis of their effectiveness on the multiple imperfect criteria available.  Campbell had an outsized impact on the field. It’s no surprise that an organization dedicated to synthesizing the best available evidence on social interventions, the Campbell Collaboration, bears his name.   A good candidate for Donald Campbell’s successor is French economist and MIT researcher Esther Duflo. Together with Abhijit Banerjee and Sendhil Mullainathan, she co-founded JPAL, which stands for the Abdul Latif Jameel Poverty Action Lab. JPAL is a global research organization headquartered at MIT that uses randomized evaluations (i.e., experiments) to answer policy questions related to poverty alleviation. The JPAL website, http://www.povertyactionlab.org, contains excellent resources about the methods of randomized evaluations, published studies, and policy briefs. Interested readers should also check out Innovations for Poverty Action, or IPA, a sister organization of sorts that is also a leader in the use of randomized evaluations to study important policy questions about global poverty.   Yet, not everyone agrees. Educational psychologist Lee Cronbach certainly did not.11 Cronbach recognized the overlap in methods and designs, but he thought that program evaluation was really designed for program implementers and funders, and that the messy nature of programs required a loosening of research standards (Cronbach 1982). Just learn what you can. In their introductory text on evaluation, Rossi et al. (2003) strike a balance in views. Their answer is perhaps a bit unsatisfying, but I’d argue true nevertheless. It depends. Program evaluations should be as rigorous as logistics, ethics, politics, and resources permit. And no less. Is there a lower bound in terms of quality that should limit what is even worth doing? Probably, but the line is so context dependent that it is not sensible to attempt a definition. If there is one rule to follow, I’d suggest that it’s this: “don’t go beyond the data”. Everyone wants to claim “impact”, but not every evaluation can based on the design and implementation. Now we turn to the “M”, program monitoring. Program monitoring is concerned with the implementation of programs, policies, or interventions. How are resources being used? Is the program being delivered as intended (a.k.a. with fidelity)? How many people participate, and does the program reach the intended targets? These are all program monitoring questions. Accurate monitoring is essential for reporting to funders, but it’s also essential for all good evaluations. The reason is simple. If a program is shown to not “work”—to have no impact—the next question is why? Did the program fail to have an impact because the idea or theory behind the program was wrong (theory failure)? Or was it the case that the implementation of the program was so troubled that there was never a chance of having an impact (implementation failure)? Every trial should include ongoing monitoring or a formal process evaluation.    1.2.2 Research problems and questions Every study begins with a research problem. A research problem represents a gap in our knowledge. In academic research, this is another way of saying a gap in “the literature”.   Usually when people speak of “the literature”, they mean scholarly or peer-reviewed journal articles. There is also something called “grey literature” that is more encompassing and harder to search systematically. Grey literature sources are typically disseminated through channels other than peer-reviewed journals. Examples could include technical reports or white papers published on the web.   Research problems are typically broad. For instance, stakeholders might want to know how to increase the use of bed nets for children under 5 years of age. Or whether all children should receive deworming medication prophylactically.   Stakeholders can refer to a wide range of people and organizations. Typically we mean donors (i.e., the public and private organizations that fund research and programs), policy makers (i.e., government officials and bureaucrats at international bodies like the WHO), program implementers (i.e., organizations like Doctors Without Borders that actually deliver services to beneficiaries, a.k.a. people), and scholars who study the topic or policy issue.   These problems have something in common: they are solvable. In his introductory text on behavioral research methods, Leary (2012) writes that this is another key criterion for scientific research. The problems must be solvable. This does not mean easy; it just means that we can use systematic, public methods to gather and analyze data on the problem. Think of it this way: we can come up with a method for studying how to get more parents to ensure that their kids sleep under a mosquito net every night, but we don’t yet have a scientific method for determining whether there is a mosquito afterlife where these pests get to buzz around for all of eternity. In order to study a broad research problem, we must narrow to a more specific research question. de Vaus (2001) says there are essentially two types of research questions:  Descriptive—what is going on? Explanatory—why is it going on?  Let’s stick with the bed net example. If we want to study uptake or use of bed nets, we might ask a descriptive research question like, “How many children sleep under bed nets?” But this is too general. Children of what age? Living where? We also need to operationalize what we mean by sleeping under a bed net. It’s common in this line of research to ask about the previous night, as in the night before the survey. As we will discuss in the chapter on measurement, we have to consider challenges to getting valid information, such as recall difficulties. A better way to phrase the question might be, “What percentage of children under 5 years of age in Kenya slept under an insecticide treated net the previous night?”   1.2.3 Research designs As Glennerster and Takavarasha (2013) explain in their excellent practical guide to running randomized evaluations, different research questions require different research designs. We’ll spend most of our time in this book looking at strategies for answering the “why” questions. I lump them into three categories and differentiate them from descriptive designs:  descriptive correlational quasi-experimental experimental   Descriptive research The goal of descriptive research is to characterize the population. Often this means estimating the prevalence of a phenomenon or disease. 20% are illiterate. 36% have an unmet need for contraception. 9% are HIV positive. Description can also be qualitative in nature (e.g., thick description). Just about every study will have some descriptive element. Some studies are exclusively descriptive. A good example are the Demographic and Health Surveys, more commonly referred to as DHS surveys (yes, “surveys” is redundant). Every student of global health should come to know what the DHS Program has to offer. The program is funded by the U.S. Agency for International Development (USAID), and registered users can request access to data from more than 300 surveys conducted in 90+ countries.   DHS surveys are a good example of demographic research. Demographers contribute to and use data sources like DHS surveys and national population and housing censuses to understand more about population size, structure, and change (e.g., birth, death, migration, marriage, employment, education).   Many countries strive to conduct a census, or an enumeration of all citizens, every 10 years. The United Nations Statistics Division and the United Nations Population Fund (UNFPA) provide technical support (a.k.a., help) to countries preparing for, conducting, and analyzing a national population and housing census. These two organizations, in partnership with the United Nations Children’s Fund (UNICEF), maintain CensusInfo, a database of global census data.   Here is the relevant table from the 2014 Kenya DHS Key Indicators Report for describing the prevalence of ITN use.12 This is a typical DHS cross tabulation (or crosstab) of the results. In this example, the percentage of children under the age of 5 that slept under an insecticide treated net the previous night in Kenya was 54.1%. This descriptive data is further disaggregated by residence and wealth quintile as is typical for DHS tables.13    Figure 1.7: Source: Kenya 2014 DHS Key Indicators Report, http://bit.ly/1g4NYS5   The data summarized in this table describes the problem of bed net use. Descriptive questions are well-suited for needs assessments. Before we can design a program or policy to increase bed net usage, for instance, we must to understand the need. In Kenya, almost half of children under 5 are not sleeping under insecticide treated nets according to the DHS. This is a particular concern for children living in areas of high risk.   This DHS report is an example of a cross-sectional study. These are typically one-off surveys but can include other forms of data collection. The key is that it’s a snapshot. The goal is often description but might also include correlation. Cross-sectional studies are differentiated from panel or longitudinal studies by their participants; the latter include the same research participants (sample) over time in multiple studies, whereas cross-sectional studies only include a particular sample once. So even though the DHS Program will conduct a new survey in a country every five years or so, they always recruit a new sample of participants (a.k.a. “successive independent samples”). This makes the DHS surveys cross-sectional rather than panel or longitudinal in design.     Correlational research This descriptive information sheds light on programmatic and policy priorities, but we have to go beyond describing the problem to make a difference. A helpful next step is often to build on descriptive insights by attempting to predict or explain the behavior or phenomenon. For instance, Noor et al. (2006) asked a correlational research question (edited below) about the factors associated with net use among children under the age of 5:  Are wealth, mother’s education, and physical access to markets associated with the use of nets purchased from the retail sector among rural children under five years of age in four districts in Kenya?  Correlational research asks questions about the relationship (a.k.a. association) between two or more variables. In this case, ITN use and a variety of potentially influential factors, such as household wealth and a mother’s education level.   You’ll encounter different labels for variables, and it can be confusing to keep straight at first. The behavior Noor et al. are trying to predict, bed net use, is the dependent variable, often referred to as the outcome, response variable, or simply Y. The factors thought to be related to ITN use are the independent variables. Some disciplines will call them predictor or explanatory variables. Sometimes they are controls, exposures, or simply X. This will become more clear as we go.   Noor and colleagues reported that only 15% of children in the rural study sample slept under a net the previous night—a much lower percentage than the national prevalence reported by recent DHS surveys. As shown in the table below, they also found that several factors were associated with higher odds of bed net use, including: greater household wealth, living closer to a market center, not having older children present in the household, having a mother who is married and not pregnant, being younger than 1 year old, and having an immunization card.    Figure 1.8: Source: Noor et al. (2006), http://bit.ly/1HoltVo     We’ll review in detail how to read tables like this in later chapters, but it might be helpful to preview some concepts here. The results in this table come from a multivariable logistic regression. The authors describe the model they fit in the article, but we won’t worry ourselves with these details. Instead, let’s highlight a few key points.   The table starts with household-level predictors of net use, specifically household wealth quintile. The first two columns show us the number and percentages of households that have or do not have retail sector nets, disaggregated by wealth quintile. So of all the households without retail sector nets, 690 or 21.6% were classified as being the most poor. The distribution looks a bit different for households with nets; 40.1% of households with retail sector nets were classified as being the least poor. This makes intuitive sense: if you are poor, you are less likely to purchase a net from the market.   The next three columns give us the results of the regression. Since wealth quintiles have different categories, the authors set one category—the most poor—to be the reference category. So the results will be relative to the poorest households.   As you can see, the odds of using a net are 10.17 higher among the “least poor” compared to the “most poor”. This does not tell us why wealthier households are more likely to use nets for their young kids, but we know that there is some relationship here.   Remember, however, that 10.17 is just what we call a point estimate for the odds ratio. The 95% confidence interval ranges from 5.45 to 18.98. We’ll talk more about the specifics as we go, but it’s important to get in the habit of evaluating the uncertainty of every estimate. In this case, it’s pretty clear that having more money is associated with better preventive behaviors.   You will often see descriptive and correlational studies like the DHS and Noor et al. classified as non-experimental or observational studies. Other observational designs include cohort and case-control studies (the topic of a later chapter). Researchers use these designs to determine whether there is an association between some exposure and a disease.   Observational studies are the bread and butter of epidemiology. Epidemiologists often conduct cross-sectional studies to estimate the prevalence and incidence of different disorders as well as correlational research to understand risk and protective factors.    Prospective cohort In a prospective cohort study, healthy participants are recruited and followed into the future for a period of time. For instance, Lindblade and colleagues (2015) conducted a prospective cohort study in Malawi to test the efficacy of ITNs in an area of moderate resistance to pyrethroids, a common class of insecticide. A prospective cohort of 1,199 healthy children aged 6-59 months was followed for a year. This group of children make up the cohort, and the fact that they were recruited and followed for a period of time into the future makes the design prospective. Compared to no bed nets, ITNs reduced the incidence of malaria infection by 30%. This is promising, but the study design has limitations. One important limitation is that the children were not randomized to ITN access. So it could be the case that children who used the ITNs were somehow different from the children who did not use the ITNs. This is a potential selection bias, a threat to internal validity. You’ll learn more about such threats in a later chapter. The basic challenge for causal inference is that the design does not rule out the possibility that something other than ITN use accounted for the reductions in malaria infections.   Case-control Sometimes it is not possible to recruit a group of healthy people and wait to see who gets sick. Imagine having to wait a decade or more to see who develops rare diseases like gliomas. This would be a very expensive study that would need to involve thousands of people to study such a rare disease that takes time to emerge. A case-control study might be a better fit. In this design, researchers identify people with the disease (cases) and without the disease (controls) and ask them about past exposures. Obala et al. (2015) did this in Kenya with 442 children hospitalized with malaria and healthy matched controls without evidence of malaria. They wanted to know why there is a high malaria burden despite high ITN coverage. The research team visited visited the home of each case and control and asked questions about ITN coverage and recent use, along with measuring the parasite burden of family members, mapping nearby potential vector breeding sites, and assessing neighborhood ITN coverage. Obala and colleagues found that ITN coverage was not correlated with hospitalizations, but consistent ITN use decreased the odds of hospitalizations by more than 70%. As with prospective cohort designs, there is a risk of selection bias. In this case, we have to be concerned that the matching process was not perfect. The matching was done on the basis of age, gender, and village. But there could be unmeasured ways in which the cases and controls differ, which would undermine the results.  Correlational studies can yield important insights of course, but they have limitations. You’ve probably heard that correlation does not equal causation. For instance, did you know there is a nearly perfect correlation between the per capita consumption of cheese and the number of people who have died by becoming tangled in their bedsheets? (If you just put down the hunk of aged cheddar you were eating, please keep reading this book!) That said, all studies have limitations and tradeoffs. Designing a good study is a process of weighing scientific objectives with logistical constraints, ethical considerations, time, money, and a host of other factors. Keep reading to learn more about how to make these tough calls.    Experimental and quasi-experimental research Without a doubt, the correlational results described in studies like Noor et al. (2006) can help to design programs and policies. But what we often want to know is whether our programs and policies “work”. When we ask whether something works, it’s a question of impact, and impact evaluations use experimental or quasi-experimental research designs. (Experimental and quasi-experimental are addressed in more detail in later chapters) The goal of impact evaluations is causal inference. Does X cause Y? Does a particular program or intervention or treatment increase or decrease a particular outcome? For reasons we will explore in greater detail later, experimental research designs offer the cleanest estimate of impact. The hallmark of an experimental design is that we as researchers manipulate some independent variable and examine changes to some dependent variable that result. A common example in global health is the randomized controlled trial (RCT) in which some units, such as individuals, schools, or communities, are randomly assigned to receive an intervention (treatment) or not (control). We measure an outcome after the intervention period and estimate the average difference between the two study arms, also known as the average treatment effect. Experiments are the “gold standard” in the eyes of many people, but researchers are not always able to assign people or clusters to study arms or otherwise manipulate an independent variable. Logistics and ethics can get in the way. In these cases, researchers might rely on non-experimental designs commonly referred to as “quasi-experimental” designs. The name of the game in quasi-experimental research is to reduce threats to internal validity, something that randomization pretty much takes care of naturally. Beware: not all non-experimental designs are created equal. We’ll discuss several of them later in this book, including:  pre-post post-test only difference-in-differences multivariate regression and matching regression discontinuity instrumental variables interrupted time series   An important global health policy question that has been studied using experimental and quasi-experimental methods is the impact of user fees on the adoption of health goods, such as bed nets. Advocates of fees argue that free distribution is not sustainable and leads to waste when people who don’t need or want the goods are recipients. There is also an argument that people only value what they pay for, so removing fees will make people less likely to use goods like bed nets. The flip side is that the provision of some health goods, in economics-speak, creates “positive externalities” and should therefore be financed with public dollars. What this means is that some interventions have spillover effects whereby people who are not treated still experience some indirect effect. A good example of a spillover effect is vaccines and the resulting herd immunity. Hawley et al. (2003) showed a similar protective effect of ITN use on child mortality and other malaria-related outcomes among households without ITNs that were located within 300 meters of households with ITNs. So we know that there is evidence that ITNs have direct (Phillips-Howard et al. 2003) and indirect benefits. The research question is then how to increase coverage and use of nets. Is free distribution the best strategy, or should users have to spend something to get a bed net that might retail for a price that is out of reach for many poor households?  Quasi-experimental Agha et al. (2007) used a quasi-experimental design to estimate the impact of a social marketing intervention on ownership and use of ITNs in rural Zambia. Nets that commonly sold for USD $27 were subsidized and sold for $2.50 at public health clinics. Neighborhood health committees were established and 600 volunteer “promoters” were trained to teach residents about malaria and encourage them to purchase the nets. To estimate the impact of the intervention, the authors analyzed data from post-intervention surveys in three intervention and two comparison districts. This study design was quasi-experimental because the districts were not randomized to the intervention or control arms.    Figure 1.9: Source: Agha et al. (2007), http://bit.ly/1MkO5a0   Agha and colleagues reported that ITN ownership and use was higher in intervention districts according to the post-intervention data, but were careful to avoid going ‘beyond the data’ to claim evidence of a causal relationship. There are several design limitations to consider here, and you will learn more about how to spot these issues as we go. Briefly, we can note that (i) the authors did not randomize districts to study arms and (ii) no baseline (a.k.a. pre-treatment) data was collected. Experimental studies benefit from but do not require baseline (or pre-intervention) data because randomization usually ensures that the treatment and comparison groups are similar at the start—if enough units are randomized. But a non-randomized study like this leaves itself open to criticism without baseline data to show that the intervention and comparison districts were similar before the intervention was introduced. The results suggest that they were different after the intervention period, but we can’t be sure this was caused by the intervention itself. Given the limitations, how should we view the results? If this was one of the first studies on the topic, we would view it as a starting point that would encourage more rigorous investigations. As part of a larger body of evidence, however, it would probably be passed over in systematic reviews and meta-analyses—studies of studies—because of the limitations of the design for causal inference.   Experimental Another limitation of Agha et al. (2007), at least for our purposes, is that it does not provide a direct answer to our policy question: should ITNs be free or subsidized? Fortunately, other studies fill this gap. Cohen and Dupas (2010) used an experimental design to study this question in Kenya where malaria is the leading cause14 of morbidity and mortality. The authors randomly assigned 20 prenatal clinics in an endemic region to 1 of 5 groups: a control group that did not distribute ITNs, a free distribution group, a group that charged 10 Ksh per ITN (97.5% subsidy), a group that charged 20 Ksh (95% subsidy), and a group that charged 40 Ksh or about $0.60 USD (90% subsidy). When units like clinics, schools, and villages are randomized, we refer to the design as a cluster-randomized trial, or CRT. The authors followed up a subset of pregnant women over time and found that those who paid a subsidized price were no more likely to use the bed nets than women who received one for free. They also found that the increase in price from $0 to $0.60 USD reduced demand for ITNs by 60%. This implies that the cost-sharing model of having women pay something for ITNs will reduce coverage. This is bad for the women who forgo a net purchase because of the direct prevention effects of ITNs, but we know from Hawley et al.’s work that it’s also bad for the community since ITNs have spillover effects. Cohen and Dupas conclude that free distribution would ultimately save more child lives.     1.2.4 Research methods If research designs are strategies for answering research questions with the best possible evidence, then research methods are the tactics for obtaining the evidence. Often methods are divided into three broad categories:  quantitative qualitative mixed  Quantitative methods are used to collect and analyze numerical data. This includes binary or dichotomous** data (e.g., hospitalized or not), categorical data (e.g., wealth quintile), and continuous data (e.g., hematocrit). A good example of a quantitative method is a survey in which people are asked to answer questions with fixed response options or provide numerical values, such as their monthly income. Lab tests resulting in disease classifications (yes/no) or a measurement such as the number of blood cells in a sample of blood are also examples of quantitative methods. Qualitative methods focus on non-numerical data. Participant observation, interviews, and focus group discussion are common qualitative methods in global health. Qualitative methods are well-suited for obtaining thick description and for exploration. For instance, Scandurra et al. (2014) analyzed data from interviews, observations, photos, and videos to study perceptions and practices related to bed net care and repair in Uganda. As is typical of manuscripts based on qualitative data, the authors include illustrative quotes, such as this one regarding net repair from a 55 year-old female:  [It] depends on one’s situation. If you have money, there is no need of sewing a net, you just buy a new one but if you are poor, you have to do it. So this is when you are poor.  Scandurra et al. found that there are strong social norms around net hygiene and appearance. Dirt floors and indoor cooking with dirty fuel sources and little ventilation tarnish the look of nets, and as a result, nets get washed frequently and may reach their lifetime wash limit much sooner than commonly assumed. If true, this could have implications for preventive efficacy. Often qualitative methods are seen as being less rigorous because they are more flexible and do not lead to the same type of hypothesis testing and results compared to quantitative methods. But this is not true. As we’ll discuss in a later chapter, rigor is a characteristic of how the methods are applied rather than the methods themselves. Your choice of methods should be based on your research question. It’s often the case that impact evaluations use quantitative methods, but there is not a 1-to-1 match between research designs and methods. Many studies incorporate both quantitative and qualitative methods, and we refer to this as mixed methods. Sometimes the goal of mixing methods is triangulation of results with respect to the same research question. Other times we begin with qualitative work to develop the tools and measures that we will use in a trial. When qualitative work follows a quantitative phase, the goal is often to explain or explore results in more depth that was not possible with the quantitative data. Increasingly you will see RCTs complement their use of quantitative methods with qualitative inquiry (O’Cathain et al. 2013). Alaii et al. (2003) provide a good example. The authors of this paper incorporated qualitative interviews on non-adherence into a larger randomized trial of the efficacy of ITNs on child morbidity and mortality in Kenya (Phillips-Howard et al. 2003). They wanted to better understand why people, particularly children under the age of 5, were not using their ITNs correctly. Alaii et al. found that more than a quarter of individuals were non-adherent, often due to excessive heat.   1.2.5 Theories and hypotheses Many impact evaluations fit the label of black box evaluations, meaning that they don’t focus on why programs do or don’t have an impact. The evaluation is not guided by theory, and the hypotheses are as simple as “the program will have an impact on the outcome”. White (2009) outlines a strategy for changing this and moving to theory-based impact evaluations (White 2009). Leary (2012) defines a theory as “a set of propositions that attempts to explain the relationships among a set of concepts”. In quantitative research, you could replace “propositions” with “hypotheses” and “concepts” with “variables”. As we reviewed earlier, the logical approach in quantitative research is often deductive. You start with theory and develop research hypotheses that are then tested. A hypothesis is an a priori prediction about what will occur—about how constructs are related. If the hypothesis is supported by the data, you have support for the underlying theory. If your study is well designed, it might be given more weight as other researchers consider the evidence in support of the theory. In theory testing, 1+1 does not always equal 2. For a hypothesis to be scientific it should be falsifiable, or testable. To return to a silly example from earlier, the following would not be a research hypothesis because it cannot be tested: “if a mosquito is killed, it goes to mosquito heaven”. Maybe, but we can’t test this hypothesis. Science progresses through the possibility of falsification, so hypotheses must be engineered to potentially fail.  Proving and disproving theories To return to an earlier example, some people advocate against the free distribution of ITNs out of the belief that there is a “sunk cost” effect when having to spend money for a bed net; people will use the net more to justify their purchase (Arkes and Blumer 1985). In this case, the theory is one of sunk costs directing behavior. The falsifiable hypothesis tested by Cohen and Dupas (2010) was that people who paid a non-zero price for an ITN would use the ITN more than those who received the ITN for free. As you will recall from our discussion, there was not support for this hypothesis. So the theory is rejected, right? Not necessarily. Leary (2012) offers some helpful advice for thinking about proof and disproof. Proof is logically impossible, whereas disproof is practically impossible. Frustrating, right?  Proof is not possible It helps to state the theory and hypothesis as an if-then statement. For example, “If the theory of sunk cost effects is true, then people who pay for an ITN will be more likely to use it than people who get an ITN for free.” If the theory is true, the hypothesis will be true. What happens if you flip this statement? If you find evidence that the hypothesis is true—as you might in a study—does it mean that the theory is true? Cohen and Dupas (???) did not find support for the hypothesis that people who paid a non-zero price for an ITN would use the ITN more than those who received the ITN for free. But let’s pretend for a moment that they did. Would that prove the sunk cost theory? No, logically it can’t. It would be like concluding that my raging fever is malaria because I have mosquito bites all over my arm. The “theory” here would be that my fever is malaria, and the hypothesis would be that I must have been bitten by a mosquito. If I have mosquito bites all over, my fever must be malaria, right? Well, no. I was bitten by a mosquito, but maybe the scene of the crime was my backyard in the eastern United States where we don’t worry about malaria. So in this case, the hypothesis was true, but it doesn’t prove the theory.   Disproof is possible, but uncommon What if the hypothesis was not supported, and I was not bitten by mosquitos? Could my “theory” be true—could my fever be malaria? No. And logic would support this. If the hypothesis is derived from the theory, and if the hypothesis is not supported, the logical inference is that the theory is wrong. Yet, we still shy away from concluding that the theory is wrong. The reason is simple: complexity. A study like Cohen and Dupas (???) could fail to reject the null hypothesis that use does not differ between free and subsidized clients—thus not supporting the hypothesis of different use rates—but there are many practical reasons for this. For instance, maybe their measure of bed net use was systematically flawed and hid the difference as a result. The possibilities are endless. This is partly why journals are hesitant to publish null results.    Science marches on So, where do we go from here? Answer: the literature! No one study is enough to lead people to discard a theory. But several null results might be. Conversely, no study ever proves a theory, but an accumulation of studies showing support for the theory-derived hypothesis builds confidence in the theory. Particularly when the studies are conducted by different researchers, across different populations, and triangulating with multiple methods. Of course you see the challenge here. How do researchers know that several studies have failed to support a certain theory if journals are reluctant to publish null results? And if negative evidence is missing, won’t the positive evidence be over-represented in the literature? Yes. This is the problem of publication bias, or the file drawer problem, and there is not an easy answer. Efforts like AllTrials to register and report the results of all trials, regardless of outcome, seem like a step in the right direction. So, to the literature we go.     "],
["literature.html", "2 Search and Ye Shall Find 2.1 Start with Systematic Reviews and Meta-Analyses 2.2 Devising a Search Strategy 2.3 For the Love of Everything Holy Use a Reference Manager", " 2 Search and Ye Shall Find The starting point of every research study is a literature review. To know where you are going, you need to know where the field has been. Technology makes this easier in some ways than it has been in the past, but we’re swimming in information, and the pool gets deeper every day. A lot deeper, actually. Google’s former CEO Eric Schmidt has said we create as much information every two days as we did from the beginning of time through 2003. Two days! And he said this back in 2012, so it’s an even shorter time span today. Of course the “cat photo” to “research finding” ratio is probably something like 1,000,000:1 nowadays, but this only makes the point that good information can be hard to find. In this chapter we’ll discuss a strategy for quickly getting a sense for the state-of-the-art in health research, and then outline the steps you need to take to ask a good research question and search the literature for primary sources.  2.1 Start with Systematic Reviews and Meta-Analyses Repeat after me:  I will not start my research by Googling “malaria”. I will not start my research by Googling “malaria”. I will not start my research by Googling “malaria”.  If your topic is malaria and you’re not sure if the vector is mosquitos or monkeys, then Wikipedia is probably a good first stop. There’s no shame in that. Otherwise, it is always a good idea to start with a check for relevant systematic reviews or meta-analyses.    Figure 2.1: Levels of evidence   Meta-analyses and systematic reviews are ‘studies of studies’, and they sit atop the evidence hierarchy. They enjoy this status because they synthesize the best available evidence. Finding a good review beats Googling “malaria” any day.  2.1.1 Meta-Analyses A meta-analysis is a quantitative approach in which the results from multiple studies are combined to estimate an overall effect size. We’ll talk more about effect sizes later, but the concept is pretty simple. Let’s use a meta-analysis by Radeva-Petrova et al. (2014) as an example. The authors reviewed 17 studies of the effects of chemoprevention on pregnant women living in malaria-endemic areas. The basic question they set out to answer with their review was as follows: Do women who take antimalarial medication during pregnancy have a lower risk of getting infected with malaria, and thus a lower risk of experiencing the bad health outcomes that are associated with malaria? One indicator of malaria infection is parasitaemia, or the presence of malaria parasites in the blood. If chemoprevention has some preventive effect, you’d expect to see less parasitaemia among women exposed to the medication (aka, treatment). Few interventions are 100% effective, so we often talk about reductions in the risk of bad outcomes like malaria. This is one type of effect size, a measure of the strength or magnitude of a relationship, such as a the relationship between taking a medicine and experiencing a bad outcome. Here is a forest plot from Radeva-Petrova et al. (2014) that shows the results of 10 studies (8 trials) that compared cases of parasitaemia among 3,663 pregnant women who were randomized to an intervention group (n=2,053) that received some preventive antimalarial drug or to a control group (n=1,610) that received a placebo (or nothing).    Figure 2.2: Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj     Here’s how Lewis and Clarke (2001) describe a forest plot:    In a typical forest plot, the results of component studies are shown as squares centred on the point estimate of the result of each study. A horizontal line runs through the square to show its confidence interval—usually, but not always, a 95% confidence interval. The overall estimate from the meta-analysis and its confidence interval are put at the bottom, represented as a diamond. The centre of the diamond represents the pooled point estimate, and its horizontal tips represent the confidence interval. Significance is achieved at the set level if the diamond is clear of the line of no effect.     The plot allows readers to see the information from the individual studies that went into the meta-analysis at a glance. It provides a simple visual representation of the amount of variation between the results of the studies, as well as an estimate of the overall result of all the studies together. Forest plots increasingly feature in medical journals, and the growth of the Cochrane Collaboration has seen the publication of thousands in recent years.    Lewis and Clarke (2001) discovered that the first forest plot was published in 1978, and first used in a meta-analysis in 1982. The name lagged behind, appearing first in 1996, apparently referring to the forest of lines typical of most forest plots.   Details about each study are reported as rows in this figure. Take a look at the study by Shulman et al. (1999) in row 6. This study found that 30 of the 567 women in the intervention group tested positive for parasitaemia (i.e., malaria). This compared to 199 of the 564 woman in the control group. This is a risk ratio of 0.15—(30/567)/(199/564) = 0.15—which means that chemoprevention reduced the risk of parasitaemia by 85%. This is a huge effect size! The effect size for each study is presented in the far right column and depicted graphically in the size of point estimate square. All of the point estimates fall to the left of the line of no effect (&lt;1), thus favoring chemoprevention because you want to reduce the risk of this bad outcome. [A risk ratio of 1 would indicate no difference in risk, and a ratio &gt;1 would mean the risk is higher among the intervention group, thus favoring the control group.] The overall (pooled) effect size is shown last as 0.39, or a 61% reduction in the risk of parasitaemia. We won’t bother ourselves with the calculation of this pooled effect size, other than to note that it’s not as simple as averaging the 10 studies. This is because the studies were not given equal weight, as you can see in the “weight” column. For instance, Greenwood et al. (1989) only had a sample size of 21+13=34 children. As a result, the effect size estimate is very noisy. The 95% confidence interval is wide and crosses 1. Consequently, it’s weight is lower than others at 6.7%. Simply put, studies weaker research designs get less weight in the analysis. So here in one figure we get a summary of the best available evidence and an estimate of the overall effect size, with uncertainly intervals. You can’t get that from a Google search.   2.1.2 Systematic Reviews You might be wondering how Radeva-Petrova et al. (2014) found these studies in the first place. The answer is through a systematic review of the literature. Most, if not all, meta-analyses will be completed as part of a systematic review of the literature, and every systematic review is a type of literature review. But not every literature review is a systematic review, even if done systematically.    Figure 2.3: Literature reviews, systematic reviews, and meta-analyses   As you can see from the table below, a systematic review requires a number of steps that are good practice, but too thorough and time-consuming for the general literature review you might prepare when starting your work. Nevertheless, you need to know the general process of preparing a systematic review to evaluate the quality of the reviews you read, and you’ll hopefully pick up some good habits along the way.  Table 2.1: Comparing systematic reviews and literature reviews.   Systematic Reviews Literature Reviews     The goal of a systematic review is to to be comprehensive and include every relevant article. The literature review that you write for the introduction of your manuscript is not expected to be exhaustive.   For this reason, most systematic reviews are conducted by teams given the large scope of the work. literature reviews can be handled solo.   Systematic reviews must define and follow a method that can be replicated, just like any other study. Literature reviews, on the other hand, don’t have to follow such rigid methods or make the methods explicit.   Most systematic reviews pre-register this plan, meaning that the authors submit their planned methods to a registry like PROSPERO prior to conducting the study. Pre-registration gives other researchers confidence that the team is not cherry picking results at the end to make an interesting paper. It also lets other researchers know that a group is already working on the same review, thus signaling that their work might duplicate efforts and fail to get published. Not the case of for literature reviews.   Included in these pre-registration plans will be a specific search strategy with exact search terms for individual scholarly databases so other researchers can recreate the search. It’s a good idea to do the same for a literature review, even if not a strict requirement.   Similarly, a systematic review must also outline clear criteria for including and excluding studies (e.g., keep if assignment to study arms was random). With these criteria in place, team members screen all search results, usually starting with title and abstract reviews only and moving to full text reviews as the pool of eligible studies dwindles. Screening for a literature review is typically less intensive.   Systematic reviews also develop and follow guidelines for extracting details from every included study, such as numbers of participants and key outcomes. An annotated bibliography might suffice for a literature review.   Finally, teams conducting systematic reviews formally assess the quality of each included study, including the potential for bias, and take these assessments into account when synthesizing the results. This process is more ad hoc for literature reviews.     Where to find systematic reviews Three excellent sources for finding systematic reviews (and meta-analyses) in global health are the Cochrane Library, the Campbell Collaboration, and 3ie. You can also get to many of the reviews in these databases by searching within PubMed using the Clinical Queries feature.   How to read systematic reviews  Abstract and plain language summary Cochrane reviews follow a standard format that can look overwhelming at first, but is actually quite easy to read and understand. As with most journal articles, Cochrane reviews begin with an Abstract. Next comes a Plain language summary which can be helpful for newcomers to a particular topic. Radeva-Petrova et al. (2014) include the following passage in their plain language summary:  For women in their first or second pregnancy, malaria chemoprevention prevents moderate to severe anaemia (high quality evidence); and prevents malaria parasites being detected in the blood (high quality evidence). It may also prevent malaria illness. We don’t know if it prevents maternal deaths, as this would require very large studies to detect an effect.  This one paragraph brings us up to speed with the state of the science for preventing malaria and its effects among pregnant women living in malaria-endemic areas (and points you to some gaps in the literature!). Google does not filter the evidence in this manner. Starting with systematic reviews pays off almost every time.   Summary tables Next come the Summary tables, such as the one presented below from Radeva-Petrova et al. (2014). These tables round out everything you need to make your initial judgment.    Figure 2.4: Malaria chemoprevention for pregnant women living in endemic areas. Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj   The first comparative risk column shows the assumed risk among the control group. For instance, the risk of antenatal parasitaemia is 286 events per every 1,000 people. This is the median control group risk across eight trials of 3,663 women. The relative risk is 0.39—recall that this is the pooled, or “meta” effect size—so you can see how the corresponding risk among the intervention group is 286*0.39=111 per 1,000 people.15 As shown in the final column, the quality of this evidence is rated as “high”. The authors are referring here to GRADE criteria, a systematic approach to evaluating the quality of empirical evidence:  High—Further research is very unlikely to change our confidence in the estimate of effect. Moderate—Further research is likely to have an important impact on our confidence in the estimate of effect and may change the estimate. Low—Further research is very likely to have an important impact on our confidence in the estimate of effect and is likely to change the estimate. Very Low—We are very uncertain about the estimate.    Background Much of what you want to know you can learn from the abstract, summary text and tables, and forest plots (if included). If you keep reading, you’ll come next to the Background section. This is typically a short overview that explains what gaps in our knowledge the review is intended to fill. Radeva-Petrova et al. (2014) use this section to present a conceptual framework for malaria prevention during pregnancy.    Figure 2.5: Drugs for preventing malaria in pregnancy: conceptual framework. Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj     Methods The Methods section details how the review was organized and conducted. The purpose of this section is to provide enough detail to enable other researchers to attempt to replicate the review. The main components are:16  A description of the population and intervention. The key outcomes of interest. The search strategy and databases. Inclusion and exclusion criteria for studies. Procedures for extracting information from each study. Procedures for assessing bias and conducting a meta-analysis (if one is included)    Results The Results section typically begins with details about how many primary articles were identified, screened, and excluded, typically presented graphically with a flow diagram like the one below from Radeva-Petrova et al. (2014).    Figure 2.6: Study flow diagram. Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj   Once the included studies are identified, it’s customary for review authors to report on the quality of the evidence presented in each study. We’ll discuss the nature of these sources of bias in a later chapter, but you should familiarize yourself with these heatmaps. While a bit on the ugly side, they provide a useful summary of bias. As a future producer of research, you should start to look at these figures and think, “how can I make sure my studies are full of green pluses?”    Figure 2.7: Risk of bias summary: review authors’ judgements about each risk of bias item for each included trial. Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj     Discussion and conclusions Discussion sections provide a short summary of the findings, commentary on the quality of the evidence, and thoughts about what the review adds to the existing literature on the topic. A discussion tends to be short relative to the size of the overall review. Discussion sections are often followed by a brief statement of the authors’ conclusions. This is an opportunity for the authors to frame the results in terms of the implications for practice and research. Radeva-Petrova et al. (2014) conclude:  Routine chemoprevention to prevent malaria and its consequences has been extensively tested in RCTs, with clinically important benefits on anaemia and parasitaemia in the mother, and on birth-weight in infants.  In other words, “chemoprevention works” in this context.   Appendices Reading the appendices will give you a sense of what it takes to put together a systematic review. There are usually tables after tables of characteristics of included and excluded studies, often followed by dozens of forest plots if the systematic review includes a meta-analysis with several outcomes or populations of interest. Radeva-Petrova et al. (2014) wrap up on page 120!      2.2 Devising a Search Strategy Hopefully at this point you’ll agree that it’s a good idea to start with a systematic review, not a search engine. Of course not every topic has been the subject of a recent systematic review or meta-analysis, so you’ll sometimes need to search the primary literature yourself. I’ll show you how, but first you need to clearly define what you’re looking for.  2.2.1 Asking a research question Here’s a helpful mnemonic for creating a good clinical question: PICO.    P Patient, Population, or Problem   I Intervention, Prognostic Factor, or Exposure   C Comparison   O Outcome    Let’s try to use PICO to create a research question for a portion of the Radeva-Petrova et al. (2014) systematic review. The problem we want to address is malaria infections. The population is pregnant women living in malaria-endemic areas. Not every clinical question involves testing of a treatment or intervention, but we’ll focus a lot on these types of questions in this book. For the example at hand, the intervention would be malaria chemoprevention. [Prognostic factor refers to covariates that could influence the prognosis of the patient. An exposure would be something that we think might increase the risk of an outcome.] Similarly, not every question involves a comparison group. Randomized trials always will. In this example, the comparison is nothing or a placebo. Radeva-Petrova et al. (2014) examine a number of outcomes. We’ll focus on parasitaemia. We can combine all of this into a research question:  Among pregnant women living in malaria-endemic areas, is chemoprevention more effective than a placebo at preventing parasitaemia?    2.2.2 Approaches With your basic research question outlined, you’re ready to begin searching. At the beginning you might take a quick and dirty™ approach to get started. Eventually you’ll need to graduate to a proper search strategy to be more systematic, even if your end goal is not a capital “S” Systematic review.  Quick and dirty A reasonable initial approach is to find a few recent articles to get a quick sense of what is out there. Google Scholar could come in handy here. For instance, my advanced Google Scholar search for “malaria chemoprevention pregnant parasitaemia” (limited to recent years) identified a paper by Braun et al. (2015) on the use of intermittent preventive treatment in pregnancy (IPTp) with sulfadoxine–pyrimethamine (SP)—a specific type of chemoprevention—on malaria infections among pregnant women in western Uganda.   Customize your Google Scholar experience by clicking on the gear icon. Enable use of a bibliography manager, and click on “Library links” to add your library to get links to full text.   A good starting point for future searching is to note an article’s keywords. Not all journals print keywords, but if they do, you’ll probably find them right after the abstract. Next comes the introduction. Some journals and disciplines have very brief introduction sections and might not be of much help. This is often true in medicine and public health. The discussion section is also a place to look for new leads. Authors typically use the discussion to link the study results to the existing literature, demonstrating how the results add to what is already known. After looking at the introduction and discussion sections, it’s often useful to skim the references to get a sense of which journals publish this type of work. If a certain journal appears to be a common outlet for this work, a scan of the journal’s table of contents for recent issues could be useful.17   If you have access to a university library, you can learn more about the scholarly journals in a field by looking up Journal Citation Reports. This annual report ranks the journals in each field according to impact factors. Impact factors are one metric used to evaluate the importance of a journal in its field.     More systematic  Plan and document your search strategy Whether or not you are conducting a capital “S” Systematic review, it’s a good idea to plan and document your search. You don’t need to be as thorough in a lit review as you would for a systematic review, but it wouldn’t hurt to take a page from the approach. Let’s look at Radeva-Petrova et al. (2014) for some inspiration. Every good systematic review will include a table or appendix like this one to make the method reproducible. If you and I run this search query at the same time on two different computers, we should get the same results.    Figure 2.8: Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj   For the purposes of your literature review, you don’t necessarily need to ensure that other people can recreate your results, but you should make sure that you can.   If you can create an account with the database, do it and login to save your searches. Also, differences in the design of each database and interface often require you to customize your search. If you are conducting an actual systematic review that you wish to publish—as opposed to just searching the literature systematically—then you would benefit from consulting with a clinical librarian who will be familiar with the intricacies of building search strategies.     Selecting a database As you can see from the table, Radeva-Petrova et al. (2014) searched five databases. MEDLINE is probably the most well known of this group. When you search in PubMed, PubMed is searching the MEDLINE database. This is typically a good place to start to find health-related studies. Talk with a research librarian to understand if other databases might be a better choice for your topic.   Generate search terms Once you decide on a database, you need to generate search terms. Start with the keywords published with the sample articles you dig up. You can learn a lot about potential keywords by searching for MeSH terms. MeSH, which stands for “Medical Subject Headings”, is a controlled vocabulary thesaurus that is used to index articles in PubMed. This thesaurus is helpful because there are often many ways to refer to the same phenomenon. For instance, the MeSH term for “breast cancer” is “Breast Neoplasm”. When you search for “breast cancer” in PubMed, the database helps you out by casting a wider net: &quot;breast neoplasms&quot;[MeSH Terms] OR (&quot;breast&quot;[All Fields] AND &quot;neoplasms&quot;[All Fields]) OR &quot;breast neoplasms&quot;[All Fields] OR (&quot;breast&quot;[All Fields] AND &quot;cancer&quot;[All Fields]) OR &quot;breast cancer&quot;[All Fields] Turns out there are a lot of ways that we refer to breast cancer! The following entry terms are indexed by PubMed humans to the MeSH term “breast neoplasms”:  Breast Neoplasm Neoplasm, Breast Neoplasms, Breast Tumors, Breast Breast Tumors Breast Tumor Tumor, Breast Mammary Neoplasms, Human Human Mammary Neoplasm Human Mammary Neoplasms Neoplasm, Human Mammary Neoplasms, Human Mammary Mammary Neoplasm, Human Mammary Carcinoma, Human Carcinoma, Human Mammary Carcinomas, Human Mammary Human Mammary Carcinomas Mammary Carcinomas, Human Human Mammary Carcinoma Breast Cancer Cancer, Breast Cancer of Breast Mammary Cancer Malignant Neoplasm of Breast Malignant Tumor of Breast Breast Carcinoma Cancer of the Breast  Back in the world of mosquitos, the MeSH term for “malaria” is “malaria”, conveniently, and a search for this term in PubMed actually searches: &quot;malaria&quot;[MeSH Terms] OR malaria[Text Word] The following entry terms are indexed to the MeSH term “malaria”:  Remittent Fever Fever, Remittent Paludism Plasmodium Infections Infections, Plasmodium Infection, Plasmodium Plasmodium Infection Marsh Fever Fever, Marsh    Running your search Once you have some initial search terms, it’s time to build and run your query. This will be an iterative process, full of trial and error. You might start with 200,000 results. Some terms and combinations will fail to narrow this field. Others will trim too much.    Figure 2.9: Boolean operators: AND, OR, NOT   You’ll need to know some basic Boolean operators to be an effective searcher: AND, OR, NOT. For instance, let’s consider the search PubMed runs when you enter “malaria OR pregnancy”: (&quot;malaria&quot;[MeSH Terms] OR &quot;malaria&quot;[All Fields]) OR (&quot;pregnancy&quot;[MeSH Terms] OR &quot;pregnancy&quot;[All Fields]) These four terms are combined with OR, meaning we keep results that match any of these terms. At the time of writing, PubMed returns 922,588 results. Of course it would make more sense to search for “malaria AND pregnancy”, instead of “malaria OR pregnancy”: (&quot;malaria&quot;[MeSH Terms] OR &quot;malaria&quot;[All Fields]) AND (&quot;pregnancy&quot;[MeSH Terms] OR &quot;pregnancy&quot;[All Fields]) The first two terms and last two terms are combined separately with OR. These combinations are then combined with AND (notice the use of parentheses to segment the operations), dropping our pool of results to 4,203 records. The AND operator will always maintain or decrease the number of results. If we want to limit the results humans, we can add AND &quot;humans&quot;[MeSH Terms] to the end.18 Doing so drops our pool of results to 3,798. (&quot;malaria&quot;[MeSH Terms] OR &quot;malaria&quot;[All Fields]) AND (&quot;pregnancy&quot;[MeSH Terms] OR &quot;pregnancy&quot;[All Fields]) AND &quot;humans&quot;[MeSH Terms] Alternatively, we could use the NOT operator to limit the results to non-humans. Not surprisingly, we get 405 records. (If you are making a surprised face, think about it this way: 405 + 3,798 = 4,203 or non-human results + human results = all results) ((&quot;malaria&quot;[MeSH Terms] OR &quot;malaria&quot;[All Fields]) AND (&quot;pregnancy&quot;[MeSH Terms] OR &quot;pregnancy&quot;[All Fields])) NOT &quot;humans&quot;[MeSH Terms] Let’s return to our PICO question and use Boolean operators to combine the components.  Among pregnant women living in malaria-endemic areas, is chemoprevention more effective than a placebo at preventing parasitaemia?  Here’s what we want to do in plain English:  humans AND pregnant women (redundant, but we’ll keep to show the strategy) AND malaria endemic AND chemoprevention AND randomized controlled trial AND parasitaemia  Within each group, we have several ORs to consider. The parentheses can get confusing, so let’s build the search one line at a time.  (&quot;humans&quot;[MeSH Terms] OR &quot;humans&quot;[All Fields]) AND (&quot;pregnant women&quot;[MeSH Terms] OR (&quot;pregnant&quot;[All Fields] AND &quot;women&quot;[All Fields]) OR &quot;pregnant women&quot;[All Fields]) (&quot;malaria&quot;[MeSH Terms] OR &quot;malaria&quot;[All Fields]) AND endemic[All Fields] (&quot;chemoprevention&quot;[MeSH Terms] OR &quot;chemoprevention&quot;[All Fields]) OR (&quot;chloroquine&quot;[MeSH Terms] OR &quot;chloroquine&quot;[All Fields]) OR (&quot;pyrimethamine&quot;[MeSH Terms] OR &quot;pyrimethamine&quot;[All Fields]) OR (&quot;proguanil&quot;[MeSH Terms] OR &quot;proguanil&quot;[All Fields]) OR (Pyrimethamine-dapsone[All Fields]) OR (&quot;fanasil, pyrimethamine drug combination&quot;[Supplementary Concept] OR &quot;fanasil, pyrimethamine drug combination&quot;[All Fields] OR &quot;sulfadoxine pyrimethamine&quot;[All Fields]) OR (&quot;mefloquine&quot;[MeSH Terms] OR &quot;mefloquine&quot;[All Fields]) [Publication Type] OR &quot;randomized controlled trials as topic&quot;[MeSH Terms] OR &quot;randomized controlled trial&quot;[All Fields] OR &quot;randomised controlled trial&quot;[All Fields] parasitaemia[All Fields] OR &quot;parasites&quot;[MeSH Terms] OR &quot;parasites&quot;[All Fields] OR &quot;parasite&quot;[All Fields]  All together now with ANDs. As I tap out these words on my keyboard, this search returns 513 records in PubMed. ((&quot;humans&quot;[MeSH Terms] OR &quot;humans&quot;[All Fields]) AND (&quot;pregnant women&quot;[MeSH Terms] OR (&quot;pregnant&quot;[All Fields] AND &quot;women&quot;[All Fields]) OR &quot;pregnant women&quot;[All Fields])) AND ((&quot;malaria&quot;[MeSH Terms] OR &quot;malaria&quot;[All Fields]) AND endemic[All Fields]) AND (&quot;chemoprevention&quot;[MeSH Terms] OR &quot;chemoprevention&quot;[All Fields]) OR (&quot;chloroquine&quot;[MeSH Terms] OR &quot;chloroquine&quot;[All Fields]) OR (&quot;pyrimethamine&quot;[MeSH Terms] OR &quot;pyrimethamine&quot;[All Fields]) OR (&quot;proguanil&quot;[MeSH Terms] OR &quot;proguanil&quot;[All Fields]) OR (Pyrimethamine-dapsone[All Fields]) OR (&quot;fanasil, pyrimethamine drug combination&quot;[Supplementary Concept] OR &quot;fanasil, pyrimethamine drug combination&quot;[All Fields] OR &quot;sulfadoxine pyrimethamine&quot;[All Fields]) OR (&quot;mefloquine&quot;[MeSH Terms] OR &quot;mefloquine&quot;[All Fields]) AND ([Publication Type] OR &quot;randomized controlled trials as topic&quot;[MeSH Terms] OR &quot;randomized controlled trial&quot;[All Fields] OR &quot;randomised controlled trial&quot;[All Fields]) AND (parasitaemia[All Fields] OR &quot;parasites&quot;[MeSH Terms] OR &quot;parasites&quot;[All Fields] OR &quot;parasite&quot;[All Fields]) Once you are satisfied with your results, you could choose to apply the same search to another database. This might be worth the effort if your topic crosses disciplinary boundaries, like economics and health. Best to check with a research librarian.   Screening results Even the best search queries return some duds, so the next step is screening. We can return to Radeva-Petrova et al. (2014) to see what a thorough approach looks like. You would likely take some shortcuts for a regular literature review. Typically systematic review searches will return hundreds or thousands of potential hits, so a study team will screen titles and abstracts to exclude obvious mistakes. When beginning this process, it’s common to have team members screen some of the same records to establish reliability, a concept that we will discuss in more depth in the chapter on measurement. Basically, you want to know that everyone screening records would make the same inclusion/exclusion decision. The Radeva-Petrova et al. (2014) search strategy turned up 179 unique records, and the authors excluded 126 of these records after screening the abstracts. The excluded studies did not meet certain pre-defined criteria. For instance, the authors only wanted to include studies using RCTs and quasi-experimental designs. This left the team with 53 studies that required a full-text review. Only 17 of the 53 studies still met eligibility criteria after this review.19   Supplemental searches It is customary in a systematic review–and helpful in general reviews—to augment database searches with reference reviews and hand searches to ensure that no key references were missed in the database query. A reference review is nothing more than a scan of an eligible article’s bibliography. In a hand search, you would go to the website of journals that published the eligible articles and scan the tables of contents for each issue published during the search window. If you find that either supplemental method turns up a lot of new results, it could make sense to revise your systematic review search strategy to be more comprehensive.   Extracting data Depending on your objectives you might choose to systematically extract data from each study—key facts related to study design, methods, and results. Or you might take a shorter path and create an annotated bibliography. If you need to be more systematic—an essential requirement for a capital “S” Systematic review—then you need to design a data extraction strategy. Your PICO research question can be a helpful guide to identifying the minimum data you should extract. Returning to Radeva-Petrova et al. (2014):  Among pregnant women living in malaria-endemic areas, is chemoprevention more effective than a placebo at preventing parasitaemia?  Some possibilities include:  study setting/population sample size sample demographics, including parity study design intervention details, such as specific medication and dose primary outcome (e.g., parasitaemia) effect size  There are numerous software options for storing your extracted data, but you’ll likely find that a simple spreadsheet with rows of studies and columns of study variables will work just fine. Lots of teams use this approach for big systematic reviews, so it will probably serve you well for something more modest.      2.3 For the Love of Everything Holy Use a Reference Manager Even if you chose to ignore everything I’ve written up to this point, do yourself a favor and use a program for managing references. I’m amazed every year when I learn that students on the precipice of graduation manually type and format their in-text citations and bibliographies. What a waste of time! There are several reference managers you might consider. I’ll mention one because it is free and open-source: Zotero. The concept of “free” does not need much explanation, but students often have several free options that are not really free. A good example is a program like EndNote. A university might make this program a free download for enrolled students, but the license expires upon graduation or soon becomes obsolete without a paid upgrade. Additionally, in global health it’s common to work with colleagues who do not have access to a program like EndNote, which makes collaboration challenging. For these reasons, I highly recommend a program like Zotero that is free to use and open to improve. A tutorial is beyond the scope of this chapter, but it’s worth mentioning some features that are common to many reference managers:  Easy importing of references from databases like PubMed. Go from your search results to reference manager in seconds. Automatic retrieval of full-text PDF. Sync PDFs in collection to tablets and phones Connections to word processing software to make inserting references in papers a snap. Automatic creation of bibliographies based on works cited. Push button reformatting of in-text citations and references to different styles, such as APA and Harvard. Shared collections with automatic syncing via the cloud to facilitate collaboration. Easy export of references for migration to just about any other reference manager.  So next time you see someone typing references and complaining about APA formatting, open your laptop and run your reference manager.   Be prepared to offer tissues. Your colleague will either cry for joy or deep despair about wasted effort. You’ll feel good about yourself regardless.     "],
["references.html", "3 References", " 3 References   Agha, Sohail, Ronan Van Rossem, Guy Stallworthy, and Thankian Kusanthan. 2007. “The Impact of a Hybrid Social Marketing Intervention on Inequities in Access, Ownership and Use of Insecticide-Treated Nets.” Malaria Journal 6 (1): 13. http://www.malariajournal.com/content/6/1/13. Alaii, Jane A, William A Hawley, Margarette S Kolczak, Feiko O Ter Kuile, John E Gimnig, John M Vulule, Amos Odhacha, Aggrey J Oloo, Bernard L Nahlen, and Penelope A Phillips-Howard. 2003. “Factors Affecting Use of Permethrin-Treated Bed Nets During a Randomized Controlled Trial in Western Kenya.” The American Journal of Tropical Medicine and Hygiene 68 (suppl 4): 137–41. http://www.ajtmh.org/content/68/4_suppl/137.long. Alonso, Pedro L, Jahit Sacarlal, John J Aponte, Amanda Leach, Eusebio Macete, Jessica Milman, Inacio Mandomando, et al. 2004. “Efficacy of the RTS, S/AS02A Vaccine Against Plasmodium Falciparum Infection and Disease in Young African Children: Randomised Controlled Trial.” The Lancet 364 (9443): 1411–20. http://www.ncbi.nlm.nih.gov/pubmed/15488216. Aponte, John J, Pedro Aide, Montse Renom, Inacio Mandomando, Quique Bassat, Jahit Sacarlal, M Nelia Manaca, et al. 2007. “Safety of the RTS, S/AS02D Candidate Malaria Vaccine in Infants Living in a Highly Endemic Area of Mozambique: A Double Blind Randomised Controlled Phase I/IIb Trial.” The Lancet 370 (9598): 1543–51. http://www.ncbi.nlm.nih.gov/pubmed/17949807. Arkes, Hal R, and Catherine Blumer. 1985. “The Psychology of Sunk Cost.” Organizational Behavior and Human Decision Processes 35 (1): 124–40. http://www.sciencedirect.com/science/article/pii/0749597885900494. Braun, Vera, Eva Rempis, Alexandra Schnack, Sarah Decker, John Rubaihayo, Nazarius Mbona Tumwesigye, Stefanie Theuring, Gundel Harms, Priscilla Busingye, and Frank P Mockenhaupt. 2015. “Lack of Effect of Intermittent Preventive Treatment for Malaria in Pregnancy and Intense Drug Resistance in Western Uganda.” Malaria Journal 14 (1): 1–10. http://link.springer.com/article/10.1186/s12936-015-0909-7. Campbell, D. T. 1969. “Reforms as Experiments.” American Psychologist 24 (4): 409. http://psycnet.apa.org/journals/amp/24/4/409/. Cohen, Jessica, and Pascaline Dupas. 2010. “Free Distribution or Cost-Sharing? Evidence from a Randomized Malaria Prevention Experiment.” Quarterly Journal of Economics 125 (1): 1–45. http://www.povertyactionlab.org/publication/free-distribution-or-cost-sharing-evidence-malaria-prevention-experiment-kenya. Cronbach, L. J. 1982. Designing Evaluations of Educational and Social Programs. Jossey-Bass. http://amzn.to/1L4gxwx. D. de Vaus. 2001. Research Design in Social Research. Sage. http://amzn.to/1MY21GT. Doherty, JF, M Pinder, N Tornieporth, C Carton, L Vigneron, P Milligan, WR Ballou, et al. 1999. “A Phase I Safety and Immunogenicity Trial with the Candidate Malaria Vaccine RTS, S/SBAS2 in Semi-Immune Adults in the Gambia.” The American Journal of Tropical Medicine and Hygiene 61 (6): 865–68. The American Journal of Tropical Medicine and Hygiene, 61(6)*](http://www.ajtmh.org/content/61/6/865.full.pdf+html. Glennerster, R., and K. Takavarasha. 2013. Running Randomized Evaluations: A Practical Guide. Princeton University Press. http://amzn.to/1eQqpvr. Greenwood, BM, AM Greenwood, Robert W Snow, Peter Byass, S Bennett, and AB Hatib-N’Jie. 1989. “The Effects of Malaria Chemoprophylaxis Given by Traditional Birth Attendants on the Course and Outcome of Pregnancy.” Transactions of the Royal Society of Tropical Medicine and Hygiene 83 (5): 589–94. http://bit.ly/1VeiDhC. Hawley, William A, Penelope A Phillips-Howard, Feiko O ter Kuile, Dianne J Terlouw, John M Vulule, Maurice Ombok, Bernard L Nahlen, et al. 2003. “Community-Wide Effects of Permethrin-Treated Bed Nets on Child Mortality and Malaria Morbidity in Western Kenya.” The American Journal of Tropical Medicine and Hygiene 68 (4 suppl): 121–27. http://www.ajtmh.org/content/68/4_suppl/121.long. Hogan, Margaret C, Kyle J Foreman, Mohsen Naghavi, Stephanie Y Ahn, Mengru Wang, Susanna M Makela, Alan D Lopez, Rafael Lozano, and Christopher JL Murray. 2010. “Maternal Mortality for 181 Countries, 1980–2008: A Systematic Analysis of Progress Towards Millennium Development Goal 5.” The Lancet 375 (9726): 1609–23. http://www.thelancet.com/journals/lancet/article/PIIS0140-6736(10)60518-1/abstract. Kassebaum, Nicholas J, Amelia Bertozzi-Villa, Megan S Coggeshall, Katya A Shackelford, Caitlyn Steiner, Kyle R Heuton, Diego Gonzalez-Medina, et al. 2014. “Global, Regional, and National Levels and Causes of Maternal Mortality During 1990–2013: A Systematic Analysis for the Global Burden of Disease Study 2013.” The Lancet 384 (9947): 980–1004. http://www.thelancet.com/journals/lancet/article/PIIS0140-6736(14)60696-6/abstract. King, G., R. O. Keohane, and S. Verba. 1994. Designing Social Inquiry: Scientific Inference in Qualitative Research. Princeton University Press. http://amzn.to/1N7Hi0c. Leary, M. 2012. Introduction to Behavioral Research Methods. 6th ed. Pearson. http://amzn.to/1In1BDE. Lewis, Steff, and Mike Clarke. 2001. “Forest Plots: Trying to See the Wood and the Trees.” British Medical Journal 322 (7300): 1479. http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1120528/. Lindblade, Kim A, Dyson Mwandama, Themba Mzilahowa, Laura Steinhardt, John Gimnig, Monica Shah, Andy Bauleni, et al. 2015. “A Cohort Study of the Effectiveness of Insecticide-Treated Bed Nets to Prevent Malaria in an Area of Moderate Pyrethroid Resistance, Malawi.” Malaria Journal 14 (1): 31. http://www.malariajournal.com/content/14/1/31. Moorthy, V. S., and W. R. Ballou. 2009. “Immunological Mechanisms Underlying Protection Mediated by RTS, S: A Review of the Available Data.” Malaria Journal 312. http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2806383/. Noor, Abdisalan M, Judith A Omumbo, Abdinasir A Amin, Dejan Zurovac, and Robert W Snow. 2006. “Wealth, Mother’s Education and Physical Access as Determinants of Retail Sector Net Use in Rural Kenya.” Malaria Journal 5 (5): 5. http://www.malariajournal.com/content/5/1/5. Obala, Andrew A, Judith Nekesa Mangeni, Alyssa Platt, Daniel Aswa, Lucy Abel, Jane Namae, and Wendy Prudhomme O’Meara. 2015. “What Is Threatening the Effectiveness of Insecticide-Treated Bednets? A Case-Control Study of Environmental, Behavioral, and Physical Factors Associated with Prevention Failure.” PLOS ONE 10 (7): e0132778. http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0132778. O’Cathain, Alicia, KJ Thomas, SJ Drabble, Anne Rudolph, and Jenny Hewison. 2013. “What Can Qualitative Research Do for Randomised Controlled Trials? A Systematic Mapping Review.” BMJ Open 3 (6): e002889. http://bmjopen.bmj.com/content/3/6/e002889.full. Phillips-Howard, Penelope A, Bernard L Nahlen, Margarette S Kolczak, Allen W Hightower, FEIKO O TER KUILE, Jane A Alaii, John E Gimnig, et al. 2003. “Efficacy of Permethrin-Treated Bed Nets in the Prevention of Mortality in Young Children in an Area of High Perennial Malaria Transmission in Western Kenya.” The American Journal of Tropical Medicine and Hygiene 68 (4 suppl): 23–29. http://www.ajtmh.org/content/68/4_suppl/23.short. Radeva-Petrova, Denitsa, Kassoum Kayentao, Feiko O ter Kuile, David Sinclair, and Paul Garner. 2014. “Drugs for Preventing Malaria in Pregnant Women in Endemic Areas: Any Drug Regimen Versus Placebo or No Treatment.” Cochrane Database of Systematic Reviews 10. http://bit.ly/1U3q2Oj. Rossi, P. H., M. W. Lipsey, and H. E. Freeman. 2003. Evaluation: A Systematic Approach. Sage Publications. http://amzn.to/1TlIqoa. RTS,S Clinical Trials Partnership. 2015. “Efficacy and Safety of RTS, S/AS01 Malaria Vaccine with or Without a Booster Dose in Infants and Children in Africa: Final Results of a Phase 3, Individually Randomised, Controlled Trial.” The Lancet 386 (9988): 31–45. http://www.ncbi.nlm.nih.gov/pubmed/25913272. Sahoo, K. C., K. R. Hulland, B. A. Caruso, R. Swain, M. C. Freeman, P. Panigrahi, and R. Dreibelbis. 2015. “Sanitation-Related Psychosocial Stress: A Grounded Theory Study of Women Across the Life-Course in Odisha, India.” Social Science &amp; Medicine 139: 80–89. http://www.sciencedirect.com/science/article/pii/S0277953615300010. Scandurra, Leah, Angela Acosta, Hannah Koenker, Daniel Musoke Kibuuka, and Steven Harvey. 2014. “‘It Is About How the Net Looks’: A Qualitative Study of Perceptions and Practices Related to Mosquito Net Care and Repair in Two Districts in Eastern Uganda.” Malaria Journal 13 (1): 1–11. http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4301822/. Shadish, W. R., T. D. Cook, and D. T. Campbell. 2003. Experimental and Quasi-Experimental Designs for Generalized Causal Inference. Cengage Learning. http://amzn.to/1E8UYIG. Shulman, CE, EK Dorman, F Cutts, K Kawuondo, JN Bulmer, N Peshu, and K Marsh. 1999. “Intermittent Sulphadoxine-Pyrimethamine to Prevent Severe Anaemia Secondary to Malaria in Pregnancy: A Randomised Placebo-Controlled Trial.” The Lancet 353 (9153): 632–36. http://www.ncbi.nlm.nih.gov/pubmed/10030329. Singla, D. R., E. E. Kumbakumba, and F. E. Aboud. 2015. “Effects of a Parenting Intervention to Address Maternal Psychological Wellbeing and Child Development and Growth in Rural Uganda: A Community-Based, Cluster-Randomized Trial.” The Lancet Global Health 3 (8): e458–69. http://www.thelancet.com/journals/langlo/article/PIIS2214-109X(15)00099-6/abstract. White, H. 2009. Theory-Based Impact Evaluation: Principles and Practice. Working Paper 3. 3ie. http://www.3ieimpact.org/media/filer_public/2012/05/07/Working_Paper_3.pdf.      OK, Jimmy is not this kid’s real name, and his poster is not really about the observation that birds are everywhere. The internet had a little fun with Photoshop and re-imagined a number of science fair posters like little Jimmy’s.↩ But as we will see later, studies will not necessarily fit into one box. Often in global health research, you will see studies using mixed methods approaches and both types of reasoning.↩ This is also an example of causal inference, a major focus in this book. Does X impact Y? Does this program cause a specific outcome? As we will discuss later, this is also an example of statistical inference. The authors recruited one sample of adult-child dyads, collected data, and used inferential statistics to generalize from the sample to the population. Don’t worry, by the time you close this book, all these terms will make sense. Repetition, repetition, repetition.↩ Grounded theory will come up again in a later chapter as a specific example of approaches to qualitative inquiry. Without getting into the weeds here, we can just say that it is an approach that involves iterative data collection and analysis. Most importantly, data come first in grounded theory. It is only through the iterative process of data collection and analysis that theories and broader implications emerge.↩ Sahoo et al. (2015) is also an example of descriptive inference. Unlike with causal inference, descriptive inference does not seek to establish that X caused Y. Yet descriptive inference goes beyond basic description—or the collection of facts—to say something about how the individual experiences and opinions of these women tell us something more universal about the nature of sanitation-related stress and its possible connections to factors like a woman’s life stage and her behavior.↩ Sahoo et al. observed that “sanitation” encompassed much more than defecation and urination, such as washing, bathing, and menstrual management. These sanitation activities brought numerous challenges that could be classified as environmental, social, or sexual and understood in the context of a woman’s life stage, living environment, and access to sanitation facilities.↩ You might be wondering how they even come up with any estimates without much data. The answer is statistical modeling. Afghanistan may not have much data on maternal mortality, but there are data on other indicators like total fertility rate, gross domestic product per head, HIV seroprevalence, female education, etc. Using the data we do have from all countries and all years, we can model how these variables are related to maternal mortality. We take this equation, plug in values for Afghanistan, and solve for maternal mortality. More or less.↩ The World Health Organization (WHO) and partners published their own estimates for 2013. They estimated that there were 289,000 maternal deaths, which is pretty close to the IHME estimate of almost 293,000. As Kassebaum et al. (2014) explain, however, the consistency in these estimates masks substantial disagreements, including estimates that diverge at least 20% in 120 countries in 2013 and different perspectives on progress toward achieving the Millennium Development Goal 5.↩ Malaria is a preventable and curable disease that took the lives of more than 500,000 people in 2013—mostly African children. Four species of parasites cause malaria and are transmitted to humans through the bites of Anopheles mosquitoes.↩ Glossary of Terms for Human Subjects Protection and Inclusion Issues, based on the 1997 Report of the NIH Director’s Panel on Clinical Research, entry: “clinical research”. Available at http://grants.nih.gov/grants/peer/tree_glossary.pdf.↩ You might know Cronbach from that statistic you report for reliability but don’t really know what it means—Cronbach’s alpha.↩ The DHS Program runs several types of surveys, with the DHS surveys being the most well known. A DHS survey takes an average of 18-20 months to complete. Preliminary results are released about a month after the end of data collection, but it can take up to a year to release the final report and data. See here for more details about the DHS process: http://dhsprogram.com/What-We-Do/Survey-Process.cfm.↩ As we will discuss later, DHS surveys include enough people to be representative for different subgroups, such as urban and rural settings or wealth quintiles (the rich, the poor, and everyone in between).↩ KEMRI (n.d.). Kenya malaria fact sheet. Available at http://www.kemri.org/index.php/help-desk/search/diseases-a-conditions/29-malaria/113-kenya-malaria-fact-sheet.↩ For more information on summary tables, see here.↩ See the PRISMA Statement for a checklist of components to include in each section.↩ This is referred to as “hand searching”.↩ PubMed lets you limit results to humans or animals from the results page with one click, so it’s not essential to use Boolean operators manually in this case. PubMed will do it behind the scenes for you.↩ These 17 trials were described in 22 articles.↩  "]
]
