[
["index.html", "Global Health Research: Design and Methods Preface About this Book Organization Icons Acknowledgements Colophon", " Global Health Research: Design and Methods Eric P. Green 2016-09-23 Preface Does the world really need another book about research methods? I think so. But I spent a fair amount of time writing down the ideas in this book, so I’m biased. But here’s my rationale. I went to graduate school for clinical psychology, and my classmates and I read all of the classic psychology texts on research design and methods. Books like “Experimental and Quasi-Experimental Designs for Generalized Causal Inference” by Shadish, Cook, and Campbell (2003). I still remember staying up late trying to memorize all of the different threats to internal validity outlined by Donald Campbell and colleagues. Meanwhile, across campus, my econ colleagues were reading the ideas of another Donald—Donald Rubin and what is now known as Rubin’s causal model. But I didn’t know this at the time. When I set off for Uganda in 2007, determined to learn more about this field called global health, I met some of these mini Donald Rubin’s in the wild. I tried communicating with them, but they had a strange dialect that I couldn’t quite understand. And they did not understand me and my Campbellian drawl. We were usually trying to say the same thing, just in the language of our peoples. But I couldn’t put all of the blame on the economists and the disciplinary gap between us. There was a lot I didn’t know that went far beyond differences in jargon. I was a psychologist trained in clinical research, and nearly every applied example I read about came from the U.S. or Europe. The young field of global mental health was still an infant when I was in school. The first Lancet series on global mental health that really put the field on the map was published in September 2007 as I was getting on a plane to fly back home. I really knew nothing about global health. Fortunately, students entering university today have many more opportunities to learn about global health through interdisciplinary studies. Duke University launched the first liberal arts global health major in the U.S. in 2013, and other universities have followed suit. The Duke program is unique because it requires global health students to co-major in another discipline, such as biology, economics, psychology, or public policy. I started teaching at Duke around the time the new co-major started, and I found myself in the position of needing to pick a textbook for a course called “Research Methods in Global Health”. I reviewed a lot of excellent books that covered the basics, but none integrated examples from the very diverse and interdisciplinary field that is global health. I saw this as a real limitation. So I decided to write my own book. About this Book A guiding principle of this book is that a student of global health needs to be a student of medicine, biology, statistics, economics, psychology, public policy, and the list goes on. Just take a topic like malaria. A literature search will return articles about the spread of the disease (epidemiology), the impact of illness on future productivity (economics), the merits of free or subsidized bed nets (public policy), mosquito habitats (ecology), the efficacy of vaccines to prevent the disease (medicine and statistics), rapid diagnostic tests (biomedical engineering), the adoption and use of bed nets (psychology), and many others. No one book or author could ever hope to provide full disciplinary coverage of even one topic like malaria, so my goal was much more modest. I wanted to create a resource that would teach the basics of research design and methods by exposing readers to real world global health examples from different disciplines. Another guiding principle is openness. Whenever possible, the examples come from open access sources. Every reader should be able to access 90% of the references in this book. Organization The book is organized as follows. One objective of my course on global health research—and thus this book—is to make students better consumers of research. I wrote the first few chapters with this end in mind. Chapter 1 reviews the fundamentals of scientific research, but with a global health spin. In Chapter 2, you’ll learn how to search the literature for existing evidence. Better yet, you’ll learn to let someone else do that for you in the form of a systematic review or meta-analysis. You’ll also learn how to begin asking your own research questions. Chapter 3 on critical appraisal will teach you how to read and evaluate scientific evidence. One of the most important claims you’ll need to assess as a consumer of research is causality. Chapter 4 examines strategies for building up causal arguments. A second objective of my course is to give students the tools they need to begin careers as producers of research. The next few chapters lay this foundation. Chapter 5 gives you practical advice on developing a theory of change to guide program development, monitoring, and evaluation. It also helps to organize your approach to measurement, the topic of Chapter 6. This chapter explains how to define the measurement of your key study outcomes and covers fundamental psychometric concepts such as reliability and validity. The next three chapters address the fundamentals of data collection Chapter 7, sampling (Chapter 8), and power (Chapter 9). With the basics out of the way, we turn to designs that you can use to answer your research questions. Chapters 10 and 11 cover non-experimental designs. Chapter 10 focuses on the observational designs you typically find in epidemiology (e.g., cohort, case-control) and psychology (e.g., correlational). Chapter 11 introduces several “quasi-experimental” designs that manipulate a cause that comes before an effect, but without the benefit of randomization, the topic of Chapter 12. All of these chapters will refer back to the foundation in cause and effect we set in Chapter 4. The book concludes with several chapters to help you use your new knowledge to make an impact. In Chapter 13 you will learn how to prepare a study protocol. Chapter 14 will introduce you to the publication process and other opportunities for disseminating your work, such as professional conferences. Chapter 15 pushes you to think beyond your study results to make an impact on policy and practice. One limitation of this book is that it does not teach statistics. Statistical concepts are discussed throughout, but not in great detail. Every statistician will tell you that you need to think through your analysis at the study design stage. Listen to this advice. While this book will not give you the technical tools you need to plan your analysis, I hope you will come away with more appreciation for the gaps in your knowledge that you need to fill with further study. A great resource for learning applied statistics is OpenIntro Stats. If you are fortunate to be at a university with an applied statistics department, chances are you could get excellent consultation on your study protocol. This book WILL prepare you to narrow your options and have a smart conversation about how to meet your study objectives. Icons I’ve sprinkled several types of asides throughout the book: Help piecing together the global health puzzle Extended discussion of a special topic Tips Videos Acknowledgements I’d like to thank some folks for their helpful feedback at various points throughout my writing process. My graduate student teaching assistants, Kaitlin Saxton, Kathleen Perry, and Jenae Logan, read and commented on the initial drafts. This could not have been fun, so thanks! Thanks to Duke librarians Megan Von Isenburg and Hannah Rozear for setting me straight on literature searches. I still have a lot to learn! Liz Turner, biostatistican extraordinaire, kept me from making too many mistakes on technical details here and there. I’d also like to thank students in my undergraduate and graduate global health research courses for test driving the book before all the parts were in place. Your feedback was [placeholder], and the book would have been [placeholder] without you. Special shoutout to the following students for sharing written feedback: Kelsey Sumner, Karly Gregory, Qian Yudong, and Christina Schmidt. Despite everyone’s best efforts to help me catch mistakes, I’m certain errors remain in the book. My bad. Colophon This book is a work in progress. If you find errors (gasp!), please create an issue on Github, email me, or shame me on Twitter (@ericpgreen). I’m writing the book in R Markdown within RStudio. The bookdown package from the makers of RStudio does most of the heavy lifting to compile the book. The source code for the book is available on Github. References "],
["science.html", "1 Research 101 1.1 Scientific Research 1.2 The Fundamentals Share Feedback Test Yourself", " 1 Research 101 Believe it or not, you already know the basics of the research process. You probably have a yellowing, tri-fold piece of cardboard tucked in the back of the closet in your parents’ house that would prove me right. Just like Jimmy.1 Figure 1.1: What you see here is the scientific method in action. Jimmy asked a question, made a hypothesis, collected and analyzed data, and ultimately, made some conclusions based on the results. Science!. Source: http://bit.ly/1HbluM4 Even if you have not been as productive as Jimmy, I’m certain that you’ve had years of practice consuming research. We’re exposed to popular press accounts of research every day on TV, the radio, and the internet. Much of it might be wrong—“new study proves that eating chocolate prevents all cancer”—but it’s a start. So it’s a safe bet that just about every reader has some research foundation to build upon. The goal of this chapter, therefore, is to re-introduce familiar concepts about scientific research from a global health perspective. We’ll come back to these fundamentals throughout the book and explore them in more detail. By the end you’ll be ready to move your work from primary school cafeteria to academic conferences, policy debates, and real world program design and delivery. Chapter overview (PDF slide deck) 1.1 Scientific Research Let’s start with what we mean by “scientific research”. King et al. (1994) offer a useful definition in their book, “Designing Social Inquiry”. They point to several main characteristics: The goal is inference The procedures are public The conclusions are uncertain 1.1.1 All about inference By stating that the goal of scientific research is inference, we mean that science goes beyond the collection of facts. When we talk about inference, we are referring to the process of making conclusions about some unobserved or unmeasured phenomenon based on our direct observations of the world. We use what we know to infer something about the things we don’t know. This process can be deductive or inductive. In deductive reasoning, we start from general theories, make hypotheses, collect data, and make conclusions based on the data. Inductive reasoning flows the other direction, from specific observations to the generation of hypotheses and theories. Remember it this way: if you are testing a specific hypothesis, you are using deductive reasoning. If you are starting with your observations and making more general statements, then you are using inductive reasoning. To say that quantitative research is deductive and qualitative research is inductive is not quite right, but it’s often true.2 For instance, Singla et al. (2015) report the results of a cluster randomized trial of a parenting intervention in rural Uganda. This study used quantitative methods; the primary outcomes of this study were cognitive and receptive language development of the children of participating caregivers measured with the Bayley Scales of Infant Development. The authors hypothesized that the intervention would improve child development. As you can see in the following table from the article, the program increased cognitive and receptive language scores, but did not have an effect on height-for-age, thus partially supporting the hypothesis. Figure 1.2: Source: Singla et al. (2015), http://bit.ly/1UcVtoZ Later in the book we’ll get into the nitty gritty details of how you read and interpret results like you see here. For now, let’s focus on the approach to reasoning. Singla et al. is an example of deductive reasoning. The authors started with a hypothesis, collected quantitative data (i.e., scores on a measure called the Bayley), and inferred something about the impact of the intervention.3 We can contrast the Singla et al. trial with a qualitative study by Sahoo et al. (2015) that exemplifies inductive reasoning. Sahoo and colleagues used a grounded theory approach to conduct and analyze interviews with 56 women in Odisha, India about their sources of stress and sanitation practices.4 This study is an example of inductive reasoning because the authors started with the data—their observations—looked for themes and patterns, and came to some conclusions about the nature of sanitation-related stress.5 One result of this work was a conceptual framework for thinking about sanitation-related psychosocial stress, as shown below.6 Figure 1.3: Source: Sahoo et al. (2015), http://bit.ly/1JB6nSs The point to take away about inference is that, regardless of the approach to reasoning, the goal of scientific research is to use what we observe to make conclusions about what we do not or cannot observe directly. This is sometimes referred to as empiricism, and our systematic observations as empirical evidence. Empiricism is at the heart of scientific research. 1.1.2 Research as a public act Scientific research uses public methods that can be examined and replicated. Replication is a core principle of scientific research. No one study rules the day. If the results of your study are robust, another research group should be able to follow your methods and replicate the findings. When findings are replicated, we all have more confidence in the results. Replications are relatively rare, however. For one, there are often few resources for replicating studies, especially when it comes to big field experiments. Second, journal space is limited (especially if there is still a print version) and peer review takes a lot of resources. Journals want to use their space and resources to publish novel ideas (ironically, novelty can sometimes mean small effects with a lot of noise that might fail to replicate). Without the promise of a publication, researchers have little incentive to spend time and money trying to replicate published findings. Publications are a key criterion for tenure and promotion in academia, so many researchers don’t waste their efforts on studies that won’t get published. What happens when replications are attempted? Well, that’s a topic for a later chapter. The short answer is bitterness. Replicators grab more headlines when they “debunk” findings, and the original authors almost invariably call into question the quality of the replication. Just see #wormwars to learn what happened when a famous de-worming study was re-examined. Or Google social psychology and priming. Yikes! A separate but related issue is reproducibility, the ability to generate a study’s findings given the original dataset and sometimes the original analysis code. Think irreproducible findings are rare? Think again. The Quarterly Journal of Political Science found that slightly more than half of their published empirical papers subjected to review had results that could not be replicated with the author’s own code. The positive part of this story is that it’s becoming more common for authors to share their data and analysis code. This has been standard practice in economics for some time, but the idea is revolutionary in medicine and public health. We’ll explore why this is so important and easier than ever to do. 1.1.3 Living with uncertainty Every method has limitations, every measurement has error, and every model is wrong to some extent. In short, research is an imperfect process. Sometimes researchers make outright mistakes. These mistakes may or may not be detected and corrected in the peer review process, or during post-publication review if authors share their data and analysis code. Other findings are free of obvious mistakes, but fail to be replicated, and over time run counter to a growing body of literature that points in the other direction. In this way science is said to be self-correcting. We’ll discuss how this ideal can fall short in the face of challenges like publication bias, but the point here is to get comfortable in the short term with the idea of uncertainty. A good example of uncertainty comes from the estimation of maternal mortality. Hogan et al. (2010) published estimates for 181 countries. Some countries like the United States have vast amounts of data; vital registries that attempt to track all births and deaths. Countries with vital registries struggle with changing definitions over time, but the uncertainty interval around their estimates is typically tight, as shown in the figure below from the Hogan et al.’s supplementary webappendix, because there is a lot of good data. Figure 1.4: Source: Hogan et al. (2010), http://bit.ly/1JBCelO In many low-income countries the situation is very different. Here is the estimate and uncertainly interval for maternal mortality in Afghanistan. There are only four data points! No wonder the uncertainty interval is so great.7 Figure 1.5: Source: Hogan et al. (2010), http://bit.ly/1JBCelO So how many women die during pregnancy or within 42 days of delivery? The same research group that published Hogan et al., the Institute for Health Metrics and Evaluation, estimated that there were 292,982 maternal deaths globally in 2013, with a 95% uncertainty interval ranging from 261,017 to 327,792; that’s a range of 66,775 for everyone who struggles with mental math (Kassebaum et al. 2014). This might seem like a lot, but remember that we’re talking about global statistics for a world population of more than 7 billion people.8 The takeaway message is that there is uncertainty in everything. Don’t take any single estimate as the “Truth”. Instead, try to learn about the origin of estimates and recognize the limitations of what we know. 1.2 The Fundamentals Before we get too far along, we need to establish a common understanding of some fundamental concepts and terms. We’ll do so in the context of research on malaria.9 1.2.1 The nature of research Research can be classified as basic or applied. Basic research—or “pure” research—is the pursuit of fundamental knowledge of phenomena. An example would be the bench science to understand the parasitic life cycle and how parasites interact with humans at different stages. Basic research can be contrasted with applied research which is focused on specific problems or applications. For instance, an applied research question is how to increase the coverage and use of bed nets that prevent malaria transmission. Applied science takes many different forms, including clinical research. Clinical research is a broad field that encompasses patient-oriented research, epidemiological and behavioral studies, and outcomes research and health services research.10 Basic research is the foundation of clinical research. Clinical trials One type of clinical research is a clinical trial. Drugs and vaccines have to pass through different phases of clinical trials before regulatory bodies will approve their use with humans: Preclinical research Phase I Phase II Phase III Phase IV Let’s take the development of a vaccine for malaria as an example of the clinical trial life-cycle. A vaccine candidate called RTS,S, or Mosquirix™, recently made news for getting one step closer to becoming a licensed vaccine after a successful Phase III trial. This moment was more than 30 years in the making. Development of RTS,S began in 1984 through a partnership between the pharmaceutical company GSK and the Walter Reed Army Institute of Research. The vaccine candidate was created in 1987 and entered preclinical research. During the pre-clinical phase, testing is performed in non-human subjects with the goal of collecting data on how well the vaccine works (efficacy), how much damage it can do to an organism (toxicity), and how it is affected by the body (pharmacokinetics). Clinical research on humans began in 1992. To obtain regulatory approval, the vaccine had to complete three phases of testing. Doherty et al. (1999) conducted a Phase I safety and immunogenicity trial with 20 adults in The Gambia in 1997. This small sample size is typical of Phase I trials where the objective is usually to find a safe dosing range and look for side effects. The authors reported that the vaccine did not have any significant toxicity but did produce the expected antibodies. Several Phase II studies conducted over a decade (Phase IIa and Phase IIb) demonstrated efficacy of the vaccine against several endpoints (a.k.a. outcomes) (Moorthy and Ballou 2009). A Phase IIb trial began in Mozambique in 2003 with more than 2,000 children aged 1 to 4 (Alonso et al. 2004). Children were randomly assigned to receive three doses of RTS,S or a control vaccine. At 6-months, the prevalence of malaria was 37% lower in the treatment group compared to the control group. A follow-up study with 214 infants also showed partial protection (Aponte et al. 2007). This Phase II trial was an important proof-of-concept. Final results of a large Phase III trial with more than 15,000 infants and young children in seven African countries were published in The Lancet in 2015 (RTS,S Clinical Trials Partnership 2015). Children in the study were randomly assigned to 1 of 3 arms: 3 doses of RTS,S and a booster dose at month 20; 3 doses of RTS,S and a booster dose of a comparator vaccine at month 20; or 4 doses of a comparator vaccine. The study reported that RTS,S reduced clinical malaria cases by 28% and 18% among young children and infants, respectively, over a 3 to 4 year period. This is the goal of a Phase III trial—to show that a treatment is efficacious. On the basis of these results, the European Medicines Agency issued a “European scientific opinion”, which could help inform the decision of the WHO and African national regulatory authorities. If RTS,S is approved for use and eventually hits the market, researchers will likely conduct Phase IV trials to evaluate the vaccine’s long-term effects. This will not be the end for research on RTS,S, however. The vaccine may be efficacious, but that does not mean it will be easy or cost-effective to deliver at scale to millions. Studies that assess how to best get efficacious treatments to the people who need it most fall under the domain of implementation science. There are many stumbling blocks from getting interventions from “bench to bedside”, so to speak. Practitioners of translational research point to four key bottlenecks: T1: translation from basic science to clinical research T2: translation from early clinical trials to Phase III trials and beyond with larger patient populations T3: translation from efficacy (Phase III) to real-world effectiveness—the domain of implementation science T4: translation from evidence about delivery at scale to new policy Behavioral research (e.g., development and evaluation of parenting interventions) does not follow the same exact phases of vaccine and drug development, but the broad principles are the same. Monitoring and evaluation Another arena of applied work in global health is monitoring and evaluation, or M&amp;E. Let’s start with the “E”, program evaluation. In the U.S., program evaluation became commonplace by the end of the 1950s and grew dramatically in the 1960s as the federal government expanded and introduced new social programs. Lawmakers wanted accountability, and the evaluation of social programs took off (Rossi, Lipsey, and Freeman 2003). But is program evaluation considered research? Methods giant Donald Campbell thought so (Campbell 1969): The United States and other modern nations should be ready for an experimental approach to social reform, an approach in which we try out new programs designed to cure specific problems, in which we learn whether or not these programs are effective, and in which we retain, imitate, modify or discard them on the basis of their effectiveness on the multiple imperfect criteria available. Campbell had an outsized impact on the field. It’s no surprise that an organization dedicated to synthesizing the best available evidence on social interventions, the Campbell Collaboration, bears his name. A good candidate for Donald Campbell’s successor is French economist and MIT researcher Esther Duflo. Together with Abhijit Banerjee and Sendhil Mullainathan, she co-founded JPAL, which stands for the Abdul Latif Jameel Poverty Action Lab. JPAL is a global research organization headquartered at MIT that uses randomized evaluations (i.e., experiments) to answer policy questions related to poverty alleviation. The JPAL website, http://www.povertyactionlab.org, contains excellent resources about the methods of randomized evaluations, published studies, and policy briefs. Interested readers should also check out Innovations for Poverty Action, or IPA, a sister organization of sorts that is also a leader in the use of randomized evaluations to study important policy questions about global poverty. Yet, not everyone agrees. Educational psychologist Lee Cronbach, for one, certainly did not.11 Cronbach thought that program evaluation is really designed for program implementers and funders, and that the messy nature of programs required a loosening of research standards (Cronbach 1982). The goal is just to learn what you can. In their introductory text on evaluation, Rossi et al. (2003) strike a balance in views. Their answer is perhaps a bit unsatisfying, but I’d argue true nevertheless. It depends. Program evaluations should be as rigorous as logistics, ethics, politics, and resources permit. And no less. Is there a lower bound in terms of quality that should limit what is even worth doing? Probably, but the line is so context dependent that it is not sensible to attempt a definition. If there is one rule to follow, I’d suggest that it’s this: “don’t go beyond the data”. Everyone wants to claim “impact”, but not every evaluation can based on the design and implementation. Now we turn to the “M”, program monitoring. Program monitoring is concerned with the implementation of programs, policies, or interventions. How are resources being used? Is the program being delivered as intended (a.k.a. with fidelity)? How many people participate, and does the program reach the intended targets? These are all program monitoring questions. Accurate monitoring is essential for reporting to funders, but it’s also essential for all good evaluations. The reason is simple. If a program is shown to not “work”—to have no impact—the next question is why? Did the program fail to have an impact because the idea or theory behind the program was wrong (theory failure)? Or was it the case that the implementation of the program was so troubled that there was never a chance of having an impact (implementation failure)? Every trial should include ongoing monitoring or a formal process evaluation. 1.2.2 Research problems and questions Every study begins with a research problem. A research problem represents a gap in our knowledge. In academic research, this is another way of saying a gap in “the literature”. Usually when people speak of “the literature”, they mean scholarly or peer-reviewed journal articles. There is also something called “grey literature” that is more encompassing and harder to search systematically. Grey literature sources are typically disseminated through channels other than peer-reviewed journals. Examples could include technical reports or white papers published on the web. Research problems are typically broad. For instance, stakeholders might want to know how to increase the use of bed nets for children under 5 years of age. Or whether all children should receive deworming medication prophylactically. Stakeholders can refer to a wide range of people and organizations. Typically we mean donors (i.e., the public and private organizations that fund research and programs), policy makers (i.e., government officials and bureaucrats at international bodies like the WHO), program implementers (i.e., organizations like Doctors Without Borders that actually deliver services to beneficiaries, a.k.a. people), and scholars who study the topic or policy issue. These problems have something in common: they are solvable. In his introductory text on behavioral research methods, Leary (2012) writes that this is another key criterion for scientific research. The problems must be solvable. This does not mean easy; it just means that we can use systematic, public methods to gather and analyze data on the problem. Think of it this way: we can come up with a method for studying how to get more parents to ensure that their kids sleep under a mosquito net every night, but we don’t yet have a scientific method for determining whether there is a mosquito afterlife where these pests get to buzz around for all of eternity. In order to study a broad research problem, we must narrow to a more specific research question. de Vaus (2001) says there are essentially two types of research questions: Descriptive—what is going on? Explanatory—why is it going on? Let’s stick with the bed net example. If we want to study uptake or use of bed nets, we might ask a descriptive research question like, “How many children sleep under bed nets?” But this is too general. Children of what age? Living where? We also need to operationalize what we mean by sleeping under a bed net. It’s common in this line of research to ask about the previous night, as in the night before the survey. As we will discuss in the chapter on measurement, we have to consider challenges to getting valid information, such as recall difficulties. A better way to phrase the question might be, “What percentage of children under 5 years of age in Kenya slept under an insecticide treated net the previous night?” 1.2.3 Research designs As Glennerster and Takavarasha (2013) explain in their excellent practical guide to running randomized evaluations, different research questions require different research designs. descriptive (what) explanatory (why) correlational quasi-experimental experimental Descriptive research The goal of descriptive research is to characterize the population. Often this means estimating the prevalence of a phenomenon or disease. 20% are illiterate. 36% have an unmet need for contraception. 9% are HIV positive. Description can also be qualitative in nature (e.g., thick description). Just about every study will have some descriptive element. Some studies are exclusively descriptive. A good example are the Demographic and Health Surveys, more commonly referred to as DHS surveys (yes, “surveys” is redundant). Every student of global health should come to know what the DHS Program has to offer. The program is funded by the U.S. Agency for International Development (USAID), and registered users can request access to data from more than 300 surveys conducted in 90+ countries. DHS surveys are a good example of demographic research. Demographers contribute to and use data sources like DHS surveys and national population and housing censuses to understand more about population size, structure, and change (e.g., birth, death, migration, marriage, employment, education). Many countries strive to conduct a census, or an enumeration of all citizens, every 10 years. The United Nations Statistics Division and the United Nations Population Fund (UNFPA) provide technical support (a.k.a., help) to countries preparing for, conducting, and analyzing a national population and housing census. These two organizations, in partnership with the United Nations Children’s Fund (UNICEF), maintain CensusInfo, a database of global census data. Here is the relevant table from the 2014 Kenya DHS Key Indicators Report for describing the prevalence of ITN use.12 This is a typical DHS cross tabulation (or crosstab) of the results. In this example, the percentage of children under the age of 5 that slept under an insecticide treated net the previous night in Kenya was 54.1%. This descriptive data is further disaggregated by residence and wealth quintile as is typical for DHS tables.13 Figure 1.6: Source: Kenya 2014 DHS Key Indicators Report, http://bit.ly/1g4NYS5 The data summarized in this table describes the problem of bed net use. Descriptive questions are well-suited for needs assessments. Before we can design a program or policy to increase bed net usage, for instance, we must to understand the need. In Kenya, almost half of children under 5 are not sleeping under insecticide treated nets according to the DHS. This is a particular concern for children living in areas of high risk. This DHS report is an example of a cross-sectional study. These are typically one-off surveys but can include other forms of data collection. The key is that it’s a snapshot. The goal is often description but might also include correlation. Cross-sectional studies are differentiated from panel or longitudinal studies by their participants; the latter include the same research participants (sample) over time in multiple studies, whereas cross-sectional studies only include a particular sample once. So even though the DHS Program will conduct a new survey in a country every five years or so, they always recruit a new sample of participants (a.k.a. “successive independent samples”). This makes the DHS surveys cross-sectional rather than panel or longitudinal in design. Correlational research This descriptive information sheds light on programmatic and policy priorities, but we have to go beyond describing the problem to make a difference. A helpful next step is often to build on descriptive insights by attempting to predict or explain the behavior or phenomenon. For instance, Noor et al. (2006) asked a correlational research question (edited below) about the factors associated with net use among children under the age of 5: Are wealth, mother’s education, and physical access to markets associated with the use of nets purchased from the retail sector among rural children under five years of age in four districts in Kenya? Correlational research asks questions about the relationship (a.k.a. association) between two or more variables. In this case, ITN use and a variety of potentially influential factors, such as household wealth and a mother’s education level. You’ll encounter different labels for variables, and it can be confusing to keep straight at first. The behavior Noor et al. are trying to predict, bed net use, is the dependent variable, often referred to as the outcome, response variable, or simply Y. The factors thought to be related to ITN use are the independent variables. Some disciplines will call them predictor or explanatory variables. Sometimes they are controls, exposures, or simply X. This will become more clear as we go. Noor and colleagues reported that only 15% of children in the rural study sample slept under a net the previous night—a much lower percentage than the national prevalence reported by recent DHS surveys. As shown in the table below, they also found that several factors were associated with higher odds of bed net use, including: greater household wealth, living closer to a market center, not having older children present in the household, having a mother who is married and not pregnant, being younger than 1 year old, and having an immunization card. Figure 1.7: Source: Noor et al. (2006), http://bit.ly/1HoltVo We’ll review in detail how to read tables like this in later chapters, but it might be helpful to preview some concepts here. The results in this table come from a multivariable logistic regression. The authors describe the model they fit in the article, but we won’t worry ourselves with these details. Instead, let’s highlight a few key points. The table starts with household-level predictors of net use, specifically household wealth quintile. The first two columns show us the number and percentages of households that have or do not have retail sector nets, disaggregated by wealth quintile. So of all the households without retail sector nets, 690 or 21.6% were classified as being the most poor. The distribution looks a bit different for households with nets; only 5.5% of households with retail sector nets were classified as being the most poor. This makes intuitive sense: if you are poor, you are less likely to purchase a net from the market. The next three columns give us the results of the regression. Since wealth quintiles have different categories, the authors set one category—the most poor—to be the reference category. So the results will be relative to the poorest households. As you can see, the odds of using a net are 10.17 higher among the “least poor” compared to the “most poor”. This does not tell us why wealthier households are more likely to use nets for their young kids, but we know that there is some relationship here. Remember, however, that 10.17 is just what we call a point estimate for the odds ratio. The 95% confidence interval ranges from 5.45 to 18.98. We’ll talk more about the specifics as we go, but it’s important to get in the habit of evaluating the uncertainty of every estimate. In this case, it’s pretty clear that having more money is associated with better preventive behaviors. You will often see descriptive and correlational studies like the DHS and Noor et al. classified as non-experimental or observational studies. Other observational designs include cohort and case-control studies (the topic of a later chapter). Researchers use these designs to determine whether there is an association between some exposure and a disease. Observational studies are the bread and butter of epidemiology. Epidemiologists often conduct cross-sectional studies to estimate the prevalence and incidence of different disorders as well as correlational research to understand risk and protective factors. Prospective cohort In a prospective cohort study, healthy participants are recruited and followed into the future for a period of time. For instance, Lindblade and colleagues (2015) conducted a prospective cohort study in Malawi to test the efficacy of ITNs in an area of moderate resistance to pyrethroids, a common class of insecticide. A prospective cohort of 1,199 healthy children aged 6-59 months was followed for one year. This group of children make up the cohort, and the fact that they were recruited and followed for a period of time into the future makes the design prospective. Compared to no bed nets, ITNs reduced the incidence of malaria infection by 30%. This is promising, but the study design has limitations. One important limitation is that the children were not randomized to ITN access. So it could be the case that children who used the ITNs were somehow different from the children who did not use the ITNs. This is a potential selection bias, a threat to internal validity. You’ll learn more about such threats in a later chapter. The basic challenge for causal inference is that the design does not rule out the possibility that something other than ITN use accounted for the reductions in malaria infections. Case-control Sometimes it is not possible to recruit a group of healthy people and wait to see who gets sick. Imagine having to wait a decade or more to see who develops rare diseases like gliomas. This would be a very expensive study that would need to involve thousands of people to study such a rare disease that takes time to emerge. A case-control study might be a better fit. In this design, researchers identify people with the disease (cases) and without the disease (controls) and ask them about past exposures. Obala et al. (2015) did this in Kenya with 442 children hospitalized with malaria and healthy matched controls without evidence of malaria. They wanted to know why there is a high malaria burden despite high ITN coverage. The research team visited visited the home of each case and control and asked questions about ITN coverage and recent use, along with measuring the parasite burden of family members, mapping nearby potential vector breeding sites, and assessing neighborhood ITN coverage. Obala and colleagues found that ITN coverage was not correlated with hospitalizations, but consistent ITN use decreased the odds of hospitalizations by more than 70%. As with prospective cohort designs, there is a risk of selection bias. In this case, we have to be concerned that the matching process was not perfect. The matching was done on the basis of age, gender, and village. But there could be unmeasured ways in which the cases and controls differ, which would undermine the results. Correlational studies can yield important insights of course, but they have limitations. You’ve probably heard that correlation does not equal causation. For instance, did you know there is a nearly perfect correlation between the per capita consumption of cheese and the number of people who have died by becoming tangled in their bedsheets? (If you just put down the hunk of aged cheddar you were eating, please keep reading this book!) That said, all studies have limitations and tradeoffs. Designing a good study is a process of weighing scientific objectives with logistical constraints, ethical considerations, time, money, and a host of other factors. Keep reading to learn more about how to make these tough calls. Experimental and quasi-experimental research Without a doubt, the correlational results described in studies like Noor et al. (2006) can help to design programs and policies. But what we often want to know is whether our programs and policies “work”. When we ask whether something works, it’s a question of impact, and impact evaluations use experimental or quasi-experimental research designs. (Experimental and quasi-experimental are addressed in more detail in later chapters) The goal of impact evaluations is causal inference. Does X cause Y? Does a particular program or intervention or treatment increase or decrease a particular outcome? For reasons we will explore in greater detail later, experimental research designs offer the cleanest estimate of impact. The hallmark of an experimental design is that we as researchers manipulate some independent variable and examine changes to some dependent variable that result. A common example in global health is the randomized controlled trial (RCT) in which some units, such as individuals, schools, or communities, are randomly assigned to receive an intervention (treatment) or not (control). We measure an outcome after the intervention period and estimate the average difference between the two study arms, also known as the average treatment effect. Experiments are the “gold standard” in the eyes of many people, but researchers are not always able to assign people or clusters to study arms or otherwise manipulate an independent variable. Logistics and ethics can get in the way. In these cases, researchers might rely on non-experimental designs commonly referred to as “quasi-experimental” designs. The name of the game in quasi-experimental research is to reduce threats to internal validity, something that randomization pretty much takes care of naturally. Beware: not all non-experimental designs are created equal. We’ll discuss several of them later in this book, including: pre-post post-test only difference-in-differences multivariate regression and matching regression discontinuity instrumental variables interrupted time series An important global health policy question that has been studied using experimental and quasi-experimental methods is the impact of user fees on the adoption of health goods, such as bed nets. Advocates of fees argue that free distribution is not sustainable and leads to waste when people who don’t need or want the goods are recipients. There is also an argument that people only value what they pay for, so removing fees will make people less likely to use goods like bed nets. The flip side is that the provision of some health goods, in economics-speak, creates “positive externalities” and should therefore be financed with public dollars. What this means is that some interventions have spillover effects whereby people who are not treated still experience some indirect effect. A good example of a spillover effect is vaccines and the resulting herd immunity. Hawley et al. (2003) showed a similar protective effect of ITN use on child mortality and other malaria-related outcomes among households without ITNs that were located within 300 meters of households with ITNs. So we know that there is evidence that ITNs have direct (Phillips-Howard et al. 2003) and indirect benefits. The research problem is then how to increase coverage and use of nets. Is free distribution the best strategy, or should users have to spend something to get a bed net that might retail for a price that is out of reach for many poor households? Quasi-experimental Agha et al. (2007) used a quasi-experimental design to estimate the impact of a social marketing intervention on ownership and use of ITNs in rural Zambia. Nets that commonly sold for USD $27 were subsidized and sold for $2.50 at public health clinics. Neighborhood health committees were established and 600 volunteer “promoters” were trained to teach residents about malaria and encourage them to purchase the nets. To estimate the impact of the intervention, the authors analyzed data from post-intervention surveys in three intervention and two comparison districts. This study design was quasi-experimental because the districts were not randomized to the intervention or control arms. Figure 1.8: Source: Agha et al. (2007), http://bit.ly/1MkO5a0 Agha and colleagues reported that ITN ownership and use was higher in intervention districts according to the post-intervention data, but were careful to avoid going ‘beyond the data’ to claim evidence of a causal relationship. There are several design limitations to consider here, and you will learn more about how to spot these issues as we go. Briefly, we can note that (i) the authors did not randomize districts to study arms and (ii) no baseline (a.k.a. pre-treatment) data was collected. Experimental studies benefit from but do not require baseline (or pre-intervention) data because randomization usually ensures that the treatment and comparison groups are similar at the start—if enough units are randomized. But a non-randomized study like this leaves itself open to criticism without baseline data to show that the intervention and comparison districts were similar before the intervention was introduced. The results suggest that they were different after the intervention period, but we can’t be sure this was caused by the intervention itself. Given the limitations, how should we view the results? If this was one of the first studies on the topic, we would view it as a starting point that would encourage more rigorous investigations. As part of a larger body of evidence, however, it would probably be passed over in systematic reviews and meta-analyses—studies of studies—because of the limitations of the design for causal inference. Experimental Another limitation of Agha et al. (2007), at least for our purposes, is that it does not provide a direct answer to our policy question: should ITNs be free or subsidized? Fortunately, other studies fill this gap. Cohen and Dupas (2010) used an experimental design to study this question in Kenya where malaria is the leading cause14 of morbidity and mortality. The authors randomly assigned 20 prenatal clinics in an endemic region to 1 of 5 groups: a control group that did not distribute ITNs, a free distribution group, a group that charged 10 Ksh per ITN (97.5% subsidy), a group that charged 20 Ksh (95% subsidy), and a group that charged 40 Ksh or about $0.60 USD (90% subsidy). When units like clinics, schools, and villages are randomized, we refer to the design as a cluster-randomized trial, or CRT. The authors followed up a subset of pregnant women over time and found that those who paid a subsidized price were no more likely to use the bed nets than women who received one for free. They also found that the increase in price from $0 to $0.60 USD reduced demand for ITNs by 60%. This implies that the cost-sharing model of having women pay something for ITNs will reduce coverage. This is bad for the women who forgo a net purchase because of the direct prevention effects of ITNs, but we know from Hawley et al.’s work that it’s also bad for the community since ITNs have spillover effects. Cohen and Dupas conclude that free distribution would ultimately save more child lives. 1.2.4 Research methods If research designs are strategies for answering research questions with the best possible evidence, then research methods are the tactics for obtaining the evidence. Often methods are divided into three broad categories: quantitative qualitative mixed Quantitative methods are used to collect and analyze numerical data. This includes binary or dichotomous** data (e.g., hospitalized or not), categorical data (e.g., wealth quintile), and continuous data (e.g., hematocrit). A good example of a quantitative method is a survey in which people are asked to answer questions with fixed response options or provide numerical values, such as their monthly income. Lab tests resulting in disease classifications (yes/no) or a measurement such as the number of blood cells in a sample of blood are also examples of quantitative methods. Qualitative methods focus on non-numerical data. Participant observation, interviews, and focus group discussion are common qualitative methods in global health. Qualitative methods are well-suited for obtaining thick description and for exploration. For instance, Scandurra et al. (2014) analyzed data from interviews, observations, photos, and videos to study perceptions and practices related to bed net care and repair in Uganda. As is typical of manuscripts based on qualitative data, the authors include illustrative quotes, such as this one regarding net repair from a 55 year-old female: [It] depends on one’s situation. If you have money, there is no need of sewing a net, you just buy a new one but if you are poor, you have to do it. So this is when you are poor. Scandurra et al. found that there are strong social norms around net hygiene and appearance. Dirt floors and indoor cooking with dirty fuel sources and little ventilation tarnish the look of nets, and as a result, nets get washed frequently and may reach their lifetime wash limit much sooner than commonly assumed. If true, this could have implications for preventive efficacy. Often qualitative methods are seen as being less rigorous because they are more flexible and do not lead to the same type of hypothesis testing and results compared to quantitative methods. But this is not true. As we’ll discuss in a later chapter, rigor is a characteristic of how the methods are applied rather than the methods themselves. Your choice of methods should be based on your research question. It’s often the case that impact evaluations use quantitative methods, but there is not a 1-to-1 match between research designs and methods. Many studies incorporate both quantitative and qualitative methods, and we refer to this as mixed methods. Sometimes the goal of mixing methods is triangulation of results with respect to the same research question. Other times we begin with qualitative work to develop the tools and measures that we will use in a trial. When qualitative work follows a quantitative phase, the goal is often to explain or explore results in more depth that was not possible with the quantitative data. Increasingly you will see RCTs complement their use of quantitative methods with qualitative inquiry (O’Cathain et al. 2013). Alaii et al. (2003) provide a good example. The authors of this paper incorporated qualitative interviews on non-adherence into a larger randomized trial of the efficacy of ITNs on child morbidity and mortality in Kenya (Phillips-Howard et al. 2003). They wanted to better understand why people, particularly children under the age of 5, were not using their ITNs correctly. Alaii et al. found that more than a quarter of individuals were non-adherent, often due to excessive heat. 1.2.5 Theories and hypotheses Many impact evaluations fit the label of black box evaluations, meaning that they don’t focus on why programs do or don’t have an impact. The evaluation is not guided by theory, and the hypotheses are as simple as “the program will have an impact on the outcome”. White (2009) outlines a strategy for changing this and moving to theory-based impact evaluations (White 2009). Leary (2012) defines a theory as “a set of propositions that attempts to explain the relationships among a set of concepts”. In quantitative research, you could replace “propositions” with “hypotheses” and “concepts” with “variables”. As we reviewed earlier, the logical approach in quantitative research is often deductive. You start with theory and develop research hypotheses that are then tested. A hypothesis is an a priori prediction about what will occur—about how constructs are related. If the hypothesis is supported by the data, you have support for the underlying theory. If your study is well designed, it might be given more weight as other researchers consider the evidence in support of the theory. In theory testing, 1+1 does not always equal 2. For a hypothesis to be scientific it should be falsifiable, or testable. To return to a silly example from earlier, the following would not be a research hypothesis because it cannot be tested: “if a mosquito is killed, it goes to mosquito heaven”. Maybe, but we can’t test this hypothesis. Science progresses through the possibility of falsification, so hypotheses must be engineered to potentially fail. Proving and disproving theories To return to an earlier example, some people advocate against the free distribution of ITNs out of the belief that there is a “sunk cost” effect when having to spend money for a bed net; people will use the net more to justify their purchase (Arkes and Blumer 1985). In this case, the theory is one of sunk costs directing behavior. The falsifiable hypothesis tested by Cohen and Dupas (2010) was that people who paid a non-zero price for an ITN would use the ITN more than those who received the ITN for free. As you will recall from our discussion, there was not support for this hypothesis. So the theory is rejected, right? Not necessarily. Leary (2012) offers some helpful advice for thinking about proof and disproof. Proof is logically impossible, whereas disproof is practically impossible. Frustrating, right? Proof is not possible It helps to state the theory and hypothesis as an if-then statement. For example, “If the theory of sunk cost effects is true, then people who pay for an ITN will be more likely to use it than people who get an ITN for free.” If the theory is true, the hypothesis will be true. What happens if you flip this statement? If you find evidence that the hypothesis is true—as you might in a study—does it mean that the theory is true? Cohen and Dupas (2010) did not find support for the hypothesis that people who paid a non-zero price for an ITN would use the ITN more than those who received the ITN for free. But let’s pretend for a moment that they did. Would that prove the sunk cost theory? No, logically it can’t. It would be like concluding that my raging fever is malaria because I have mosquito bites all over my arm. The “theory” here would be that my fever is malaria, and the hypothesis would be that I must have been bitten by a mosquito. If I have mosquito bites all over, my fever must be malaria, right? Well, no. I was bitten by a mosquito, but maybe the scene of the crime was my backyard in the eastern United States where we don’t worry about malaria. So in this case, the hypothesis was true, but it doesn’t prove the theory. Disproof is possible, but uncommon What if the hypothesis was not supported, and I was not bitten by mosquitos? Could my “theory” be true—could my fever be malaria? No. And logic would support this. If the hypothesis is derived from the theory, and if the hypothesis is not supported, the logical inference is that the theory is wrong. Yet, we still shy away from concluding that the theory is wrong. The reason is simple: complexity. A study like Cohen and Dupas (2010) could fail to reject the null hypothesis that use does not differ between free and subsidized clients—thus not supporting the hypothesis of different use rates—but there are many practical reasons for this. For instance, maybe their measure of bed net use was systematically flawed and hid the difference as a result. The possibilities are endless. This is partly why journals are hesitant to publish null results. Science marches on So, where do we go from here? Answer: the literature! No one study is enough to lead people to discard a theory. But several null results might be. Conversely, no study ever proves a theory, but an accumulation of studies showing support for the theory-derived hypothesis builds confidence in the theory. Particularly when the studies are conducted by different researchers, across different populations, and triangulating with multiple methods. Of course you see the challenge here. How do researchers know that several studies have failed to support a certain theory if journals are reluctant to publish null results? And if negative evidence is missing, won’t the positive evidence be over-represented in the literature? Yes. This is the problem of publication bias, or the file drawer problem, and there is not an easy answer. Efforts like AllTrials to register and report the results of all trials, regardless of outcome, seem like a step in the right direction. So, to the literature we go. Share Feedback This book is a work in progress. You’d be doing me a big favor by taking a moment to tell me what you think about this chapter. Test Yourself References "],
["literature.html", "2 Searching the Literature 2.1 Start with Systematic Reviews and Meta-Analyses 2.2 Devising a Search Strategy 2.3 For the Love of Everything Holy Use a Reference Manager Share Feedback Test Yourself", " 2 Searching the Literature The starting point of every research study is a literature review. To know where you are going, you need to know where the field has been. Technology makes this easier in some ways than it has been in the past, but we’re swimming in information, and the pool gets deeper every day. A lot deeper, actually. Google’s former CEO Eric Schmidt has said we create as much information every two days as we did from the beginning of time through 2003. Two days! And he said this back in 2012, so it’s an even shorter time span today. Of course the “cat photo” to “research finding” ratio is probably something like 1,000,000:1 nowadays, but this only makes the point that good information can be hard to find. In this chapter we’ll discuss a strategy for quickly getting a sense for the state-of-the-art in health research, and then outline the steps you need to take to ask a good research question and search the literature for primary sources. 2.1 Start with Systematic Reviews and Meta-Analyses Systematic reviews and meta-analyses (PDF slide deck) Repeat after me: I will not start my research by Googling “malaria”. I will not start my research by Googling “malaria”. I will not start my research by Googling “malaria”. If your topic is malaria and you’re not sure if the vector is mosquitos or monkeys, then Wikipedia is probably a good place to start. There’s no shame in that. Otherwise, it is always a good idea to begin with a check for relevant systematic reviews or meta-analyses. Finding a good review beats Googling “malaria” any day. 2.1.1 Meta-Analyses A meta-analysis is a quantitative approach in which the results from multiple studies are combined to estimate an overall effect size. We’ll talk more about effect sizes later, but the concept is pretty simple. Let’s use a meta-analysis by Radeva-Petrova et al. (2014) as an example. The authors reviewed 17 studies of the effects of chemoprevention on pregnant women living in malaria-endemic areas. The basic question they set out to answer with their review was as follows: Do women who take antimalarial medication during pregnancy have a lower risk of getting infected with malaria, and thus a lower risk of experiencing the bad health outcomes that are associated with malaria? One indicator of malaria infection is parasitaemia, or the presence of malaria parasites in the blood. If chemoprevention has some preventive effect, you’d expect to see less parasitaemia among women exposed to the medication (aka, treatment). Few interventions are 100% effective, so we often talk about reductions in the risk of bad outcomes like malaria. This is one type of effect size, a measure of the strength or magnitude of a relationship, such as the relationship between taking a medicine and experiencing a bad outcome. Here is a forest plot from Radeva-Petrova et al. (2014) that shows the results of 10 studies (8 trials) that compared cases of parasitaemia among 3,663 pregnant women who were randomized to an intervention group (n=2,053) that received some preventive antimalarial drug or to a control group (n=1,610) that received a placebo (or nothing). Figure 2.1: Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj Here’s how Lewis and Clarke (2001) describe a forest plot: In a typical forest plot, the results of component studies are shown as squares centered on the point estimate of the result of each study. A horizontal line runs through the square to show its confidence interval—usually, but not always, a 95% confidence interval. The overall estimate from the meta-analysis and its confidence interval are put at the bottom, represented as a diamond. The center of the diamond represents the pooled point estimate, and its horizontal tips represent the confidence interval. Significance is achieved at the set level if the diamond is clear of the line of no effect. The plot allows readers to see the information from the individual studies that went into the meta-analysis at a glance. It provides a simple visual representation of the amount of variation between the results of the studies, as well as an estimate of the overall result of all the studies together. Forest plots increasingly feature in medical journals, and the growth of the Cochrane Collaboration has seen the publication of thousands in recent years. Lewis and Clarke (2001) discovered that the first forest plot was published in 1978, and first used in a meta-analysis in 1982. The name lagged behind, appearing first in 1996, apparently referring to the forest of lines typical of most forest plots. Details about each study are reported as rows in this figure. Take a look at the study by Shulman et al. (1999) in row 6. This study found that 30 of the 567 women in the intervention group tested positive for parasitaemia (i.e., malaria). This compared to 199 of the 564 woman in the control group. This is a risk ratio of 0.15—(30/567)/(199/564) = 0.15—which means that chemoprevention reduced the risk of parasitaemia by 85%. This is a huge effect size! The effect size for each study is presented in the far right column and depicted graphically in the size of point estimate square. All of the point estimates fall to the left of the line of no effect (&lt;1), thus favoring chemoprevention because you want to reduce the risk of this bad outcome. [A risk ratio of 1 would indicate no difference in risk, and a ratio &gt;1 would mean the risk is higher among the intervention group, thus favoring the control group.] The overall (pooled) effect size is shown last as 0.39, or a 61% reduction in the risk of parasitaemia. We won’t bother ourselves with the calculation of this pooled effect size, other than to note that it’s not as simple as averaging the 10 studies. This is because the studies were not given equal weight, as you can see in the “weight” column. For instance, Greenwood et al. (1989) only had a sample size of 21+13=34 children. As a result, the effect size estimate is very noisy. The 95% confidence interval is wide and crosses 1. Consequently, its weight of 6.7% is lower than the others. Simply put, studies weaker research designs get less weight in the analysis. So here in one figure we get a summary of the best available evidence and an estimate of the overall effect size, with uncertainly intervals. You can’t get that from a Google search. 2.1.2 Systematic Reviews You might be wondering how Radeva-Petrova et al. (2014) found these studies in the first place. The answer is through a systematic review of the literature. Most, if not all, meta-analyses will be completed as part of a systematic review of the literature, and every systematic review is a type of literature review. But not every literature review is a systematic review, even if done systematically. Figure 2.2: Literature reviews, systematic reviews, and meta-analyses As you can see from the table below, a systematic review requires a number of steps that are good practice, but too thorough and time-consuming for the general literature review you might prepare when starting your work. Nevertheless, you need to know the general process of preparing a systematic review to evaluate the quality of the reviews you read, and you’ll hopefully pick up some good habits along the way. Table 2.1: Comparing systematic reviews and literature reviews. Systematic Reviews Literature Reviews The goal of a systematic review is to be comprehensive and include every relevant article. The literature review that you write for the introduction of your manuscript is not expected to be exhaustive. For this reason, most systematic reviews are conducted by teams given the large scope of the work. literature reviews can be handled solo. Systematic reviews must define and follow a method that can be replicated, just like any other study. Literature reviews, on the other hand, don’t have to follow such rigid methods or make the methods explicit. Most systematic reviews pre-register this plan, meaning that the authors submit their planned methods to a registry like PROSPERO prior to conducting the study. Pre-registration gives other researchers confidence that the team is not cherry picking results at the end to make an interesting paper. It also lets other researchers know that a group is already working on the same review, thus signaling that their work might duplicate efforts and fail to get published. Not the case for literature reviews. Included in these pre-registration plans will be a specific search strategy with exact search terms for individual scholarly databases so other researchers can recreate the search. It’s a good idea to do the same for a literature review, even if not a strict requirement. Similarly, a systematic review must also outline clear criteria for including and excluding studies (e.g., keep if assignment to study arms was random). With these criteria in place, team members screen all search results, usually starting with title and abstract reviews only and moving to full text reviews as the pool of eligible studies dwindles. Screening for a literature review is typically less intensive. Systematic reviews also develop and follow guidelines for extracting details from every included study, such as numbers of participants and key outcomes. An annotated bibliography might suffice for a literature review. Finally, teams conducting systematic reviews formally assess the quality of each included study, including the potential for bias, and take these assessments into account when synthesizing the results. This process is more ad hoc for literature reviews. Where to find systematic reviews Three excellent sources for finding systematic reviews (and meta-analyses) in global health are the Cochrane Library, the Campbell Collaboration, and 3ie. You can also get to many of the reviews in these databases by searching within PubMed using the Clinical Queries feature. How to read systematic reviews Abstract and plain language summary Cochrane reviews follow a standard format that can look overwhelming at first, but is actually quite easy to read and understand. As with most journal articles, Cochrane reviews begin with an Abstract. Next comes a Plain language summary which can be helpful for newcomers to a particular topic. Radeva-Petrova et al. (2014) include the following passage in their plain language summary: For women in their first or second pregnancy, malaria chemoprevention prevents moderate to severe anemia (high quality evidence); and prevents malaria parasites being detected in the blood (high quality evidence). It may also prevent malaria illness. We don’t know if it prevents maternal deaths, as this would require very large studies to detect an effect. This one paragraph brings us up to speed with the state of the science for preventing malaria and its effects among pregnant women living in malaria-endemic areas (and points you to some gaps in the literature!). Google does not filter the evidence in this manner. Starting with systematic reviews pays off almost every time. Summary tables Next come the Summary tables, such as the one presented below from Radeva-Petrova et al. (2014). These tables round out everything you need to make your initial judgment. Figure 2.3: Malaria chemoprevention for pregnant women living in endemic areas. Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj The first comparative risk column shows the assumed risk among the control group. For instance, the risk of antenatal parasitaemia is 286 events per every 1,000 people. This is the median control group risk across eight trials of 3,663 women. The relative risk is 0.39—recall that this is the pooled, or “meta” effect size—so you can see how the corresponding risk among the intervention group is 286*0.39=111 per 1,000 people.15 As shown in the final column, the quality of this evidence is rated as “high”. The authors are referring here to GRADE criteria, a systematic approach to evaluating the quality of empirical evidence: High—Further research is very unlikely to change our confidence in the estimate of effect. Moderate—Further research is likely to have an important impact on our confidence in the estimate of effect and may change the estimate. Low—Further research is very likely to have an important impact on our confidence in the estimate of effect and is likely to change the estimate. Very Low—We are very uncertain about the estimate. Background Much of what you want to know you can learn from the abstract, summary text and tables, and forest plots (if included). If you keep reading, you’ll come next to the Background section. This is typically a short overview that explains what gaps in our knowledge the review is intended to fill. Radeva-Petrova et al. (2014) use this section to present a conceptual framework for malaria prevention during pregnancy. Figure 2.4: Drugs for preventing malaria in pregnancy: conceptual framework. Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj Methods The Methods section details how the review was organized and conducted. The purpose of this section is to provide enough detail to enable other researchers to attempt to replicate the review. The main components are:16 A description of the population and intervention. The key outcomes of interest. The search strategy and databases. Inclusion and exclusion criteria for studies. Procedures for extracting information from each study. Procedures for assessing bias and conducting a meta-analysis (if one is included) Results The Results section typically begins with details about how many primary articles were identified, screened, and excluded, typically presented graphically with a flow diagram like the one below from Radeva-Petrova et al. (2014). Figure 2.5: Study flow diagram. Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj Once the included studies are identified, it’s customary for review authors to report on the quality of the evidence presented in each study. We’ll discuss the nature of these sources of bias in a later chapter, but you should familiarize yourself with these heatmaps. While a bit on the ugly side, they provide a useful summary of bias. As a future producer of research, you should start to look at these figures and think, “how can I make sure my studies are full of green pluses?” Figure 2.6: Risk of bias summary: review authors’ judgements about each risk of bias item for each included trial. Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj Discussion and conclusions Discussion sections provide a short summary of the findings, commentary on the quality of the evidence, and thoughts about what the review adds to the existing literature on the topic. A discussion tends to be short relative to the size of the overall review. Discussion sections are often followed by a brief statement of the authors’ conclusions. This is an opportunity for the authors to frame the results in terms of the implications for practice and research. Radeva-Petrova et al. (2014) conclude: Routine chemoprevention to prevent malaria and its consequences has been extensively tested in RCTs, with clinically important benefits on anemia and parasitaemia in the mother, and on birth-weight in infants. In other words, “chemoprevention works” in this context. Appendices Reading the appendices will give you a sense of what it takes to put together a systematic review. There are usually tables after tables of characteristics of included and excluded studies, often followed by dozens of forest plots if the systematic review includes a meta-analysis with several outcomes or populations of interest. Radeva-Petrova et al. (2014) wrap up on page 120! 2.2 Devising a Search Strategy Hopefully at this point you’ll agree that it’s a good idea to start with a systematic review, not a search engine. Of course not every topic has been the subject of a recent systematic review or meta-analysis, so you’ll sometimes need to search the primary literature yourself. I’ll show you how, but first you need to clearly define what you’re looking for. 2.2.1 Asking a research question Here’s a helpful mnemonic for creating a good clinical question: PICO. P Patient, Population, or Problem I Intervention, Prognostic Factor, or Exposure C Comparison O Outcome Let’s use PICO to develop a well-focused, searchable research question on treating malaria during pregnancy. The problem we want to address is malaria infections. The population is pregnant women living in malaria-endemic areas. Not every clinical question involves testing of a treatment or intervention, but we’ll focus a lot on these types of questions in this book. For the example at hand, the intervention would be malaria chemoprevention. [Prognostic factor refers to covariates that could influence the prognosis of the patient. An exposure would be something that we think might increase the risk of an outcome.] Similarly, not every question involves a comparison group.17 In this example, the comparison is nothing or a placebo. There are many potential outcomes for treating malaria. In this case, let’s focus on parasitaemia. We can combine all of this into a research question: Among pregnant women living in malaria-endemic areas, is chemoprevention more effective than a placebo at preventing parasitaemia? 2.2.2 Approaches With your basic research question outlined, you’re ready to begin searching. At the beginning you might take a quick and dirty™ approach to get started. Eventually you’ll need to graduate to a proper search strategy to be more systematic, even if your end goal is not a capital “S” Systematic review. Quick and dirty A reasonable initial approach is to find a few recent articles to get a quick sense of what is out there. Google Scholar could come in handy here. For instance, my advanced Google Scholar search for PICO terms “malaria pregnant chemoprevention parasitaemia” (limited to recent years) identified a paper by Braun et al. (2015) on the use of intermittent preventive treatment in pregnancy (IPTp) with sulfadoxine–pyrimethamine (SP)—a specific type of chemoprevention—on malaria infections among pregnant women in western Uganda. Customize your Google Scholar experience by clicking on the gear icon. Enable use of a bibliography manager, and click on “Library links” to add your library to get links to full text. A good starting point for future searching is to note an article’s keywords. Not all journals print keywords, but if they do, you’ll probably find them right after the abstract. Next comes the introduction. Some journals and disciplines have very brief introduction sections and might not be of much help. This is often true in medicine and public health. The discussion section is also a place to look for new leads. Authors typically use the discussion to link the study results to the existing literature, demonstrating how the results add to what is already known. After looking at the introduction and discussion sections, it’s often useful to skim the references to get a sense of which journals publish this type of work. If a certain journal appears to be a common outlet for this work, a scan of the journal’s table of contents for recent issues could be useful.18 If you have access to a university library, you can learn more about the scholarly journals in a field by looking up Journal Citation Reports. This annual report ranks the journals in each field according to impact factors. Impact factors are one metric used to evaluate the importance of a journal in its field. More systematic Plan and document your search strategy Whether or not you are conducting a capital “S” Systematic review, it’s a good idea to plan and document your search. You don’t need to be as thorough in a lit review as you would for a systematic review, but it wouldn’t hurt to take a page from the approach. Let’s look at Radeva-Petrova et al. (2014) for some inspiration. Every good systematic review will include a table or appendix like this one to make the method reproducible. If you and I run this search query at the same time on two different computers, we should get the same results. Figure 2.7: Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj For the purposes of your literature review, you don’t necessarily need to ensure that other people can recreate your results, but you should make sure that you can. Create an account within the database you are searching to login and save your search approaches and make it easier to retrace your steps at a later date. Also, differences in the design of each database and interface often require you to customize your search. If you are conducting an actual systematic review that you wish to publish—as opposed to just searching the literature systematically—then you should consult with a librarian who will be familiar with the intricacies of building search strategies. Selecting a database As you can see from the table, Radeva-Petrova et al. (2014) searched five databases. MEDLINE is probably the most well known of this group. When you search PubMed, you are searching the MEDLINE database. This is typically a good place to start to find health-related studies. Talk with a research librarian to understand if other databases might be a better choice for your topic. Generate search terms Once you decide on a database, you need to generate search terms. Start with the keywords published with the sample articles you dig up. You can learn a lot about potential keywords by searching for MeSH terms. MeSH, which stands for “Medical Subject Headings”, is a controlled vocabulary thesaurus that is used to index articles in PubMed. This thesaurus is helpful because there are often many ways to refer to the same phenomenon. For instance, the MeSH term for “breast cancer” is “Breast Neoplasm”. When you search for “breast cancer” in PubMed, the database helps you out by casting a wider net by including the MeSH term automatically behind the scenes (note: it doesn’t always do this! You can check up on what PubMed includes in the search by clicking on the search details link after running a search): &quot;breast neoplasms&quot;[MeSH Terms] OR (&quot;breast&quot;[All Fields] AND &quot;neoplasms&quot;[All Fields]) OR &quot;breast neoplasms&quot;[All Fields] OR (&quot;breast&quot;[All Fields] AND &quot;cancer&quot;[All Fields]) OR &quot;breast cancer&quot;[All Fields] Turns out there are a lot of ways that we refer to breast cancer! The following entry terms are indexed to the MeSH term “breast neoplasms” by humans at PubMed: Breast Neoplasm Neoplasm, Breast Neoplasms, Breast Tumors, Breast Breast Tumors Breast Tumor Tumor, Breast Mammary Neoplasms, Human Human Mammary Neoplasm Human Mammary Neoplasms Neoplasm, Human Mammary Neoplasms, Human Mammary Mammary Neoplasm, Human Mammary Carcinoma, Human Carcinoma, Human Mammary Carcinomas, Human Mammary Human Mammary Carcinomas Mammary Carcinomas, Human Human Mammary Carcinoma Breast Cancer Cancer, Breast Cancer of Breast Mammary Cancer Malignant Neoplasm of Breast Malignant Tumor of Breast Breast Carcinoma Cancer of the Breast Back in the world of mosquitos, the MeSH term for “malaria” is “malaria”, conveniently, and a search for this term in PubMed actually searches: &quot;malaria&quot;[MeSH Terms] OR malaria[All fields] The following entry terms are indexed to the MeSH term “malaria”: Remittent Fever Fever, Remittent Paludism Plasmodium Infections Infections, Plasmodium Infection, Plasmodium Plasmodium Infection Marsh Fever Fever, Marsh Running your search Once you have some initial search terms, it’s time to build and run your query. This will be an iterative process, full of trial and error. You might start with 200,000 results. Some terms and combinations will fail to narrow this field. Others will trim too much. Figure 2.8: Boolean operators: AND, OR, NOT You’ll need to know some basic Boolean operators to be an effective searcher: AND, OR, NOT. For instance, let’s consider the search PubMed runs when you enter “malaria OR pregnancy”: (&quot;malaria&quot;[MeSH Terms] OR &quot;malaria&quot;[All Fields]) OR (&quot;pregnancy&quot;[MeSH Terms] OR &quot;pregnancy&quot;[All Fields]) These four terms are combined with OR, meaning we keep results that match any of these terms. At the time of writing, PubMed returns 922,588 results. Of course it would make more sense to search for “malaria AND pregnancy”, instead of “malaria OR pregnancy”: (&quot;malaria&quot;[MeSH Terms] OR &quot;malaria&quot;[All Fields]) AND (&quot;pregnancy&quot;[MeSH Terms] OR &quot;pregnancy&quot;[All Fields]) The first two terms and last two terms are combined separately with OR. These combinations are then combined with AND (notice the use of parentheses to segment the operations), dropping our pool of results to 4,203 records. The AND operator will always maintain or decrease the number of results. If we want to limit the results humans, we can add AND &quot;humans&quot;[MeSH Terms] to the end.19 Doing so drops our pool of results to 3,798. (&quot;malaria&quot;[MeSH Terms] OR &quot;malaria&quot;[All Fields]) AND (&quot;pregnancy&quot;[MeSH Terms] OR &quot;pregnancy&quot;[All Fields]) AND &quot;humans&quot;[MeSH Terms] Alternatively, we could use the NOT operator to limit the results to non-humans, but NOT is not commonly used. No it’s not. Let’s return to our PICO question and use Boolean operators to combine the components. Among pregnant women living in malaria-endemic areas, is chemoprevention more effective than a placebo at preventing parasitaemia? Here’s what we want to do in plain English: pregnancy OR pregnant women AND malaria endemic AND chemoprevention (if you know specific drugs to search, string together with ORs) AND randomized controlled trial AND parasitaemia As I tap out these words on my keyboard, this search returns 513 records in PubMed. Once you are satisfied with your results, you could choose to apply the same search to another database. This might be worth the effort if your topic crosses disciplinary boundaries, like economics and health. Best to check with a research librarian. Oh hey, here’s one now: 5 tips for PubMed searching Screening results Even the best search queries return some duds, so the next step is screening. We can return to Radeva-Petrova et al. (2014) to see what a thorough approach looks like. You would likely take some shortcuts for a regular literature review. Typically systematic review searches will return hundreds or thousands of potential hits, so a study team will screen titles and abstracts to exclude obvious mistakes. When beginning this process, it’s common to have team members screen some of the same records to establish reliability, a concept that we will discuss in more depth in the chapter on measurement. Basically, you want to know that everyone screening records would make the same inclusion/exclusion decision. The Radeva-Petrova et al. (2014) search strategy turned up 179 unique records, and the authors excluded 126 of these records after screening the abstracts. The excluded studies did not meet certain pre-defined criteria. For instance, the authors only wanted to include studies using RCTs and quasi-experimental designs. This left the team with 53 studies that required a full-text review. Only 17 of the 53 studies still met eligibility criteria after this review.20 Supplemental searches It is customary in a systematic review–and helpful in general reviews—to augment database searches with reference reviews and hand searches to ensure that no key references were missed in the database query. A reference review is nothing more than a scan of an eligible article’s bibliography. In a hand search, you would go to the website of journals that published the eligible articles and scan the tables of contents for each issue published during the search window. If you find that either supplemental method turns up a lot of new results, it could make sense to revise your systematic review search strategy to be more comprehensive. Extracting data Depending on your objectives you might choose to systematically extract data from each study—key facts related to study design, methods, and results. Or you might take a shorter path and create an annotated bibliography. If you need to be more systematic—an essential requirement for a capital “S” Systematic review—then you need to design a data extraction strategy. Your PICO research question can be a helpful guide to identifying the minimum data you should extract. Returning to Radeva-Petrova et al. (2014): Among pregnant women living in malaria-endemic areas, is chemoprevention more effective than a placebo at preventing parasitaemia? Some possibilities include: study setting/population sample size sample demographics, including parity study design intervention details, such as specific medication and dose primary outcome (e.g., parasitaemia) effect size There are numerous software options for storing your extracted data, but you’ll likely find that a simple spreadsheet with rows of studies and columns of study variables will work just fine. Lots of teams use this approach for big systematic reviews, so it will probably serve you well for something more modest. 2.3 For the Love of Everything Holy Use a Reference Manager Even if you chose to ignore everything I’ve written up to this point, do yourself a favor and use a program for managing references. I’m amazed every year when I learn that students on the precipice of graduation manually type and format their in-text citations and bibliographies. What a waste of time! There are several reference managers you might consider. I’ll mention one because it is free and open-source: Zotero. The concept of “free” does not need much explanation, but students often have several free options that are not really free. A good example is a program like EndNote. A university might make this program a free download for enrolled students, but the license expires upon graduation or soon becomes obsolete without a paid upgrade. Additionally, in global health it’s common to work with colleagues who do not have access to a program like EndNote, which makes collaboration challenging. For these reasons, I highly recommend a program like Zotero that is free to use and open to improve. A tutorial is beyond the scope of this chapter, but it’s worth mentioning some features that are common to many reference managers: Easy importing of references from databases like PubMed. Go from your search results to reference manager in seconds. Automatic retrieval of full-text PDF. Sync PDFs in collection to tablets and phones Connections to word processing software to make inserting references in papers a snap. Automatic creation of bibliographies based on works cited. Push button reformatting of in-text citations and references to different styles, such as APA and Harvard. Shared collections with automatic syncing via the cloud to facilitate collaboration. Easy export of references for migration to just about any other reference manager. So next time you see someone typing references and complaining about APA formatting, open your laptop and run your reference manager. Watch them weep! Share Feedback This book is a work in progress. You’d be doing me a big favor by taking a moment to tell me what you think about this chapter. Test Yourself References "],
["critical.html", "3 Critical Appraisal 3.1 Be Skeptical of News Reports and Press Releases 3.2 Peer-Reviewed Does Not Mean Correct 3.3 How to be a Good Consumer of Research Additional Resources Share Feedback Test Yourself", " 3 Critical Appraisal Scholars in every discipline have their own ways of reading and evaluating the literature. In medicine, this process is called critical appraisal and it’s part of a larger approach called evidence-based medicine (EBM) or, more generally, evidence-based practice (EBP). In EBM, the goal is to integrate the best available evidence with clinical judgment and context, such as a patient’s preferences and values. Sackett (1996) offers a more formal definition: The conscientious, explicit and judicious use of current best evidence in making decisions about the care of the individual patient. It means integrating individual clinical expertise with the best available external clinical evidence from systematic research. There are several steps in EBM. We covered 2 and 3 in the previous chapter. ASSESS the patient: What is the clinical problem? ASK the question: Develop a clinical question using tools like PICO that we introduced in the previous chapter. ACQUIRE the evidence: Search the literature or find systematic reviews or meta-analyses. APPRAISE the evidence: Evaluate the quality and applicability of the research evidence. APPLY this to the patient’s case: Combine the best empirical evidence with clinical judgement and the patient’s preferences. EBM is focused on patient care, but the approach has been extended to evidence-based public health (Brownson, Fielding, and Maylahn 2009) and evidence-based global health policy (Yamey and Feachem 2011). Just as in EBM, the aim of these two new EB__’s is to adopt policies and programs that have been shown to save lives and improve health at scale. But before policymakers can make evidence-based decisions, the scientific community must evaluate the strength of the evidence. So let’s dive into step 4, critical appraisal. 3.1 Be Skeptical of News Reports and Press Releases Much of what we learn about scientific results from the media comes in the form of clickbait, such as this article in Discover Magazine titled “Want to avoid malaria? Just wear a chicken.” Would you be surprised if I told you that the study did not actually require people to wear chickens?21 Are you worried about getting malaria? Well, according to this study, you might be able to avoid it by carrying a chicken everywhere you go. These findings…give “tastes like chicken” a whole new meaning! Despite the outlandish title, this news report got it mostly right, so it’s better than most. If you turn on the news or read a university press release, you’ll often find summaries and claims that go far beyond the conclusions of the original article. This is because most studies give us a small glimpse at the “truth”, but the measured and careful language of scientific articles does not always capture the attention of the public. Part of the problem is also that good science writers are hard to come by. One of the best is Ben Goldacre, a British psychiatrist who runs the EMB Data Lab at the Centre for Evidence-Based Medicine at the University of Oxford. He wrote the Guardian’s “Bad Science” column for a decade, and published a great book with the same title. In 2011 he published a paper with some colleagues on the reliability of health reporting. Here’s how he described that work in the Guardian: Here’s what we did. First, we needed a representative, unbiased sample of news stories, so we bought every one of the top 10 bestselling UK newspapers, every day for one week…We went through these to pull out every story with any kind of health claim, about any kind of food or drink, which could be interpreted by a reader as health advice. So “red wine causes breast cancer” was in, but “oranges contain vitamin C” was not. Then the evidence for every claim was checked…Finally, to produce data for spotting patterns, the evidence for each claim was graded using two standard systems for categorising the strength of evidence. Here’s what we found: 111 health claims were made in UK newspapers over one week. The vast majority of these claims were only supported by evidence categorised as “insufficient” (62%). After that, 10% were “possible”, 12% were “probable”, and in only 15% was the evidence “convincing”. Yikes! 3.2 Peer-Reviewed Does Not Mean Correct Journalists and communications professionals can and do make mistakes in summarizing and interpreting scientific findings, but sometimes the problem is further upstream with the study authors. But how can published research be flawed, you ask? It was peer-reviewed! Peer review is an important component of the scientific process, but it is not a guarantee of “truth” or a certification of the results. So what is peer review? 3.2.1 Getting past the gatekeepers Let’s say you just completed what you think is a fascinating new study that upends years of conventional thinking on your research question. You could issue a press release and tell the world, but most scientists would reserve judgement or consider your results preliminary pending peer review. They would expect you to write a manuscript detailing your research design, data, methods, and results, and then submit this manuscript to an academic or scholarly journal. Typically, your submission would be screened by the journal’s editor, a role often filled by a senior scientist in your field. If the editor thinks that your paper is free of obvious fatal flaws and will be of interest the journal’s readership, then the editor might assign it to an associate editor with some expertise on your topic to manage the peer review process. 3.2.2 Who is a peer? The associate editor will then attempt to find 3 or more scholars in your field—your peers—to review the paper and comment on its merits. Some journals give you the option to recommend reviewers who might be a good fit and to request that certain colleagues are not considered. The editorial team does not need to respect your wishes, but finding appropriate reviewers is a challenge and you can help by suggesting scholars who are qualified to evaluate your work. So a “peer” can be: someone at your level, more junior, or more senior someone who shares the same conceptual framework regarding your topic of study, or someone who takes a different view entirely someone who works on a parallel topic, or someone who is a direct “competitor” someone who is a topic expert, or, when it’s hard to find the right person, someone who does not have much background at all someone who is a technical expert on your methods, or someone who does not know the first thing about your chosen analytical approach You’ll probably never know. Most journals use a blind review process by which you don’t know who accepts the editor’s request to review your paper, and the reviewers are not informed of your identity. At least that’s the idea. Sometimes reviewers give hints about their identity by recommending that you cite a lot of their own work. And sometimes it’s easy to determine your identity as the author because your current work builds on your previous studies or you’ve already presented the work at scientific conferences. 3.2.3 What happens during the review process? Once your paper arrives on a reviewer’s desk, he or she will take a few weeks (or months!) to recommend that your paper be rejected or accepted with no, minor, or major revisions. Some reviewers enumerate your perceived flaws in painstaking detail. Others give high-level comments that might be too vague to be helpful to you or the editor. The editorial team reviews these reviews, and it’s up to them to make a decision. Most academics are happy to get a “revise and resubmit” letter (aka, “R&amp;R”). The editor will usually give some indication that a revised paper would have a good chance of publication, but it’s not a guarantee. Sometimes the revised paper will go back out for further review, but the editor can also make the decision to accept the revision without additional input. The editor has a tough task because it’s often the case that reviewers take different positions on your submission. As Smith (2006) suggested, these recommendations can be direct opposites: Reviewer A: `I found this paper an extremely muddled paper with a large number of deficits’ Reviewer B: `It is written in a clear style and would be understood by any reader’. So peer review is just like course evaluations! 3.2.4 What does NOT happen during the review process? A critical thing to note, however, is that reviewers almost never have access to your data or analysis code. They base their decisions on what you said you did (your methods), what you said you found (your results), and what you said it all means (your discussion). You have to describe your data sources, but no one is checking your work. So even if reviewers find possible flaws in your logic, analysis mistakes and fraud go largely unchecked.22 This lack of verification is why it is wrong for people to conclude that published in a peer-reviewed journal means correct. Journalists can harbor this belief, and defensive authors sometimes promote it when challenged on their study’s findings. If it were true that published==correct, then we’d have no need for corrigendum and retractions—and the website Retraction Watch would be empty. 3.3 How to be a Good Consumer of Research Here’s a nice framework for how to be a good peer reviewer (aka, referee) that is a good starting point for thinking about how to be a good consumer of research more generally. In this guide, Leek describes a scientific paper as consisting of four parts.23 I take the liberty of collapsing methodologies and data, as well as adding the Introduction. An introduction that frames the research question A set of methodologies and a description of data A set of results A set of claims Leek offers a helpful recommendation about how to approach a new paper: Your prior belief about [#2-3] above should start with the assumption that the scientists in question are reasonable people who made efforts to be correct, thorough, transparent, and not exaggerate. This book focuses mainly on how to read and write sections 1 and 2. I think you’ll gain new insight into how to evaluate research results and claims if you make it to the end, but it’s probably safe to say that you’ll need more background in analysis to feel confident in your ability to critique findings and plan your own analysis. 3.3.1 Introduction A good Introduction will explain the aim of the paper and put the research question in context.24 In public health and medicine, this section will often be very short compared to what you’ll find in other disciplines like economics. As a reader, you want to focus on understanding the research question. If you are familiar with the research area, you might also read the Introduction with a critical eye toward the literature reviewed. Did the authors miss any key references that could signal that they are unaware of developments in the field?25 3.3.2 Method A good Method section will provide enough information to let the reader attempt to replicate the findings in a new study. Journal space constraints make this challenging, so you’ll often find that authors post supplemental materials online that give additional details.26 Even with supplemental materials, however, it would be common to need to contact the author for additional details and materials if you really wanted to attempt a replication. The organization of the Method section will vary by discipline, but you should expect to find some information about the research design, subjects, materials or measures, data sources and procedures, and analysis strategy. The Equator Network, which awkwardly stands for “Enhancing the QUAlity and Transparency Of health Research”, is a good resource for understanding modern reporting standards. If you are preparing your own manuscript, most journals will expect you to include all of the information outlined in the checklist that’s relevant for your research design. If you are reviewing an article, you can do your part to promote comprehensive reporting by referring authors to these checklists. Include the completed checklist as an appendix with your article submission to head off reviewers who will complain about missing information that you definitely included on page 5, line 20 thank you very much. Is the research design well-suited to answer the research question? This book will introduce you to common research designs in global health. What you should know at this point in your reading is that there are many different designs that could potentially answer most research questions, but not all designs are created equal. And it’s not always possible to use the design that would produce the highest quality evidence. Figure 3.1: Levels of evidence A graphic like this is commonly used in the EBM literature to convey the point that research designs are not created equal. The meta-analyses and systematic reviews that you read about in Chapter 2 are ‘studies of studies’ and they sit atop the evidence hierarchy. They enjoy this status because they synthesize the best available evidence. If you accept that no one study is the final word on a research question, then it will make sense that a meta-analysis that pools together results and accounts for variable study quality could potentially give you a better answer than any one study. Is there a risk of bias and confounding? Risk of bias and confounding (PDF slide deck) Some study designs are better than others in theory because of their ability to address potential bias when conducted properly. As we discussed in Chapter 1, the goal of scientific research is inference and we must live with some error and uncertainty. As a consumer of research, you have to accept this as fact and assess the extent to which a study’s design and methods might lead us away from the “truth”. The Cochrane Handbook for Systematic Reviews (2011) cautions us to pay attention to design features (e.g., how participants were selected) rather than labels (e.g., cohort study) because such labels are broad categories. Therefore, don’t rely too much on the evidence hierarchy shown above. These rankings reflect ideals. It’s possible to have a poorly designed or implemented RCT. The evidence from such a flawed study will not necessarily be better than the evidence from a non-randomized study just because it carries the label “randomized”. Error comes in two flavors: random and systematic. Random error adds noise (aka, variability) to your data, but it does not affect the average. For instance, I might step on a scale and see that I weigh 185.12. I step off and back on, and this time I weigh 185.13.27 This is random error that results from the limitations of my scale. If I keep taking measurements, this random error will balance out. Random means that the readings won’t be systematically too high or too low. You’ve probably already guessed that systematic error is not random. Systematic error is also known as bias and represents a deviation from the “truth”. Let’s imagine that my scale is broken and I don’t really weigh 185. I weigh 200. I can worry about the imprecise measurements of 185.12 and 185.13 all day, but I’d be missing the bigger problem that my scale is systematically reading the wrong weight. I can keep taking measurements over and over, but my scale is just wrong. If my goal is 186, I would come to the wrong conclusion that I can stop dieting! You can estimate random error (as we’ll discuss in Chapter 7), but you typically don’t know the extent to which bias affects your study results. For this reason, we often frame this as a “risk of bias”. In a non-randomized design, the biggest risk of bias comes from potential selection bias (Higgins and Green 2011). Selection bias can take different forms. In the context of intervention research, selection bias represents pre-treatment (aka, baseline) differences between study groups. For instance, Webster et al. (2003) conducted a case-control study—a non-randomized, or observational study design that we’ll discuss more in Chapter 10—in Eastern Afghanistan to study the efficacy of bednets as a tool for preventing malaria. Patients who presented at the study clinic with a fever were tested for malaria. Those who tested positive were classified as “cases”, and the rest were classified as “controls”. The researchers asked cases and controls about their bednet use, education, income, and several other characteristics. Then they compared bednet users and non-users on their odds of malaria (i.e., being classified as cases). Webster et al. (2003) wanted to look at potential selection effects with this particular research design, so they also examined patients’ use of chloroquine prior to attending the clinic. If a patient was classified as a control (negative blood film) but tested positive for chloroquine, this would indicate that the patient received treatment for malaria prior to arriving at the clinic, meaning they really should have been classified as a case. To determine if this misclassification of cases as controls could introduce selection bias, the authors looked at chloroquine use in bednet users and non-users. They found that the use of chloroquine prior to clinic testing was LESS common among patients who reported using bednets compared to non-users. If chloroquine use was less common among bednet users, it would underestimate the estimated effect of bednets. Consider the following example. Figure 3.2: Selection bias Panels A and B show cases (those who tested positive for malaria) and controls by their reported bednet usage. In Panel A, there are 4 patients who are misclassified as controls, meaning that they tested negative for malaria but only because they treated themselves with chloroquine prior to the test. You can see that chloroquine use is less common among net users. Still in Panel A, we see that the odds of malaria (cases) among bednet users is 12/30 and the odds of malaria among non-users is 12/18. This is an odds ratio of 0.60, suggesting that bednets protect against malaria (a value of 1 would indicate no effect). However, Panel B shows that this effect might be an underestimate. If we move the misclassified control patients to the case group where they belong, the odds change. Now the odds of malaria among bednet users is 13/30 and the odds among non-users is 15/18. This is an odds ratio of 0.52, suggesting an even greater protective effect. In Panel A, the effect was biased toward the null, meaning that the effect looked smaller than it probably is in reality. This bias results in confounding, and we would call chloroquine use a confounding variable. Confounding variables are correlated with both the “treatment” (i.e., bednet use) and the outcome (i.e., malaria). As we’ll explore later, an experimental design typically overcomes this risk of bias and confounding because bednet ownership can be randomly assigned. If the sample size is large enough, we’d expect chloroquine use to be equally distributed between the bednet and non-bednet groups. The key word here is “typically”. Things can still go wrong in an experimental design that result in a risk of bias. For this reason, every Cochrane systematic review assess several types of known risks of bias in RCTs (Higgins and Green 2011): selection bias performance bias detection bias attrition bias reporting bias We’ll discuss these sources of bias more in Chapter 12, but the takeaway should be that every study has a potential for bias and, as a reviewer or general research consumer, you should assess the risks of bias that might challenge the validity of the results. As you’ll see later, this type of validity—are the study results “correct”?—is typically referred to as internal validity (Higgins and Green 2011). At the end of this chapter you’ll read about another dimension of validity called external validity. Who (or what) was the subject of study and how were these subjects recruited and/or selected? Studies of human subjects typically have a subsection of the Method section that describe participant selection and recruitment. What made someone eligible or ineligible to participate? Who was excluded, intentionally or not? These details help to define the population of interest and will inform the study’s generalizability, a concept we’ll discuss shortly. Once eligible participants were identified, how were they selected and recruited? Was this process random, or did the researchers invite who was available? As you’ll learn in Chapter 7, the method of sampling has implications for what inferences are possible about the population. What materials and/or measures were used in the course of the study? Almost every study uses some type of materials or measures. Diagnostic studies, for instance, evaluate a diagnostic test or a piece of hardware that analyzes the test samples. Environmental studies might use sophisticated instruments to take atmospheric measurements. Expect studies like these to provide specific details about the materials and equipment. Study variables also need to be precisely defined. For instance, hyperparasitemia describes a condition of many malaria parasites in the blood. But what constitutes “many”? The WHO defines it as “a parasite density &gt; 4% (~200,000/µL)” (WHO 2015). Does the study use this definition? Another one? In studies measuring social or psychological constructs such as anxiety, you’d expect to read about how this thing called “anxiety” is defined and measured. Is anxiety diagnosed by a psychiatrist (if so what is the basis for this diagnosis?) or is anxiety inferred from a participant’s self-reported symptoms on a checklist or screening instrument (if so, what are the questions and how is the instrument scored?)? We’ll dive into measurement issues in Chapter 6. How was the study conducted and how was the data collected? This part of a Method section should describe what happened after participants were recruited and enrolled. What happened first, second, third? If the study is observational, the procedures might be limited to data collection. Who collected the data, and how were they trained? Where were the data collected? For intervention studies, the procedures will describe how participants were randomized to study arms and what happened (or did not happen) in each arm. Were the participants, data collectors, and/or patients blind to the treatment assignment? How was the data analyzed? If the study uses a hypothesis-testing framework (and not all do), then you’ll find details about study hypotheses in the Introduction or Method section, depending on the journal. The Method section should also detail how the analysis will be carried out. For instance, if you are reading an intervention study, how was the effect size be estimated? Ordinary least squares regression? Logistic regression? The list goes on and on. When you are preparing your own manuscript, remember that the Method section is where you should define variables and specify your analysis. Your Results section should just get to the business of reporting the findings. There’s no need to re-explain your analysis. Was the study pre-registered and approved by an ethics board? The US Federal Policy for the Protection of Human Subjects (aka, the “Common Rule”) defines research as “a systematic investigation, including research development, testing and evaluation, designed to develop or contribute to generalizable knowledge…” If the research involves human subjects, it must be reviewed and approved by an Institutional Review Board before any subjects can be enrolled. Most studies fall under IRB oversight, but some might qualify as exempt. Figure 3.3: Is the human subjects research eligible for exemption?; Source: http://bit.ly/2brlbKR. Increasingly researchers are taking the additional step of registering a study protocol prior to the study launch in a clearinghouse like https://clinicaltrials.gov/. This is a requirement for investigations of drugs that are regulated by the FDA, and now it’s an expectation of many journals.28 Pre-registration does not ensure trustworthy results, but the practice is a welcome increase in research transparency. If the analysis described in an article deviates from the planned analysis, you would expect the authors to provide a compelling justification. Studies often measure a number of outcomes (sometimes in a number of different ways), and it’s easy to cherry-pick results and find something to present. Sometimes authors will actually deviate from the pre-registered protocol and present different results when the pre-registered plan does not work out. This is called outcome switching. Some medical journals don’t seem to care, but the COMPare Trials Project thinks they should. Figure 3.4: Is outcome switching a problem in medical trials?; Source: http://compare-trials.org/. 3.3.3 Results Can each finding be linked to data and procedures presented in the Methods Every finding in the Results section should be linked to a methodology and source of data documented in the Method section. Articles in medical journals are some of the shortest, so you might need to download supplemental materials posted online to get a clearer sense of what the authors did and found. Remember, a plot twist can be a useful literary device in a work of fiction. This advice does not extend to journal articles. Is the analysis correct? This can be a difficult question to answer as a new consumer of research. Without access to the data and any analysis code—still the norm for most publications—you can’t independently verify the results. Even if you did have access, some analyses are so complex that only people with extensive training feel qualified to question the results of published works. Thankfully you don’t need a degree in biostatistics to be a good consumer of research. You can get a long way by adopting some basic approaches to critical appraisal. Exhibit A: a Lancet article published by Pronyk et al. (2012) about the impact of the Millennium Villages Project (MVP) on child survival in rural sub-Saharan Africa. The MVP was founded by economist Jeffrey Sachs as a proof of his concept that extreme poverty could be solved, and the Millennium Development Goals met, with a big financial push. The basic idea behind a Millennium Village is to intervene across sectors simultaneously—water, sanitation, education, health, etc.—to give communities a chance to escape the poverty trap. Sachs and his colleagues published their first comprehensive report on the MVP in the Lancet with the stated aim to “assess progress towards the MDGs and child survival over the project’s first 3 years and compare these changes to local trends”. The authors examined child mortality rates across 9 Millennium Villages and concluded: The average annual rate of reduction of mortality in children younger than 5 years of age was three-times faster in Millennium Village sites than in the most recent 10-year national rural trends (7·8% vs 2·6%) When this article was published online, Bump et al. (2012) challenged the calculation of both of these figures and the resulting interpretation that the MVP had a large impact on child mortality. One of the authors of this criticism, Gabriel Demombynes, explained the errors in this post. First, Pronyk et al. (2012) annualized (aka, divided) the 21.7% cumulative decline in mortality by 3 for the number of years the intervention was active rather than by 4, the correct time window for this retrospective assessment of mortality. This correction reduces the annual rate of decline among Millennium Villages from a claim of 7.8% to an actual 5.4%. Second, Pronyk et al. (2012) compared their estimated annual decline after 2006 (7.8%) to the results of national surveys in MVP countries from 2001-10. A more appropriate comparison using all of the available post-2006 data suggests an average annual rate of decline of 6.4% across the 9 MVP countries. Figure 3.5: Correcting the MVP estimate of impact on child mortality; Source: http://bit.ly/2aGSk2m Bump et al. (2012) conclude: The above observations imply that a key finding of the paper—that child mortality fell at the treatment sites at triple the nationwide rural background rate—is incorrect. Child mortality fell at 5·9% per year at the sites versus 6·4% per year on average across all areas of the countries in question (probably more in rural areas alone) according to the available data that most closely match the project period. This difference is not significant. Pronyk (2012) published a correction that was noted by Retraction Watch. 3.3.4 Discussion Is each claim linked to a finding presented in the Results? Each claim (e.g., the world is flat) should be supported by results that are reported in the paper (e.g., summary of altitude data). If you don’t find a link between a claim in the Discussion section with a finding in the Results section, you should begin to wonder if the author is “going beyond the data”. For instance, if I present results on the efficacy of a new treatment for malaria but do not present any data on cost, then it would be inappropriate for me to claim that the treatment is “cost-effective”. It’s legitimate to speculate a bit in the Discussion section based on documented findings, but authors should be careful to label all speculation as such—and these thought exercises should never find their way into the article’s Abstract. Is each claim justified? Once you establish that there is a link between a claim and a set of results, you want to make sure that the claim represents a correct interpretation of these findings. For instance, do the authors tell you that the results should not be interpreted as a causal relationship and then go on to recommend that we impose a tax on margarine in order to strengthen the American family? Figure 3.6: Divorce rate in Maine correlates with per capita consumption of margarine, r=0.99; Source: http://www.tylervigen.com/spurious-correlations Are the claims generalizable? Most [studies] are highly localized and paticularistic…Yet readers of [your study’s] results are rarely concerned with what happened in that particular, past, local study. Rather, they usually aim to learn either about theoretical constructs of interest or about a larger policy. That’s Shadish et al. (2003) writing about the importance of generalizability of research findings and claims. When a study is so highly localized that the results are unlikely to generalize to new people and places, we’d say that the study has low external validity. One approach to promoting generalizability is to use formal probability sampling. We’ll cover this more in Chapter 7, but randomly sampling participants from the population of interest is one way to increase the external validity of a study. For instance, Wanzira et al. (2016) analyzed data from the 2014 Uganda Malaria Indicator Survey, a large national survey, and found that women who knew that sulfadoxine/pyrimethamine (SP) is a medication used to prevent malaria during pregnancy had greater odds of taking at least two doses as compared to women who did not have this knowledge. Since the UMIS is nationally representative, we could assume that the results could apply to Ugandan women who did not participate in the study. Would the results generalize to women in Tanzania? Yes, one could make an argument that they would. Would the results generalize to women in France? No, probably not. For one, malaria is not an issue there. Are the claims put in context? A good Discussion section will put the study’s findings in context by suggesting how the study adds to the literature. Do the results replicate or support other work? Or do the findings run contrary to other published studies? What are the limitations? No study is perfect, so there’s no need to pretend that yours is any different. In addition to knowing how the results fit into the bigger research landscape, it’s also important for readers to understand potential limitations of your design and method. Additional Resources Critical appraisal worksheets from the Centre for Evidence-Based Medicine BMJ Series on “How to Read a Paper” Critical appraisal resources from Duke Medicine Share Feedback This book is a work in progress. You’d be doing me a big favor by taking a moment to tell me what you think about this chapter. Test Yourself References "],
["causeeffect.html", "4 Cause and Effect 4.1 Fundamental Challenge of Causal Inference 4.2 Threats to Internal Validity 4.3 Research Designs to Estimate Causal Impact", " 4 Cause and Effect Do bednets prevent malaria? Do vouchers increase access to treatment? Do cash transfers improve mental health? What these research questions share in common is a focus on causal impact—a difference in outcomes that can be attributed to a treatment, intervention, policy, or exposure. In many ways, as an applied field global health is the study of causal impact. 4.1 Fundamental Challenge of Causal Inference The fundamental challenge of causal inference (PDF slide deck) In an ideal research world we could answer the question, “Do bednets prevent malaria?”, by cloning you and simultaneously giving a bednet to you but not your clone. This would help us to understand what really happens in the absence of the intervention because the only difference between you and your clone would be that one of you received the intervention.29 Of course we don’t have clones, and we can’t simultaneously give you a bednet and not give you a bednet. We only get to observe what happens to you, not a clone of you who did not receive the intervention. So we have to ask, hypothetically, what would have happened if you had not been given a bednet. This hypothetical situation—what would have happened in the absence of the intervention—is referred to as the counterfactual (or “potential outcome” in the language of the Neyman–Rubin causal model). Not being able to observe the counterfactual directly is the so called “fundamental challenge of causal inference”. So you could also say that this is the primary reason we need books about research designs. Much of what follows in this book deals with strategies for causal inference in the absence of a true counterfactual. 4.1.1 Understanding Causal Relationships Humans like you and me have a pretty decent understanding of cause and effect (hand touch fire. fire hot. fire burn hand.). Philosopher humans, on the other hand, have spent centuries convincing us that causality is actually much more complicated than it might seem on the surface. They did a good job because causal inference is a vibrant field of study today, and researchers continue to develop new techniques for drawing causal inferences from experimental and non-experimental data. Let’s review some of the basics. Causes In his book Causal Inference in Statistics, computer scientist Judea Pearl provides a simple definition of causes: “A variable X is a cause of a variable Y if Y in any way relies on X for its value”. The phrase “in any way” is a reminder that most of the causal relationships we investigate in global health are not deterministic and effects can have more than one cause. Think about it this way: you give an experimental treatment to 100 people suffering from a disease and only 60 get better. If the causal relationship between the drug and disease state were deterministic, all 100 patients would have recovered. This is not what happened, however. The causal relationship only increased the probability that the effect would occur. Effects Causal impact is the difference in counterfactual outcomes (aka, “potential outcomes”) caused by some exposure, program, intervention, or policy. Sounds simple enough, but we’re back to our fundamental problem: we can only observe one counterfactual outcome for an individual; we don’t get to observe someone in two states simultaneously, e.g., treatment and control. This means we can’t observe an effect of the program on an individual. We have to turn instead to groups of individuals. The best we can do is hope to infer the counterfactual by comparing some people who get some treatment to other people who do not.30 Most often we think about comparing two different sets of people who who exist in either a treatment or an intervention group, but the logic also extends to (a) more than two groups (aka, study arms) and (b) just one group of people observed at different time points. “But I thought you said an individual like me can’t exist in two states at once!” That’s correct. We can only observe someone in one state (e.g., treatment or control), but we could chose to compare YOU before you receive a treatment to YOU after you receive a treatment. This is a “pre-post” or “before/after” comparison. We’ll talk more about specific designs soon. The most common estimate of causal impact is the average treatment effect (ATE). We can’t observe an effect of X on Y for any specific individual (who can only exist in one state at a time), but we can determine if X causes Y on average. This is possible because the average difference in potential outcomes (which we can’t observe) is equal to the difference of averages. The following graphic might help.31 Figure 4.1: Average causal effects can be estimated even though individual effects cannot be observed. Panel A shows hypothetical results when all subjects are assigned to treatment T(1) or control T(0). There are two data points for each person corresponding to their hypothetical potential outcomes (remember that in reality we can only observe one point per person since it’s not possible to be in two states at once). Both sets of points have an average value, depicted by the dashed lines that you see in Panel A and Panel C. In the middle Panel B, the differences between each pair of outcomes in A are plotted as individual effect sizes (e.g., 1.0-(-1.5)=2.5 for person 6). The dashed green line depicts the average causal effect. Notice that this average of individual differences in B is equal to the difference in averages in Panel C. The point is that we cannot observe effects on individuals since we cannot measure both potential outcomes for any one person; however, we can estimate the average causal effect by comparing the average value from a group of people who receive the intervention to the average value from a group of people who do not. Causal Relationships OK, so we’ve established that it’s possible to estimate the average difference between two groups, but can we claim that this difference is a valid estimate of the causal impact of X on Y? In other words, how do we know that X and Y causally related? Shadish et al. (2003) point to three characteristics of causal relationships that we should be on the lookout for: The cause is related (aka, associated) to the effect. The cause comes before the effect. There are no plausible alternative explanations for the effect aside from the cause. Condition #1 is easy to establish. Is X correlated with Y? In fact it’s so easy to establish that someone came up with the maxim, “correlation does not prove causation”, to remind us that the burden of proof is greater than the output of correlate x y or cor(x, y), or whatever command your favorite statistical software package would have you run. But it’s a start. Condition #2 is a bit harder to demonstrate conclusively because X and Y might be correlated, but maybe the causal relationship runs in the opposite direction—maybe Y causes X. Correlations don’t tell us which comes first, X or Y. Let’s take malaria and poverty as an example. Jeffrey Sachs and Pia Malaney (2002) published a paper in Nature in which they wrote: As a general rule of thumb, where malaria prospers most, human societies have prospered least…This correlation can, of course, be explained in several possible ways. Poverty may promote malaria transmission; malaria may cause poverty by impeding economic growth; or causality may run in both directions. Condition #3 is the trickiest of all: ruling out plausible alternative explanations. As Sachs and Malaney note, the literature on poverty and malaria has not found a way to do so conclusively. They write that it’s “possible that the correlation [between malaria and poverty] is at least partly spurious, with the tropical climate causing poverty for reasons unrelated to malaria.” The authors are proposing that climate is a potential cause of both poverty and malaria. If true, that would make climate a confounding (aka, lurking) variable that accounts for the observed relationship between poverty and malaria. 4.2 Threats to Internal Validity The possibility of plausible alternative explanations is what keeps researchers up at night, particularly non-experimentalists. They are always on the lookout for threats to making valid causal inferences—aka, threats to internal validity. The “randomistas”, on the other hand, tuck themselves in early after a glass of warm milk knowing that random assignment will generally make plausible alternative explanations implausible. To be sure, the randomistas will have bias-filled nightmares on occasion when they learn that an experiment did not quite go as planned, but they are generally heavy sleepers. You’ll recall that internal validity is Campbell’s (1957) notion about whether an observed association between X and Y represents a causal relationship. If X comes before Y, and if there are no other plausible explanations for the covariation between X and Y then causal inference about X and Y is valid. Threats to causal inference are threats to internal validity. Shadish, Cook, and Campbell (2003) outlined nine primary reasons why it might not be valid to assume a relationship between X and Y is causal. Table 4.1: Threats to internal validity. Source: Shadish et al. (2002), http://amzn.to/2cBaAM1. Threats Definitions Ambiguous temporal precedence Lack of clarity about which variable occurred first may yield confusion about which variable is the cause and which is the effect. Selection Systematic differences over conditions in respondent characteristics that could also cause the observed effect. History Events occurring concurrently with treatment could cause the observed effect. Maturation Naturally occurring changes over time could be confused with a treatment effect Regression When units are selected for their extreme scores, they will often have less extreme scores on other variables, an occurrence that can be confused with a treatment effect. Attrition Loss of respondents to treatment or to measurement can produce artifactual effects if that loss is systematically correlated with conditions. Testing Exposure to a test can affect scores on subsequent exposures to that test, an occurrence that can be confused with a treatment effect. Instrumentation The nature of a measure may change over time or conditions in a way that could be confused with a treatment effect. Additive and interactive effects The impact of a threat can be added to that of another threat or may depend on the level of another threat. 4.2.1 Ambiguous Temporal Precedence Correlational studies can establish that X and Y are related, but often it’s not clear that X occurred before Y. Uncertainty about which way a causal effect might flow is referred to as ambiguous temporal precedence—or simply the chicken and egg problem. Sometimes the direction is clear because it’s not possible for Y to cause X. For instance, hot weather (X) might drive ice cream sales (Y), but ice cream sales (Y) cannot cause the temperature to rise (X). Most relationships we care about in global health are not so clear, however. Take bednet use and education as an example. Does bednet use prevent malaria and allow for greater educational attainment? Or does greater education lead to a better understanding and appreciation of the importance of preventive behaviors like bednet use?32 4.2.2 Selection As we discussed earlier, the fundamental challenge of causal inference is that we cannot observe the counterfactual directly. So often we look to compare a group of people who were exposed to the potential cause to a group of people who were not exposed. No matter how hard we try to make sure that these two groups of people are equivalent before the treatment occurs, there can be observable and unobservable ways in which these groups differ. These differences represent selection bias, a threat to internal validity. For instance, Bradley et al. (1986) compared parasite and spleen rates among bednet users and non-users in The Gambia and concluded that bednets had a “strong protective effect” against malaria; however, the authors also observed that bednet use and malaria prevalence were also associated with ethnic group and place of residence. This makes ethnic group and place of residence confounding variables—plausible alternative explanations for the relationship between bednet use and malaria. Identifying selection bias and trying to account for it in the analysis is like squashing an ant on your countertop. You got that one, good for you, but there are probably many more hiding in the walls. You just can’t see them. It’s the same with selection threats. You might measure some of them, but many confounding variables will probably go unobserved. The only way to be certain that you are free of such threats is to randomly assign people to conditions. 4.2.3 History History threats pick up where selection threats leave off. Whereas selection threats are reasons that the groups might differ before the treatment occurs, history threats occur between the start of the treatment and the posttest observation. Before/after studies (aka, pre-post studies) are particularly susceptible to history threats. In these designs researchers assess the same group of people before and after some intervention. There is no separate control or comparison group. The assumed counterfactual for what would have happened in the absence of the intervention is simply the pre-intervention observation of the group. Okabayashi et al. (2006) is a good example. In this study, the researchers conducted a baseline survey and then began a school-based malaria control program. Nine months later, they conducted a post-program survey with the same principals, teachers, and students. On the basis of the pre-post differences they observed, the authors concluded the educational program had a positive impact on preventive behaviors. For instance, student-reported use of bednets (“always”) increased from 81.8% before the program to 86.5% after the program. It’s possible that the program changed behavior, but without evidence to the contrary, it’s also possible that something else was responsible for the change. Maybe another program was active at the same time. Maybe there was a marketing campaign for a new type of bednet just hitting the market. Maybe the posttest occurred during the rainy season when people know the risk of malaria is greater. These are all examples of possible history threats that could invalidate causal inference about the impact of the program on behavior change. 4.2.4 Maturation Single group designs like Okabayashi et al. (2006) are also subject to maturation threats. The basic issue is that people, things, and places change over time, even in the absence of any treatment. This is easy to imagine if you picture the learning that happens in one year of school for young kids. Comparing kids at the end of the year to their younger selves a year earlier and making a causal inference about some program or intervention is problematic because kids gain new cognitive skills as they age. Maybe the change you observe is due to a specific program or intervention, or maybe it is just the passage of time. Without a comparison group of similar aged kids it can be hard to know the difference. 4.2.5 Regression Artifacts This threat occurs most often when people are selected for a study because they have very high or very low scores on some outcome. It’s often the case that scores will be less extreme at retest, independent of any intervention. This statistical phenomenon is called regression to the mean. It happens because of measurement error and imperfect correlation. 4.2.6 Attrition Attrition happens when study participants do not participate in outcome assessments. When attrition is uneven between study groups, it can be described as systematic attrition. Just like selection bias makes groups unequal at the beginning of a study, attrition bias makes groups unequal at the end of the study for reasons other than the treatment under investigation. For instance, let’s say that researchers recruit depressed patients to take part in an RCT of a new psychotherapy that is delivered over the course of 10 weekly sessions. If the most depressed patients in the treatment group drop out because the schedule is too demanding, then the analysis would compare the control group (with the most depressed patients still enrolled) to a treatment group that is missing the most depressed patients.33 It would appear as if the treatment group got better on average, but part or all of the observed treatment effect would be due to attrition of the most depressed patients from the treatment group, not because of the treatment. 4.2.7 Testing Repeated administrations of the same test can have an effect on test scores, independent of the program that the test is designed to evaluate. For instance, practice can lead to better performance on cognitive assessments, and this improved performance can be mistaken as a treatment effect if there is not a comparison group. Testing threats decrease as the interval between administrations increases. 4.2.8 Instrumentation Testing threats describe changes to how participants perform on tests over time due to repeated administrations. When the tests themselves change over time we call this an instrumentation threat. For instance, if a study uses different microscopes or changes measurement techniques for the posttest assessment, it’s possible that differences in blood smear results could be incorrectly attributed to an intervention. 4.2.9 Additive and Interactive Effects Unfortunately, a study can be subject to more than one of these threats to internal validity. It’s possible for threats to work in opposite directions, or to interact and make matters a lot worse. For instance, if Okabayashi et al. (2006) had decided to compare students who received the program to students from another part of the country who did not receive the program, they might have observed a selection x history threat. The two groups of students might have been different to begin with (selection) and might have had different experiences over the study period unrelated to their treatment or non-treatment status (history). 4.3 Research Designs to Estimate Causal Impact If given the choice, most researchers would choose an experimental design to estimate causal impact. Experiments are subject to bias when things don’t go as planned (e.g., systematic attrition), but a good experiment is subject to fewer threats to internal validity compared to every other design for two main reasons: The cause always comes before the effect in an experiment (and quasi-experiment) because the treatment is “manipulated”; some people get the treatment but others don’t. After the treatment is administered to some people, outcomes are observed. Cause before effect. Random assignment makes plausible alternative explanations implausible. This is the big one. Whereas other designs require stronger assumptions about selection threats, experiments dismiss them by distributing observable and unobservable differences approximately equally across study arms. RCTs are not without serious critics. They don’t come more serious than Professor Sir Angus Deaton. In 2015 Deaton was awarded the Nobel Prize in Economics for his work on the measurement of poverty and inequality. In this video he presents a counterargument to Abhijit Banerjee’s defense of RCTs. Video from the entire 2012 “Debates in Development” conference can be viewed here. 4.3.1 When Life Gives You Non-Experimental Data As great as experiments are for mimicking the counterfactual, it’s not always logistically possible, politically feasible, or ethically justified to run an RCT. Most often, researchers have to infer causal inference from non-experimental data. How this is done in practice is shaped in part by disciplinary traditions. For instance, psychologists trained in the tradition of Campbell tend to focus on design choices you can make before a study is launched to improve causal inference by ruling out alternative explanations (Shadish, Cook, and Campbell 2003). This is the idea of the primacy of control by design—try to prevent confounding or at least investigate the plausibility of alternative explanations by adding design elements like more pre-test observations and comparison groups. Economists have a similar preference for strong designs, but their approach to causal inference tends to focus more on the analysis that comes after data collection. Whereas psychologists might ask about threats to internal validity, economists are likely to ask “what’s your identification strategy?”. Econometricians Angrist and Krueger (1999) defined identification strategies as “the combination of a clearly labeled source of identifying variation in a causal variable and the use of a particular econometric technique to exploit this information.” For instance, economists spend a lot of time thinking about the returns on schooling. The most common identification strategy to estimate the impact of schooling (the proposed causal variable) uses regression to control for potential confounds. In addition to regression, the econometrics (or ’metrics, if you are cool) toolkit for non-experimental data also includes instrumental variables, regression discontinuity, and differences-in-differences (Angrist and Pischke 2015). Let’s add interrupted time series to the list. Psychologists would label these quasi-experimental designs because they involve some manipulable cause that occurs before an effect is measured but lack random assignment. Let’s not forget about epidemiology. Epidemiologists are typically associated with observational designs such as cross-sectional surveys, case-control studies, and cohort studies, though plenty of epidemiologists design and implement RCTs. Epidemiologists do not typically distinguish between quasi-experimental and other non-experimental (or observational) designs like case-control or cohort studies. As you can see, practitioners within every discipline working on global health challenges are interested causal relationships, even when RCTs are not possible. So when life gives you non-experimental data, make causal inference! (and assumptions!) References "],
["logic.html", "5 Theory of Change 5.1 Overview 5.2 What Makes a Good Theory of Change? 5.3 Other Approaches", " 5 Theory of Change If you run out of small talk at your next mixer for M&amp;E professionals, ask people to tell you about their theory of change. You’ll get an earful. Let’s cover the basics so you too can become the life of the party. 5.1 Overview A theory of change articulates how an intervention—or a policy, program, or treatment—is expected to impact an outcome. It explains how X causes Y, and what is needed for this to happen. You might see this referred to as a logic model, logical framework, causal model, results chain, pipeline model, results framework, program theory, or one of several other combinations of these words. They all pretty much mean the same thing, but don’t claim this at your party. I’ll deny saying it. I believe there are three stages to learning how to develop a theory of change (and logic models, results frameworks, etc): Smugness: This is so basic. I can’t believe people write books about this. Confusion: (raises hand) Wait, what is the difference between an outcome and and output again? Smugness: That’s an output, not an outcome. How do you expect your program to work if you can’t articulate a clear theory of change? Let’s get started! 5.2 What Makes a Good Theory of Change? The first time you encounter a program’s theory of change, it will most likely be in the form of a graphic. This is not always a good thing. Just ask retired Army general Stanley McChrystal. Figure 5.1: Source: PA Consulting Group, via MSNBC, http://bit.ly/1gRbouw When he saw this PowerPoint slide about the American military strategy in Afghanistan during a briefing in Kabul in 2009, Gen. McChrystal remarked, “When we understand that slide, we’ll have won the war.”34 And that is how it goes with theories of change. If your model is grounded in the evidence, testable, and clearly articulated, you might actually make an impact. If not, you lose. Let’s look at the theory of change for another USG effort to solve a protracted problem: hunger and poverty. 5.2.1 A Plan to End Hunger and Poverty In 2009, President Obama pledged $3.5 billon dollars over three years to a global hunger and food security initiative that came to be called Feed the Future. FTF is said to take a “whole of government” approach. USAID and a coalition of 10 other federal agencies work in partnership with 19 focus countries, including Kenya, to promote agricultural development and end poverty and hunger. The 2010-15 strategy document for the FTF portfolio in Kenya exemplifies how large donors like the USG use theories of change in support of global health and development initiatives(Future 2011). Here is how the theory of change is explained in the text: To generate the economic growth needed to reduce poverty and hunger and to achieve the GOK’s vision of a commercial and modern agricultural sector, FTF in Kenya will invest in transforming agriculture through improved competitiveness of high-potential value chains and the promotion of diversification into higher-return, on- and off-farm activities. As documented by IFPRI, the development of selected value chains will have multiplier effects that spawn off- and non-farm employment opportunities. This is known as development speak. One of the hallmarks of development speak is that you reach the end of the paragraph, nod in approval, and then say “wait, what?”, suddenly realizing you know less than you did when you started the paragraph. This diagram (more) clearly depicts what the report prose struggles to do: Figure 5.2: FTF Theory of Change. Source: Feed the Future, http://1.usa.gov/1NDuQpn 5.2.2 Core Components Theory of change diagrams come in a variety of flavors. There is no RIGHT WAY™ to create one, as long as you can convey the fundamentals of how X leads to Y. If people understand your diagram, it is a good diagram. People are more likely to understand your diagram and your theory of change if you include a few common elements. The United Kingdom’s Department for International Development, commonly known as DFID or UK Aid, commissioned a report on the uses of theories of change in international development that identified several common components:35 influence of context discussion of long-term change process/sequence of change explained underlying assumptions presented as a diagram and narrative summary Going by this list, the FTF diagram could be improved by the addition of contextual factors and underlying factors. Templates are your friend My preferred approach to outlining a theory of change is to follow this template from the W.K. Kellogg Foundation. Here is an editable PowerPoint version. Figure 5.3: Theory of change template. Source: W.K. Kellogg Foundation, http://bit.ly/1My75Ay Start with the (research) problem statement (Box 1). This box gets at the heart of the reason your intervention exists. What are you trying to solve? You’d be surprised how often program design is disconnected from the problem that led to the program in the first place. This planning process will help you prevent you from ending up in this situation. Next think about what assets already exist and what needs remain (Box 2). There is always something to build upon, which is why it’s important to look for strengths in addition to challenges. This process is best done in collaboration with people impacted by the problem so that the proposed solution is grounded in their reality. If available, descriptive data sources like the DHS can help to outline (1) and (2). For a more local perspective, it might be beneficial to conduct a brief needs assessment in partnership with the local community if resources permit. Box 3 jumps to the desired results. If the program works, what will change? We’ll come back to what is meant by “outputs, outcomes, and impact” in a moment. With the results articulated, you can go back to the beginning and consider what factors might affect your program’s success positively or negatively (Box 4). The next step is to outline strategies for achieving your desired results that take into account potential barriers and facilitators (Box 5). This is where you articulate what your program will actually do. Finally, Box 6 reminds you that every theory of change is built on a set of assumptions. Be thorough and transparent as you think through the hidden beliefs that underlie your ideas about how your program will achieve its results. Example: Kenyan “Sugar Daddies” In 2014, an estimated 1.4 million people in Kenya were living with HIV. This is a prevalence rate of 5.3% among adults aged 15 to 49. Without a cure for AIDS, prevention remains critical to ending the epidemic. Starting in 2001, Kenya integrated HIV/AIDS education into the primary school curriculum as a new prevention strategy [jpal:2007]. At the time, the focus of this program—and many others programs across sub-Saharan Africa—was complete risk avoidance (Dupas 2011). Abstinence. Information on risk reduction was limited. In particular, students were not learning about the differential prevalence of HIV infection by age and gender. Girls were not learn that the older “sugar daddies” who provide nice things like phones and airtime in return for sex are more likely than the girls’ goofy age mates to be infected. An organization called ICS Africa set out to change this by rolling out a “Relative Risk Information Campaign” in Kenya. The intervention was brilliant in its simplicity. A program staffer would talk with students for 40 minutes. During this time, the staffer showed the class a 10-minute video on sugar daddies and led a discussion about cross-generational sex. During the session, the staffer reviewed results of recent studies and wrote facts about the distribution of HIV prevalence on the chalk board. JPAL researchers tested ICS Africa’s risk reduction strategy in a randomized experiment in Western Kenya. In the first phase (2003), 328 schools were randomized to teacher training on the national HIV prevention curriculum (Duflo et al. 2006). In the second phase (2004), 71 of these schools were stratified and randomized to receive the sugar daddy intervention (Dupas 2011). In total there were four study arms: (i) teacher training only, (ii) sugar daddy only, (iii) teacher training and sugar daddy, and (iv) nothing. The results were shocking. Teacher training was a bust. Sure, the training led to a change in teaching practices—notably that trained teachers mentioned HIV in class more often than non-trained teachers—but it had little effect on HIV knowledge or childbearing. Increasing knowledge about HIV makes intuitive sense as an outcome for a study about HIV prevention. But why childbearing? Well, babies don’t come from storks. They come from unprotected sex. It’s harder to lie about birthing a baby than it is to lie about your private sexual behavior, so childbearing is thought to be a more objective measure of unprotected sex. Unprotected sex is a main driver of HIV transmission, so measuring childbearing is a proxy for HIV risk from unprotected sex. In contrast, the 40-minute sugar daddy discussion and video reduced childbearing with men at least five years older by 65%—and not because girls started having babies with males their own age. The overall incidence of childbearing fell by 28%. With a cost of $28.20 USD per school and $0.80 per student, the cost per childbirth averted was $91 (JPAL 2007). Returning to our template, let’s piece together the theory of change for the relative risk reduction program. Figure 5.4: Sugar daddy awareness theory of change 5.3 Other Approaches While a theory of change tends to be a high-level depiction of the “why”, other approaches are more detailed and focus on the “how”. 5.3.1 Results Framework Donors like USAID commonly require project staff to create results frameworks to monitor progress toward achieving the stated goals. Here’s an example results framework from the Feed the Future strategic plan for Kenya. Figure 5.5: FTF Results Framework. Source: Feed the Future, http://1.usa.gov/1NDuQpn You can read this diagram from the top down. The overall goal reflects the mission of the broader FTF program: reduce poverty and hunger. The plan for attaining this goal is to reach two objectives: inclusive agricultural sector growth and improved nutritional status. If the goal represents the ultimate desired impact, objectives are long range strategies. You know you are on track to reaching your objectives if you hit several intermediate results, or IRs. For instance, IR6, “Improved utilization of MCH and nutrition services”, is expected to improve the nutritional status of women and children. This IR has three sub IRs, including 6.3 “Strengthened MCH nutrition surveillance”. The rationale for sub-IR 6.3 is that better monitoring and data will enable earlier identification of at-risk individuals, which in turn should mean earlier initiation of nutrition interventions. Every IR has a set of indicators for measuring progress. In the case of IR6, FTF lists the following illustrative indicators: prevalence of maternal anemia number of children under five years of age who received vitamin A from USG-supported programs number of people trained in child health and nutrition through USG-supported health area programs (disaggregated by gender) number of clients who received food and/or nutrition services (PEPFAR indicator) 5.3.2 Logic Models DFID requires grantees to prepare logic models—or logframes as they like to say—for planning, monitoring program implementation, and program evaluation and reporting. Logic models take the “results chain” or “pipeline” approach shown below. Figure 5.6: Logic model. Source: W.K. Kellogg Foundation, http://bit.ly/1My75Ay In a logic model, inputs and activities represent your planned work. Outputs, outcomes, and impact are your intended results. Inputs: The resources needed to implement the program. People, money, time, etc. Activities: What your program will do. Trainings, events, distribution of goods, etc. Outputs: What your program did. Number of people trained, number of events held, number of goods delivered and number of people who benefitted. Outcomes: Short- and medium-term results of your program. Increased knowledge, decreased risky behavior, improved functioning, etc. Impacts: Long-term effects of the program outcomes. Lower HIV prevalence, reduced morbidity and mortality, etc. Here’s what a logic model might look like for the Kenyan relative risk reduction program. Figure 5.7: Sugar daddy awareness logic model References "],
["measurement.html", "6 Measure Twice, Cut Once 6.1 What is an Indicator? 6.2 Selecting Good Indicators 6.3 Evaluating Data Collection Instruments and Methods", " 6 Measure Twice, Cut Once Source: http://bit.ly/1X5yjU2 This proverb, well known to carpenters everywhere, should be the motto of every aspiring researcher. Ignore it and you run the risk of severely limiting what you can learn from your study—or ruining the whole thing altogether. Despite its importance, however, measurement rarely gets the attention it deserves. One reason, I think, is that to really understand measurement issues in a particular sub-field you need to get deep into the weeds. It’s relatively easy to pick up a journal article outside of your major area of study and find something to say about the study design. Not so when it comes to the measurement. Take an issue like maternal mortality. Dead or alive. Easy, right? Not quite. What counts as a maternal death? What if a pregnant woman slips on a patch of ice and dies? Is that a maternal death? What if she dies from an infection 40 days after giving birth to a healthy baby girl? Is that a maternal death? No and yes. I’ll explain why. In the process, I’ll introduce some key measurement issues to consider when designing or reviewing a study. I’ll also introduce you to some population-level indicators that every global health data junkie should know. 6.1 What is an Indicator? Part of developing a good logic model is to identify indicators for each part of the causal chain.36 An indicator is an observable measure of a concept.37 For instance, a primary outcome in the Kenya “sugar daddy” intervention study discussed in Chapter 3 was “reduction in unprotected sex”. We can’t observe unprotected sex directly, so it is a concept in need of a measurable indicator. That indicator could be self-reported unprotected sex, but it might be hard to get people to report honestly. A potentially more accurate measure of unprotected sex among girls of reproductive age is the incidence of childbirth. Let’s look at another example and consider the importance of measuring indicators at each step of the causal chain. {width=“wide”} | Logic Model | Concept | Indicator | |: —|:—|:—| | Input | Financial resources | Expenditures | | Activities | Treatment fidelity | Checklist score | | Outputs | Compliance | # TBAs trained | | Outcomes | Fewer complications | # hemorrhage | | Impacts | Reduced mortality | # maternal death | Our example logic model comes from a study by Jokhio and colleagues (2005) of the impact of training traditional birth attendants on stillbirths, neonatal deaths, and maternal deaths.38 When this study was conducted in 1998, the maternal mortality ratio in Pakistan was more than 280 per 100,000 live births (compared to 170 in 2013).39 This represents more than 13,000 women who died giving birth. The tragedy in this number is that maternal mortality is preventable. Women continue to die before, during, and after childbirth because of three major delays: (i) delays in seeking healthcare; (ii) delays in reaching healthcare; and (iii) delays in receiving high quality healthcare upon reaching a medical facility.40 The most common proximate causes of death are postpartum hemorrhage, hypertensive disorders, and sepsis.41 It’s hard to predict who will suffer life threatening complications, so there is debate about how to best intervene. Some argue that we should invest in interventions to increase facility deliveries so that women would be better able to access emergency obstetric care if needed. Others point to the low frequency of complications and the high prevalence of poor quality care at medical facilities as a reason that women should be supported in their decision to stay at home where the family can better support them. At the time Jokhio and colleagues conducted this study, facility deliveries in Pakistan were rare. The authors cited descriptive studies showing that most women in Pakistan deliver at home (80%) without a skilled birth attendant (89%). Only 1 out of 20 women who experience complications makes it to a medical facility with emergency obstetric care. Given the high prevalence of home deliveries, Jokhio et al. tested an intervention that trained traditional birth attendants—the mothers, aunts, sisters, and neighbors who already help women through childbirth but have no formal medical training—and provided them with delivery kits. The training lasted 3 days and covered topics such as checking for danger signs, conducing a clean delivery using the provided kits, and making emergency referrals. Trained TBAs were asked to visit with women at least three times during the pregnancy to look for danger signs, and then again for the delivery. The study design was a cluster randomized trial. Seven subdisticts of a rural district were randomized to the TBA intervention or the standard of care. TBAs in subdistricts assigned to the control group did not receive any training or delivery kits. The primary endpoint was reduced perinatal deaths (stillbirths and neonatal deaths) and maternal deaths. Before discussing the study results, let’s think about measurement throughout the causal chain. 6.1.1 PROCESS INDICATORS M&amp;E managers think long and hard about how to track inputs, activities, and outputs—process or monitoring indicators—and how to summarize the data in quarterly monitoring reports to donors or NGO/ministry supervisors. All of these stakeholders want to know how much was spent and for what purpose, whether the implementation plans were followed, and how many people benefitted. As researchers we also care about collecting good process data, but usually as a means to developing a better understanding why programs do or don’t work. Examples include program costs (to estimate cost-effectiveness), treatment fidelity (to judge the quality of implementation), and compliance with study protocols (to assess threats to internal validity). 6.1.1.1 Inputs 6.1.1.1.1 Cost and cost-effectiveness Impact evaluations produce estimates of the effectiveness of a program or intervention. Does the program “work”? For some public health and behavioral health nerds, evidence of impact is enough because they are narrowly focused on developing and testing new interventions. Not true for many economists and policymakers who are thinking about delivering programs at scale with limited public funding; they want to know whether the intervention is cost-effective, not just effective.42 A cost-effectiveness analysis requires close tracking of the cost of all program inputs.43 Unfortunately, Jokhio et al. don’t tell us about the cost, other than to say the intervention was designed to be “low cost and sustainable.”44 {icon=puzzle-piece} G&gt; Cost effectiveness analysis (CEA) is not the same thing as cost-benefit analysis (CBA). The latter involves putting a dollar amount to benefits, such as the benefit to society of averting a maternal death. This is hard to do and quite subjective. This is why you will come across CEA much more frequently in global health. The typical calculation is the effect size divided by the costs. There is no effort to put costs and effects on the same dollar metric. 6.1.1.2 Activities/Outputs 6.1.1.2.1 Treatment fidelity The TBA intervention included a 3-day training and the provision of delivery kits. If a trainer decided to skip portions of the training or knock off a day, the program would not be delivered as intended (aka, without fidelity to the treatment). The consequence of low treatment fidelity is usually an attenuation of treatment effects.45 This is a threat to internal validity. If the study shows no effect but treatment fidelity is low, we can’t be confident in the null result. Implementation failure rather than theory or program failure could be to blame. Low fidelity is also a threat to external validity because it isn’t possible to truly replicate the study. The first step in developing a measure of fidelity is to identify what constitutes the intervention. Then for each component of the intervention, you define the benchmark for success. If your intervention involves the distribution of goods such as delivery kits, this is relatively straightforward: count the kits delivered. When your intervention involves the delivery of a service, fidelity tracking is more complicated. Training is an example of a service. In the write-up of the TBA study, we’re told that TBAs were trained on the following topics: giving advice on antepartum, intrapartum, and postpartum care; how to conduct a clean delivery; how to use the disposable delivery kit; when to refer women for emergency obstetrical care; and care of the newborn. There are two levels of training we can evaluate: the training of the trainers (TOT) and the competency of the trainees. First, we want to make sure that the people leading the TBA training (i) adhere to the training manual (“do the facilitators deliver all of the training content?) and (ii) correctly present all of the training material (”do they present the training material correctly?“). We might do this by creating a TOT fidelity checklist that can be used to assess adherence and competence.46 For instance: Adherence trainer reviews didactic material on conducting a clean delivery trainer facilitates the demonstration activity on how to use the disposable delivery kit Competence trainer correctly explains the key danger signs to look for during the antepartum period trainer demonstrates proper use of the delivery kits Second, we want to know that the TBAs learned something from the training. This could be as simple as administering a knowledge and practical test at the end of the training. To continually monitor fidelity over time, we could arrange to observe or record TBA interactions with patients and evaluate fidelity with another checklist. Depending on the situation, we could also use “mystery clients”—confederates who pose as real clients—to evaluate fidelity without the target’s knowledge. 6.1.1.2.2 Treatment compliance In a randomized trial, people (or other units like schools and clinics) are randomly assigned to different study arms. In Jokhio et al., subdistricts were randomly assigned to the TBA intervention group (treatment) or the no program group (control). Sometimes plans don’t work out and people or units are “non-compliant” with these assignments. For instance: Some people assigned to the treatment group are never treated or only partially treated. Maybe some TBAs never showed up for the training program or stopped attending before the end. Some people assigned to the control (comparison) group receive the treatment despite their assignment to the inactive control group. An example would be if TBAs in control subdistricts heard about the nearby training and decided to attend. Just like low-treatment fidelity, partial compliance with the study protocol can attenuate treatment effects. We’ll discuss these threats more in the chapter on experimental designs. The take home point for now is that close tracking of outputs can be critical for your analysis of treatment effects. 6.1.2 OUTCOME/IMPACT INDICATORS Researchers tend to spend the most time and effort defining measurement strategies for study outcomes and impacts. Jokhio et al. evaluated the effect of the TBA intervention on a set of outcomes that included the occurrence of major complications of pregnancy and referral to emergency obstetric care. The authors also examined the impact of the intervention on maternal mortality, but the study only had the statistical power—the topic of a later chapter—to detect a 90% drop in maternal deaths. Thus the authors were unlikely to find a statistically significant effect on maternal deaths, but the indictor made sense in the theory of change: better equipped TBAs who are present at deliveries will reduce the incidence of major complications that are most likely to lead to maternal deaths; better access to emergency referrals will have the same life saving effect. {icon=comment} G&gt; ## So what did they find? G&gt; G&gt; The intervention reduced the odds of hemorrhage and sepsis by 39% and 83%, respectively. Women exposed to trained TBAs were also 1.5 times more likely to get a referral to emergency obstetric care.47 Obstructed labor was more likely among the intervention group, but this can’t be caused by TBAs. So how do we interpret this finding given that the study design was an experiment, and we know experiments are made for causal inference? Increased reporting. TBAs were trained to recognize danger signs and make referrals. It’s likely that trained TBAs were more likely to see something and say something. G&gt; G&gt; So what about maternal mortality? The effect size estimate suggested a 26% reduction in maternal deaths, but the estimate was not very precise. The study was only designed to detect a reduction of 90% with precision, so this is not surprising. So no definitive evidence about the impact of the intervention on maternal mortality, but large effects on the intermediate outcomes supports the underlying theory of change. 6.2 Selecting Good Indicators There are some people who say that indicators must be SMART™: Secific | clearly defined | Measurable | able to be quantified | Attainable | target must be realistic | Relevant | related to the construct | Time-bound | observed at specific times | I think indicators should be DREAMY™: Defined | clearly specified | Relevant | related to the construct | Expedient | feasible to obtain | Accurate | valid measure of construct | Measurable | able to be quantified | customarY48 | recognized standard | 6.2.1 DREAMY INDICATORS™ Jokhio and colleagues wanted to develop and test an intervention that would make motherhood safer. The ultimate indicator of safe motherhood is reduced maternal mortality. Let’s put this indicator to the DREAMY test. 6.2.1.0.1 Defined The first step to becoming a good indicator is having a clear definition. Jokhio et al. used the following definition of a maternal death: A&gt; Maternal deaths were defined as death of the mother during pregnancy, delivery, and up to six weeks post-partum, excluding deaths known to have been due to injury or accident. 6.2.1.0.2 Relevant Maternal deaths are clearly relevant to the construct of safe motherhood. 6.2.1.0.3 Expedient Data on maternal deaths are not easy to collect. Jokhio et al. relied on Lady Health Workers, a cadre of educated women with 3 to 6 months of training in primary health care and family planning who are an official part of Pakistan’s public health system. Despite the challenges of collecting data on maternal mortality, the Lady Health Workers were surprisingly effective; they successfully followed up with 99.8% of the 19,557 women recruited into the study. 6.2.1.0.4 Accurate Simply, yes. Maternal deaths measure what we think they measure—mortality. 6.2.1.0.5 Measurable Not such a simple yes. Maternal deaths can be quantified, but without a registry of death certificates indicating the cause of death, it’s easy to make mistakes. So what did Jokhio et al. do? A&gt; In cases of maternal death, the cause was ascertained by Lady Health Workers on the basis of oral reports from relatives, neighbors, or traditional birth attendants. The authors do not use the term “verbal autopsy”, but that is the inferred method. In this particular study, a VA is a reasonable choice because the timing of death was ascertainable for all women. It is possible that some deaths due to accident or injury were incorrectly labeled as maternal deaths, but there is no way to know. {icon=comment} G&gt; Measurement can change over time, and these changes can impact your analysis. For instance, the U.S. revised the U.S. Standard Certificate of Death in 2003 to include “late maternal deaths” (43 to 365 days) and “pregnancy-related deaths” to better align with the ICD-10 criteria. At the time, only 21 of 50 states recorded a woman’s pregnancy status; those that did used 1 of 6 different questions.49 G&gt; G&gt; 6.2.1.0.6 customarY Jokhio et al.’s definition of maternal mortality will look familiar to folks who study reproductive health issues because it largely follows the authoritative definition that comes from a set of classification standards known as the “International Statistical Classification of Diseases and Related Health Problems”, or ICD. According to the latest version, the ICD-10:50 A&gt; A maternal death is the death of a woman while pregnant or within 42 days of termination of pregnancy, irrespective of the duration and the site of the pregnancy, from any cause related to or aggravated by the pregnancy or its management, but not from accidental or incidental causes.51 6.2.2 USE INTERNATIONAL STANDARDS Whenever possible, you should seek out standard indicators and follow existing definitions and calculation methods. The WHO publishes a Global Reference List of the 100 core health indicators.52 The 2015 version categorizes the indicators several ways: domain health status risk factors service coverage health systems (service delivery, quality of care, health financing, essential medicines, the health workforce, health information) subdomains communicable diseases (HIV/AIDS, sexually transmitted infections, tuberculosis, malaria, neglected tropical diseases, outbreaks, epidemic diseases) reproductive, maternal, newborn, child and adolescent health (including sexual health and reproductive rights and immunization), noncommunicable diseases (including chronic disease, health promotion, nutrition, mental health and substance abuse) injuries and violence and the environment levels of the results chain framework Here is the full list of indicators organized by steps in a logic model. Source: http://bit.ly/1NgGeLh In the entry for maternal mortality ratio, we find the following definition: A&gt; The annual number of female deaths from any cause related to or aggravated by pregnancy or its management (excluding accidental or incidental causes) during pregnancy and childbirth or within 42 days of termination of pregnancy, irrespective of the duration and site of the pregnancy, expressed per 100 000 live births, for a specified time period. The calculation of the MMR is specified in this entry, as is the method of measurement and estimation. 6.3 Evaluating Data Collection Instruments and Methods Sometimes it’s possible to use existing data sources, such as administrative or medical records.53 It’s often the case, however, that administrative records are incomplete, unreliable, and unsuitable for constructing your specific outcome of interest—especially in developing countries. In all likelihood, especially in the case of program evaluations, part or all of your data will come from original data collection efforts. Potentially the most common forms of data collection in global health are interviews and surveys. Some less commonly used non-survey instruments include:54 direct observation random spot checks mystery clients incognito enumerators physical tests biomarkers mechanical tracking devices spatial demography participatory resource appraisals observe purchasing decisions games standardized tests vignettes 6.3.1 PSYCHOMETRICS In some fields such as psychology, it is common to measure outcomes by administering self-reported instruments to participants and calculating a score from their responses. Whether you are using an off-the-shelf instrument or developing your own, you want to be sure that the instrument is a reliable and valid measure of your indicator. A reliable measure is one that is capable of producing the same results when measuring the same thing repeatedly. Imagine your bathroom scale. If you stepped on, then off, then on again and the scale read 210 lbs and then 185 lbs, you would realize that you are the owner of a broken, unreliable scale. Undeterred, you head to the store and pick up a new scale—this time one that posts your weight to Facebook to keep you “accountable”. So you step on, off, and back on again. The scale reads 400 lbs then 402 lbs. As your Facebook wall updates, you whack the scale and get back on again. It reads 399. Your mom posts a comment congratulating you on dropping out of the 400s and 50 people like it. Meanwhile, you call customer support and explain that your scale is very reliable (precise), but also very inaccurate. You actually tell the customer service rep that the scale is an “invalid measure of your current weight”. He’s heard it before, but it’s actually true in your case. Rule #1: Don’t get a scale that posts to Facebook. Rule #2: An instrument can be reliable but not valid. Rule #3: But an instrument cannot be valid without being reliable. 6.3.1.1 Reliability There are several methods for assessing the reliability of an instrument. Let’s imagine that you developed a new 8-item measure of anxiety in which people have the following response options: (0) never, (1) rarely, (2) sometimes, or (3) often. This means that possible scores could range from 0 to 24. 6.3.1.1.1 Test-retest reliability This is when scores are correlated on repeat administrations of the instrument. Participants complete your survey today and then again in a week. If each person’s score is the exactly the same the second time, your instrument would be perfectly reliable. It won’t be, but you’ll hope for a high correlation coefficient (conventionally higher than 0.70). For instance, let’s say we survey 50 people and enter their data. Here are the responses for the first 6 people in the dataset. Each row consists of data for 1 person. The last column is the sum of the the person’s responses to the instrument’s 8 items. {linenos=off} ~~ &gt; head(datT1) q1 q2 q3 q4 q5 q6 q7 q8 t1.sum 1 1 1 1 0 2 2 2 2 11 2 2 2 2 1 3 2 2 3 17 3 3 3 3 0 2 3 3 3 20 4 2 3 3 0 2 2 2 2 16 5 2 3 3 1 3 3 2 3 20 6 2 2 2 0 3 3 2 2 16 ~~ Then we go back and survey these same people again and enter their data once more. Here’s the data for the first 6 people in the re-test dataset. {linenos=off} ~~ &gt; head(datT2) q1 q2 q3 q4 q5 q6 q7 q8 t2.sum 1 0 1 0 0 0 3 2 3 9 2 3 1 3 2 3 3 3 3 21 3 3 3 2 2 3 1 3 3 20 4 3 3 3 1 2 3 2 3 20 5 3 3 3 3 3 3 2 3 23 6 3 3 3 1 3 2 2 2 19 ~~ To calculate the test-retest reliability, we’ll correlate the sum scores for each administration: t1.sum and t2.sum. We find a high correlation of 0.78, suggesting that the instrument has good test-retest reliability. {linenos=off} ~~ &gt; cor(datT1\\(t1.sum, datT2\\)t2.sum) [1] 0.7753866 ~~ 6.3.1.1.2 Interitem reliability This is when responses to items in your instrument are consistent. If not, you have to wonder if they are measuring the same underlying construct of anxiety. Item total correlation: One approach to finding ugly ducklings in your instrument is to calculate item-total correlations. It’s easy: correlate scores on each item with the total scores. Generally item-total correlations exceeding 0.30 are sufficient. Returning to our dataset from the first administration, we see that most items have an acceptable item-total correlation, except q4. This item seems to be measuring something different than the others. Maybe the rest are clearly tapping into the construct of anxiety, whereas q4 asks people about how often they feel sad. Maybe it would fit better in an instrument assessing depression. {linenos=off} ~~ &gt; library(multilevel) &gt; item.total(datT1[,1:8]) Variable Item.Total Alpha.Without N 1 q1 0.6021512 0.8033555 50 2 q2 0.5914340 0.8044035 50 3 q3 0.7402437 0.7808027 50 4 q4 0.1452794 0.8458352 50 5 q5 0.5295935 0.8127216 50 6 q6 0.5636561 0.8081818 50 7 q7 0.5369509 0.8117495 50 8 q8 0.6430992 0.7967467 50 ~~ Cronbach’s alpha: Instruments are imperfect. Even your new anxiety instrument that you worked so hard to create. Flawed. The result is that every person’s observed score is actually a function of their ‘true’ score (which we can’t know) plus some amount of measurement error. Cronbach’s alpha gives you an estimate of how much variance in people’s scores on your instrument is measurement error. When you calculate Cronbach’s alpha in a program like R, behind the scenes R does the equivalent of splitting the dataset into two halves over and over again and calculating the correlation between total scores for the first half with total scores for the second half. Cronbach’s alpha is the average of all possible correlation coefficients. If alpha is 0.70—a rough guide for the low-end of acceptable—it means that only 30% of the variance in scores is measurement error. In our sample dataset, alpha is 0.81, meaning that 19% of the variance is error. {linenos=off} ~~ &gt; library(psych) &gt; a &lt;- alpha(datT1[,1:8]) &gt; a[[1]][2] std.alpha 0.814491 ~~ 6.3.1.1.3 Interrater reliability This is another type of reliability that indicates whether two observers are consistent in their ratings of the same thing. Instead of using an 8-item self-report instrument, we might want to have two observers watch a video of a parent and child interacting and rate each behavior as “warm” or “cold”. If the observers agree a lot, they would be reliable. Percent agreement is one way to measure interrater reliability, but it’s not the best. It does not account for agreement that can happen by chance. Cohen’s kappa coefficient does, and you generally want a value greater than 0.40. 6.3.1.2 Validity Does your measure measure what you intend to measure with your measure? (unrelated bonus: how many metrics could your measure measure if your measure could measure metrics?). In other, less annoying words: Does your 8-item anxiety instrument actually measure this thing called anxiety? If not, it’s not a valid measure of anxiety. There are several types of validity that you can use to determine whether your instrument is valid. 6.3.1.2.1 Face validity This is the weakest form of validity. The basic idea is: if it looks like a duck, quacks like a duck, it’s a duck. If people think your anxiety instrument asks about anxiety, it has face validity as a measure of anxiety. This is a weak standard, however. A great looking instrument can perform very poorly in practice, and an instrument that appears to lack face validity might perform very well. The bottom line is that if you read an article and the only mention of validity is face validity, it’s a lame duck. 6.3.1.2.2 Construct validity Anxiety is a hypothetical construct. If your 8-item anxiety instrument has construct validity, it will be related to other instruments that are also thought to measure anxiety (convergent validity), but it will not be related to other instruments that have nothing to do with anxiety (discriminant validity). For instance, your anxiety instrument would have convergent validity if participant scores on your measure are correlated with their scores on an existing measure like the Multidimensional Anxiety Scale for Children (MASC). Your instrument would have discriminant validity if scores were not strongly correlated with scores on a measure of narcissism. If your instrument has both convergent and discriminant validity, you have more confidence that it measures the construct of anxiety that you think it measures. 6.3.1.2.3 Criterion-related validity An even more robust form of validity is criterion validity. Do scores on your 8-item anxiety instrument correctly predict which participants will be diagnosed as suffering from an anxiety disorder when evaluated independently by a psychologist or psychiatrist? In this case, the mental health professional’s judgement—made completely independent of the data generated by your instrument—is the “gold standard”. A valid instrument will produce accurate predictions. This is easier to understand with a figure. Confusion matrix Your anxiety instrument has a possible range in scores from 0 to 24. Let’s say we have reason to believe that the cutoff for anxiety is a score of 15. Anyone with a result greater than or equal to 15 will be classified by your instrument as “anxious”. Everyone else is classified as “not anxious”. These are the two possible test results. There are also two possible diagnostic results from the gold standard mental health professional: has anxiety and doesn’t have anxiety. Taken together, there are four possible combinations of test and gold standard results: true positive (a): both the test result and the gold standard indicate that the person is anxious true negative (d): both the test result and the gold standard indicate that the person is NOT anxious false positive (b): the test result suggests anxiety, but the gold standard disagrees false negative (c): the test result suggest no anxiety, but the gold standard disagrees The true positive rate is known as sensitivity. It is calculated as follows: true positive / all positives = a / (a + c) Sensitivity 30 / (30 + 3) = 91% 91% of patients with anxiety correctly screened positive. 9% of patients did not (false negative rate). The true negative rate is known as specificity. It is calculated as follows: true negatives / all negatives = d / (b + d) Specificity 10 / (5 + 10) = 67% 67% of patients with no anxiety correctly screened negative. 33% incorrectly screened positive (false positive rate). There is often a tradeoff between sensitivity and specificity. The tradeoff can be visualized with a receiver operating characteristic (ROC) analysis. ROC curves plot sensitivity (the true positive rate) against 1 minus specificity for a range of different cutoff points. Earlier I mentioned that the cutoff score for your anxiety measure was 15. To determine if 15 is the best cut point, we could calculate the sensitivity and specificity again for several different cutoff points and plot the relationship in a ROC curve. For instance, Rathore et al. (2014) evaluated different cut scores for the Patient Health Questionnaire 9 (PHQ-9) when administered to adults with epilepsy and displayed the results on a ROC curve.55 ROC. Source: http://1.usa.gov/1EHevjO The area under the ROC curve was 0.90, generally considered to be a very accurate benchmark. It appears that a cut score of 12 on the PHQ-9 balances sensitivity and specificity. Decreasing false positives (higher specificity) could be favored to avoid labeling non-depressed patients as depressed and using resources for unnecessary additional evaluations. Doing so would mean missing more true positives, but it could be a defensible tradeoff. 6.3.2 TO USE, ADAPT, OR DEVELOP? When you design a new study that includes new data collection, you have to decide whether to use or adapt existing instruments that measure your indicators, or to develop your own. There are pros and cons to each approach. There is a good chance that the off-the-shelf instrument you want to use in Country Z was developed in Country A and never used or at least never validated in Country Z. Using it would enable some cross-cultural and cross-national comparisons, but without running your own validity study first, you don’t have much confidence that the instrument is valid or reliable in Country Z. An alternative approach is to start with the off-the-shelf measure and adapt it for your needs. You translate it from one language to another, and you have someone else translate it back to the original language before you and your colleagues attempt to resolve discrepancies. Maybe you conduct some formative qualitative research to determine if there are some constructs not represented in the existing instrument that you could add. Or you pilot test the existing instrument and ask people to explain to you their understanding of the questions and response choices (cognitive interviewing). Then you ask yourself these six questions about cross-cultural validity:56 What is the purpose of the instrument? What is the construct to be measured? What are the contents of the construct? What are the idioms used to identify psychological symptoms and behaviors? How should questions and responses be structured? What does a score on the instrument mean? Or at the other end of the continuum, you believe that your instrument should be completely grounded in the lived experience of where you are working in Country Z, so you start from scratch with formative qualitative work. It is only through this work that a new instrument emerges.57 The key measurement lessons of this chapter apply whether your research question is descriptive, correlational, or causal.↩ You’ll sometimes see the terms outcome, indicator, measure, target, instrument, and variable used interchangeably. Don’t fall into this trap. Indicators are measures of outcomes (or impacts). A target is a goal for change in the value of an indicator (e.g., reduce maternal mortality ratio by half). Instruments are the tools for measuring indicators. (Instruments might also go by different names, including scales, measures, inventories, batteries). Variables are the numeric values of the indicators that can vary between observations.↩ Jokhio, A. H., Winter, H. R., &amp; Cheng, K. K. (2005). An intervention involving traditional birth attendants and perinatal and maternal mortality in Pakistan. New England Journal of Medicine, 352(20), 2091-2099.↩ WHO (2014). Trends in maternal mortality: 1990 to 2013. Estimates by WHO, UNICEF. UNFPA, the World Bank and the United Nations Population Division. Geneva, World Health Organization.↩ Thaddeus, S., &amp; Maine, D. (1994). Too far to walk: maternal mortality in context. Social Science &amp; Medicine, 38(8), 1091-1110.↩ Say, L., Chou, D., Gemmill, A., Tunçalp, Ö., Moller, A. B., Daniels, J., … &amp; Alkema, L. (2014). Global causes of maternal death: a WHO systematic analysis. The Lancet Global Health, 2(6), e323-e333.↩ You’ll recall from Chapter 1 that this gap between developing evidence of effective programs and actually implementing them at scale is an example of a “T4” translational research bottleneck.↩ See the “WHO guide to cost-effectiveness analysis” for guidance.↩ The Kenya “sugar daddies” study from Chapter 3 included a cost-effectiveness analysis. Recall that the program reduced the incidence of childbearing by 28%. The program was reported to cost $28.20 USD per school and $0.80 per student. This worked out to $91 per childbirth averted.↩ Think about it this way: if you replace half the dose of an injection with saline, the medication would not work as well, if it even works at all.↩ Breitenstein, S. M., Fogg, L., Garvey, C., Hill, C., Resnick, B., &amp; Gross, D. (2010). Measuring implementation fidelity in a community-based parenting intervention. Nursing Research, 59(3), 158.↩ Which is not to say that they were 1.5 times more likely to need this referral or that they were 1.5 times more likely to actually receive referral care. Just that the a referral was more likely to be made.↩ Anyone who figures out a better Y gets a free signed copy of this book.↩ Hoyert, D. L. (2007). Maternal Mortality and Related Concepts.↩ The ICD-9 definition—which would have been the standard at the time of the study—is very similar. For more information, see http://1.usa.gov/1JF0vnv.↩ Maternal deaths are therefore characterized by a causal and a temporal component. In the chapter opening I asked whether it would be a maternal death if a pregnant woman slips on a patch of ice and dies—or if she dies from an infection 40 days after giving birth to a healthy baby girl. No and yes. Accidents like slipping on ice don’t count, but a non-accidental death within 42 days of a baby’s birth would.↩ WHO (2015). Global Reference List of 100 Core Health Indicators.↩ There is also secondary data analysis in which you analyze data that have already been collected for another purpose—typically another study.↩ Glennerster, R., &amp; Takavarasha, K. (2013). Running Randomized Evaluations: A Practical Guide. Princeton University Press.↩ Rathore, J. S., Jehi, L. E., Fan, Y., Patel, S. I., Foldvary-Schaefer, N., Ramirez, M. J., … &amp; Tesar, G. E. (2014). Validation of the Patient Health Questionnaire-9 (PHQ-9) for depression screening in adults with epilepsy. Epilepsy &amp; Behavior, 37, 215-220.↩ Kohrt, B. A., Jordans, M. J., Tol, W. A., Luitel, N. P., Maharjan, S. M., &amp; Upadhaya, N. (2011). Validation of cross-cultural child mental health and psychosocial research instruments: adapting the Depression Self-Rating Scale and Child PTSD Symptom Scale in Nepal. BMC Psychiatry, 11(1), 127.↩ And if you realize that you do not have a gold standard to help validate your new culturally-anchored instrument, you’ll read: Bolton, P. (2001). Cross-cultural validity and reliability testing of a standard psychiatric assessment instrument without a gold standard. The Journal of Nervous and Mental Disease, 189(4), 238-242.↩ "],
["references.html", "7 References", " 7 References "]
]
